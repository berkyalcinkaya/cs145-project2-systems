{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berkyalcinkaya/cs145-project2-systems/blob/main/cs145_project2_systems_template_fa2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM8u06dhhieu"
      },
      "source": [
        "## Collaborators\n",
        "\n",
        "1.   Berk Yalcinkaya\n",
        "2.   Nick Allen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T7CuUFejs1R"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kSLvQ6xjjqQh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import uuid\n",
        "import argparse\n",
        "import time\n",
        "import psutil\n",
        "import heapq\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import shutil\n",
        "import glob\n",
        "import gc\n",
        "from IPython.display import display\n",
        "import tempfile\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwVW6zMghmMq"
      },
      "source": [
        "# Section 0: Generate Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y0yOxXShWkL"
      },
      "source": [
        "This section has already been implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Djt3vqewhA76"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "\n",
        "def generate_songs_chunk(start, size, string_length=100):\n",
        "    data = {\n",
        "        \"song_id\": range(start, start + size),\n",
        "        \"title\": [f\"Song_{i}\" for i in range(start, start + size)],\n",
        "    }\n",
        "    base_strings = generate_base_strings(size, string_length)\n",
        "    for i in range(1, 11):\n",
        "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_users_chunk(start, size, string_length=100):\n",
        "    data = {\n",
        "        \"user_id\": range(start, start + size),\n",
        "        \"age\": [18 + ((start + i) % 60) for i in range(size)],\n",
        "    }\n",
        "    base_strings = generate_base_strings(size, string_length)\n",
        "    for i in range(1, 11):\n",
        "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_listens_chunk(start, size, num_users, num_songs, string_length=16):\n",
        "    data = {\n",
        "        \"listen_id\": range(start, start + size),\n",
        "        \"user_id\": np.random.randint(0, num_users, size=size),\n",
        "        \"song_id\": np.random.randint(0, num_songs, size=size),\n",
        "    }\n",
        "    base_strings = generate_base_strings(size, string_length)\n",
        "    for i in range(1, 11):\n",
        "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_base_strings(num_records, string_length):\n",
        "    chars = np.array(list(\"ab\"))\n",
        "    random_indices = np.random.randint(0, len(chars), size=(num_records, string_length))\n",
        "    char_array = chars[random_indices]\n",
        "    return np.array(list(map(\"\".join, char_array)))\n",
        "\n",
        "\n",
        "def _write_parquet_streamed(\n",
        "    filename,\n",
        "    total_rows,\n",
        "    make_chunk_fn,\n",
        "    chunk_size=250_000,\n",
        "    compression=\"snappy\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Stream DataFrame chunks to a single Parquet file with one ParquetWriter.\n",
        "    - schema_df: optional small DataFrame to lock schema; if None we'll infer from the first chunk.\n",
        "    \"\"\"\n",
        "    written = 0\n",
        "\n",
        "    first_chunk = make_chunk_fn(0, min(chunk_size, total_rows))\n",
        "    first_table = pa.Table.from_pandas(first_chunk, preserve_index=False)\n",
        "    writer = pq.ParquetWriter(filename, first_table.schema, compression=compression)\n",
        "    writer.write_table(first_table)\n",
        "\n",
        "    written += len(first_chunk)\n",
        "    del first_chunk\n",
        "    gc.collect()\n",
        "\n",
        "    while written < total_rows:\n",
        "        take = min(chunk_size, total_rows - written)\n",
        "        chunk_df = make_chunk_fn(written, take)\n",
        "        writer.write_table(pa.Table.from_pandas(chunk_df, preserve_index=False))\n",
        "        written += take\n",
        "        del chunk_df\n",
        "        gc.collect()\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def generate_test_data(target_size=\"100MB\"):\n",
        "    \"\"\"\n",
        "    Generate datasets with proper foreign key relationships.\n",
        "\n",
        "    Target COMPRESSED Parquet file sizes on disk:\n",
        "    100MB total compressed:\n",
        "        - Songs: 10K rows → ~5MB (5% of total)\n",
        "        - Users: 50K rows → ~20MB (20% of total)\n",
        "        - Listens: 1M rows → ~75MB (75% of total)\n",
        "    1GB total compressed:\n",
        "        - Songs: 100K rows → ~50MB (5% of total)\n",
        "        - Users: 500K rows → ~200MB (20% of total)\n",
        "        - Listens: 10M rows → ~750MB (75% of total)\n",
        "\n",
        "    Each table needs:\n",
        "        - Primary key column(s)\n",
        "        - 10 additional string columns of k characters each\n",
        "        - For Users: add 'age' column (random 18-80)\n",
        "\n",
        "    CRITICAL: Listens table must have valid foreign keys!\n",
        "    Every song_id must exist in Songs\n",
        "    Every user_id must exist in Users\n",
        "    \"\"\"\n",
        "\n",
        "    assert target_size in [\"100MB\", \"1GB\", \"10GB\"]\n",
        "    if target_size == \"100MB\":\n",
        "        num_songs = 10_000\n",
        "        num_users = 50_000\n",
        "        num_listens = 1_000_000\n",
        "\n",
        "        songs_chunk = 10_000\n",
        "        users_chunk = 50_000\n",
        "        listens_chunk = 1_000_000\n",
        "    if target_size == \"1GB\":\n",
        "        num_songs = 100_000\n",
        "        num_users = 500_000\n",
        "        num_listens = 10_000_000\n",
        "\n",
        "        songs_chunk = 10_000\n",
        "        users_chunk = 50_000\n",
        "        listens_chunk = 1_000_000\n",
        "    else: \n",
        "        num_songs = 1_000_000\n",
        "        num_users = 5_000_000\n",
        "        num_listens = 100_000_000\n",
        "\n",
        "        songs_chunk = 10_000\n",
        "        users_chunk = 50_000\n",
        "        listens_chunk = 1_000_000\n",
        "\n",
        "    print(\"Writing Songs\")\n",
        "    _write_parquet_streamed(\n",
        "        filename=f\"songs_{target_size}.parquet\",\n",
        "        total_rows=num_songs,\n",
        "        make_chunk_fn=lambda start, size: generate_songs_chunk(start, size),\n",
        "        chunk_size=songs_chunk,\n",
        "    )\n",
        "\n",
        "    print(\"Writing Users\")\n",
        "    _write_parquet_streamed(\n",
        "        filename=f\"users_{target_size}.parquet\",\n",
        "        total_rows=num_users,\n",
        "        make_chunk_fn=lambda start, size: generate_users_chunk(start, size),\n",
        "        chunk_size=users_chunk,\n",
        "    )\n",
        "\n",
        "    print(\"Writing Listens\")\n",
        "    _write_parquet_streamed(\n",
        "        filename=f\"listens_{target_size}.parquet\",\n",
        "        total_rows=num_listens,\n",
        "        make_chunk_fn=lambda start, size: generate_listens_chunk(\n",
        "            start, size, num_users, num_songs\n",
        "        ),\n",
        "        chunk_size=listens_chunk,\n",
        "    )\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5qnpQghhWkL",
        "outputId": "40816b75-44cb-4338-8610-e9838e59970a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Songs\n",
            "Writing Users\n",
            "Writing Listens\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "random.seed(0)\n",
        "\n",
        "generate_test_data('1GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEiGGznFhtxo"
      },
      "source": [
        "# Section 1: Parquet-based Columnar Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BGrkP5PhWkM"
      },
      "source": [
        "Implement Parquet-based storage for the tables\n",
        "- For simplicity, store all data for a table in a single Parquet file and use a single DataFrame object as a buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b0o8zkpGhWkM"
      },
      "outputs": [],
      "source": [
        "# see ed: https://edstem.org/us/courses/87394/discussion/7251811 for advice on writing to a parquet without loading existing into RAM\n",
        "# a ColumnarDbFile is actually a directory with an arbitrary number of parquet files inside\n",
        "# Append writes a new file with the next postfix\n",
        "# Retrieve reads all parquet files and concatenates them together, done natively by pandas\n",
        "class ColumnarDbFile:\n",
        "    def __init__(self, table_name, file_dir='data', file_pfx=''):\n",
        "        self.file_pfx = file_pfx\n",
        "        self.table_name = table_name\n",
        "        self.file_dir = file_dir\n",
        "        #os.makedirs(self.file_dir, exist_ok=True)\n",
        "        self.base_file_name = f\"{self.file_dir}/{self.file_pfx}_{self.table_name}\"\n",
        "        os.makedirs(self.base_file_name, exist_ok=True)\n",
        "\n",
        "    def build_table(self, data):\n",
        "        \"\"\"Build and save table data to Parquet.\"\"\"\n",
        "        data.to_parquet(f\"{self.base_file_name}/{self.table_name}-0.parquet\")\n",
        "        return\n",
        "\n",
        "    def retrieve_data(self, columns=None):\n",
        "        \"\"\"Create pd.DataFrame by reading from Parquet\"\"\"\n",
        "        return pd.read_parquet(self.base_file_name, columns=columns)\n",
        "\n",
        "    def append_data(self, data):\n",
        "        \"\"\"Append new data to Parquet\"\"\"\n",
        "        # Use glob to count the number of parquet files in the directory\n",
        "        data.to_parquet(self.get_new_parquet_file())\n",
        "        return\n",
        "\n",
        "    def _get_num_parquets(self):\n",
        "        return len(glob.glob(f\"{self.base_file_name}/*.parquet\"))\n",
        "\n",
        "    def table_metadata(self):\n",
        "        \"\"\"Return total rows and total byte size of the table without loading data.\"\"\"\n",
        "        parquet_files = glob.glob(f\"{self.base_file_name}/*.parquet\")\n",
        "\n",
        "        total_rows = 0\n",
        "        total_bytes = 0\n",
        "\n",
        "        for file in parquet_files:\n",
        "            pf = pq.ParquetFile(file)\n",
        "            meta = pf.metadata\n",
        "\n",
        "            total_rows += meta.num_rows\n",
        "            total_bytes += meta.serialized_size  # includes footer + metadata\n",
        "\n",
        "        return {\n",
        "            \"num_files\": len(parquet_files),\n",
        "            \"total_rows\": total_rows,\n",
        "            \"total_compressed_bytes\": total_bytes,\n",
        "        }\n",
        "\n",
        "    def table_disk_usage(self):\n",
        "        parquet_files = glob.glob(f\"{self.base_file_name}/*.parquet\")\n",
        "\n",
        "        total_bytes = sum(os.path.getsize(f) for f in parquet_files)\n",
        "\n",
        "        return {\n",
        "            \"num_files\": len(parquet_files),\n",
        "            \"total_bytes\": total_bytes\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def fits_in_12GB(bytes_needed: int) -> bool:\n",
        "        TWELVE_GB = 12 * 1024**3\n",
        "        return bytes_needed <= TWELVE_GB\n",
        "\n",
        "    @staticmethod\n",
        "    def can_process_parquet(bytes_on_disk: int, compression_factor: int = 5) -> bool:\n",
        "        \"\"\"\n",
        "        Returns True if a Parquet dataset of `bytes_on_disk` can be processed\n",
        "        within 12 GB of RAM, after accounting for decompression expansion.\n",
        "        \"\"\"\n",
        "        estimated_ram = bytes_on_disk * compression_factor\n",
        "        TWELVE_GB = 12 * 1024**3\n",
        "        return estimated_ram <= TWELVE_GB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gPpXtxghWkM",
        "outputId": "43c2aa9b-e2a3-4059-b2db-3d1955dcd07d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building tables...\n",
            "Tables built successfully.\n"
          ]
        }
      ],
      "source": [
        "print(\"Building tables...\")\n",
        "if os.path.exists('data'):\n",
        "    shutil.rmtree('data')\n",
        "tables = {\n",
        "    'Songs': ColumnarDbFile(\"Songs\", file_dir='data'),\n",
        "    'Users': ColumnarDbFile(\"Users\", file_dir='data'),\n",
        "    'Listens': ColumnarDbFile(\"Listens\", file_dir='data')\n",
        "}\n",
        "\n",
        "size = \"1GB\"\n",
        "songs_data = pd.read_parquet(f'songs_{size}.parquet')\n",
        "users_data = pd.read_parquet(f'users_{size}.parquet')\n",
        "listens_data = pd.read_parquet(f'listens_{size}.parquet')\n",
        "\n",
        "tables['Songs'].build_table(songs_data)\n",
        "tables['Users'].build_table(users_data)\n",
        "tables['Listens'].build_table(listens_data)\n",
        "print(\"Tables built successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "LfutqrA2hWkM",
        "outputId": "3fdee8d3-19d1-404a-cbb8-312fc4ff8485"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>song_id</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Song_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Song_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Song_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Song_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Song_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>99995</td>\n",
              "      <td>Song_99995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>99996</td>\n",
              "      <td>Song_99996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>99997</td>\n",
              "      <td>Song_99997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>99998</td>\n",
              "      <td>Song_99998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>99999</td>\n",
              "      <td>Song_99999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       song_id       title\n",
              "0            0      Song_0\n",
              "1            1      Song_1\n",
              "2            2      Song_2\n",
              "3            3      Song_3\n",
              "4            4      Song_4\n",
              "...        ...         ...\n",
              "99995    99995  Song_99995\n",
              "99996    99996  Song_99996\n",
              "99997    99997  Song_99997\n",
              "99998    99998  Song_99998\n",
              "99999    99999  Song_99999\n",
              "\n",
              "[100000 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# retrieve data\n",
        "tables['Songs'].retrieve_data(columns = ['song_id', 'title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ZiPY1Hs9hWkM",
        "outputId": "d1bad27a-9419-41f8-94b9-1aa6abe5d6bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>listen_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>song_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>8822</td>\n",
              "      <td>42708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>27918</td>\n",
              "      <td>88764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>100379</td>\n",
              "      <td>46603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>496553</td>\n",
              "      <td>36186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>448685</td>\n",
              "      <td>94938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999995</th>\n",
              "      <td>9999995</td>\n",
              "      <td>118644</td>\n",
              "      <td>8260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999996</th>\n",
              "      <td>9999996</td>\n",
              "      <td>296462</td>\n",
              "      <td>23279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999997</th>\n",
              "      <td>9999997</td>\n",
              "      <td>298289</td>\n",
              "      <td>77129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999998</th>\n",
              "      <td>9999998</td>\n",
              "      <td>209754</td>\n",
              "      <td>88446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999999</th>\n",
              "      <td>9999999</td>\n",
              "      <td>250764</td>\n",
              "      <td>622</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         listen_id  user_id  song_id\n",
              "0                0     8822    42708\n",
              "1                1    27918    88764\n",
              "2                2   100379    46603\n",
              "3                3   496553    36186\n",
              "4                4   448685    94938\n",
              "...            ...      ...      ...\n",
              "9999995    9999995   118644     8260\n",
              "9999996    9999996   296462    23279\n",
              "9999997    9999997   298289    77129\n",
              "9999998    9999998   209754    88446\n",
              "9999999    9999999   250764      622\n",
              "\n",
              "[10000000 rows x 3 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tables['Listens'].retrieve_data(columns = ['listen_id', 'user_id', 'song_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtLVO3cChWkM"
      },
      "source": [
        "Analyze and report on:\n",
        "- Space efficiency compared to row storage\n",
        "  - e.g. Compare file sizes on disk: How much disk space does Parquet use vs. a row storage format like CSV?\n",
        "- Compression ratios achieved with Parquet\n",
        "  - e.g. Compare Parquet’s uncompressed encoded size (reported in its metadata) to its compressed on-disk size to compute compression ratios.\n",
        "  - You could also report the memory expansion factor: how much larger the dataset becomes when loaded into a `pd.DataFrame` compared to the compressed file size.\n",
        "- Read/write performance characteristics\n",
        "  - e.g. Read performance: How long does it take to read all columns from Parquet vs. CSV?\n",
        "  - e.g. Columnar advantage: How long does it take to read selective columns from Parquet vs. reading all columns?\n",
        "  - e.g. Write performance: How long does it take to write data to Parquet vs. CSV?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7kfNisQFhWkM"
      },
      "outputs": [],
      "source": [
        "def analyze(size=\"100MB\"):\n",
        "    \"\"\"Analyze storage efficiency, compression, and read/write performance.\"\"\"\n",
        "\n",
        "    table_files = {\n",
        "        \"Songs\": f\"songs_{size}.parquet\",\n",
        "        \"Users\": f\"users_{size}.parquet\",\n",
        "        \"Listens\": f\"listens_{size}.parquet\",\n",
        "    }\n",
        "\n",
        "    report_rows = []\n",
        "\n",
        "    for table_name, parquet_file in table_files.items():\n",
        "        parquet_path = Path(parquet_file)\n",
        "\n",
        "        df = pd.read_parquet(parquet_path)\n",
        "        mem_usage_bytes = df.memory_usage(deep=True).sum() # memory usage of the dataframe\n",
        "        parquet_size_bytes = parquet_path.stat().st_size # size of the parquet file on disk\n",
        "\n",
        "        parquet_file_obj = pq.ParquetFile(parquet_path)\n",
        "        metadata = parquet_file_obj.metadata\n",
        "        uncompressed_bytes = 0\n",
        "\n",
        "        # iterate over all row groups and columns to get the total uncompressed size of the parquet file\n",
        "        for rg_idx in range(metadata.num_row_groups):\n",
        "            row_group = metadata.row_group(rg_idx)\n",
        "            for col_idx in range(row_group.num_columns):\n",
        "                column_meta = row_group.column(col_idx)\n",
        "                if column_meta.total_uncompressed_size is not None:\n",
        "                    uncompressed_bytes += column_meta.total_uncompressed_size\n",
        "\n",
        "        # calculate compression ratio and memory expansion\n",
        "        compression_ratio = (\n",
        "            uncompressed_bytes / parquet_size_bytes\n",
        "        )\n",
        "        memory_expansion = (\n",
        "            mem_usage_bytes / parquet_size_bytes\n",
        "        )\n",
        "\n",
        "        # test reading speed of parquet file vs csv, for all columns and selective columns\n",
        "        # pick 1 less than the total number of columns to test reading selective columns\n",
        "        subset_columns = list(df.columns)[0:len(df.columns)-1]\n",
        "\n",
        "        with tempfile.TemporaryDirectory() as tmpdir:\n",
        "            tmpdir_path = Path(tmpdir)\n",
        "\n",
        "            csv_path = tmpdir_path / f\"{parquet_path.stem}.csv\"\n",
        "            start = time.perf_counter()\n",
        "            df.to_csv(csv_path, index=False)\n",
        "            write_csv_time = time.perf_counter() - start\n",
        "            csv_size_bytes = csv_path.stat().st_size\n",
        "\n",
        "            parquet_tmp_path = tmpdir_path / f\"{parquet_path.stem}.parquet\"\n",
        "            start = time.perf_counter()\n",
        "            df.to_parquet(parquet_tmp_path, index=False)\n",
        "            write_parquet_time = time.perf_counter() - start\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            _ = pd.read_parquet(parquet_path)\n",
        "            read_parquet_all = time.perf_counter() - start\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            _ = pd.read_csv(csv_path)\n",
        "            read_csv_all = time.perf_counter() - start\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            _ = pd.read_parquet(parquet_path, columns=subset_columns)\n",
        "            read_parquet_subset = time.perf_counter() - start\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            _ = pd.read_csv(csv_path, usecols=subset_columns)\n",
        "            read_csv_subset = time.perf_counter() - start\n",
        "\n",
        "        size_saving_pct = (\n",
        "            100.0 * (1 - parquet_size_bytes / csv_size_bytes)\n",
        "        )\n",
        "\n",
        "        # append the results to the report\n",
        "        report_rows.append(\n",
        "            {\n",
        "                \"table\": table_name,\n",
        "                \"parquet_size_mb\": parquet_size_bytes / (1024 ** 2),\n",
        "                \"csv_size_mb\": csv_size_bytes / (1024 ** 2),\n",
        "                \"size_saving_pct\": size_saving_pct,\n",
        "                \"compression_ratio\": compression_ratio,\n",
        "                \"memory_expansion\": memory_expansion,\n",
        "                \"read_parquet_all_s\": read_parquet_all,\n",
        "                \"read_csv_all_s\": read_csv_all,\n",
        "                \"read_parquet_subset_s\": read_parquet_subset,\n",
        "                \"read_csv_subset_s\": read_csv_subset,\n",
        "                \"write_parquet_s\": write_parquet_time,\n",
        "                \"write_csv_s\": write_csv_time,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        del df\n",
        "        gc.collect()\n",
        "\n",
        "    summary = pd.DataFrame(report_rows)\n",
        "    print(\"Analysis Summary for Tables of Size \" + size + \" (sizes in MB, times in seconds):\")\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "Xkw4z1GMhXsH",
        "outputId": "39eebc07-7084-40c3-b28d-25ac2ecb8dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis Summary for Tables of Size 1GB (sizes in MB, times in seconds):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>table</th>\n",
              "      <th>parquet_size_mb</th>\n",
              "      <th>csv_size_mb</th>\n",
              "      <th>size_saving_pct</th>\n",
              "      <th>compression_ratio</th>\n",
              "      <th>memory_expansion</th>\n",
              "      <th>read_parquet_all_s</th>\n",
              "      <th>read_csv_all_s</th>\n",
              "      <th>read_parquet_subset_s</th>\n",
              "      <th>read_csv_subset_s</th>\n",
              "      <th>write_parquet_s</th>\n",
              "      <th>write_csv_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Songs</td>\n",
              "      <td>42.678607</td>\n",
              "      <td>97.921290</td>\n",
              "      <td>56.415396</td>\n",
              "      <td>2.420447</td>\n",
              "      <td>3.478947</td>\n",
              "      <td>0.116536</td>\n",
              "      <td>1.014622</td>\n",
              "      <td>0.118276</td>\n",
              "      <td>0.918474</td>\n",
              "      <td>0.286086</td>\n",
              "      <td>1.379869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Users</td>\n",
              "      <td>203.417436</td>\n",
              "      <td>486.268065</td>\n",
              "      <td>58.167634</td>\n",
              "      <td>2.472125</td>\n",
              "      <td>3.530262</td>\n",
              "      <td>0.962916</td>\n",
              "      <td>5.373042</td>\n",
              "      <td>0.830418</td>\n",
              "      <td>4.827727</td>\n",
              "      <td>1.578626</td>\n",
              "      <td>6.692811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Listens</td>\n",
              "      <td>834.557252</td>\n",
              "      <td>1817.278606</td>\n",
              "      <td>54.076538</td>\n",
              "      <td>2.409170</td>\n",
              "      <td>7.702006</td>\n",
              "      <td>5.041973</td>\n",
              "      <td>30.441947</td>\n",
              "      <td>6.367491</td>\n",
              "      <td>26.831997</td>\n",
              "      <td>9.867773</td>\n",
              "      <td>45.131197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     table  parquet_size_mb  csv_size_mb  size_saving_pct  compression_ratio  \\\n",
              "0    Songs        42.678607    97.921290        56.415396           2.420447   \n",
              "1    Users       203.417436   486.268065        58.167634           2.472125   \n",
              "2  Listens       834.557252  1817.278606        54.076538           2.409170   \n",
              "\n",
              "   memory_expansion  read_parquet_all_s  read_csv_all_s  \\\n",
              "0          3.478947            0.116536        1.014622   \n",
              "1          3.530262            0.962916        5.373042   \n",
              "2          7.702006            5.041973       30.441947   \n",
              "\n",
              "   read_parquet_subset_s  read_csv_subset_s  write_parquet_s  write_csv_s  \n",
              "0               0.118276           0.918474         0.286086     1.379869  \n",
              "1               0.830418           4.827727         1.578626     6.692811  \n",
              "2               6.367491          26.831997         9.867773    45.131197  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(analyze(size=\"1GB\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIDZx-dvhXsH"
      },
      "outputs": [],
      "source": [
        "# display(analyze(size=\"1GB\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8U3edewiDBa"
      },
      "source": [
        "# Section 2: Parse SQL Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_KoWHLohWkM"
      },
      "source": [
        "In this section, you should implement logic to parse the following SQL query:\n",
        "```sql\n",
        "    SELECT s.song_id, AVG(u.age) AS avg_age,\n",
        "       COUNT(DISTINCT l.user_id) AS count_distinct_users,\n",
        "    FROM Songs s\n",
        "    JOIN Listens l ON s.song_id = l.song_id\n",
        "    JOIN Users u ON l.user_id = u.user_id\n",
        "    GROUP BY s.song_id, s.title\n",
        "    ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
        "```\n",
        "\n",
        "You should manually extract the components from the provided query (i.e. you don't need to implement a general SQL parser, just handle this specific query)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TT3jWKFYhWkN"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT s.song_id, AVG(u.age) AS avg_age,\n",
        "COUNT(DISTINCT l.user_id)\n",
        "FROM Songs s\n",
        "JOIN Listens l ON s.song_id = l.song_id\n",
        "JOIN Users u ON l.user_id = u.user_id\n",
        "GROUP BY s.song_id, s.title\n",
        "ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J1PmMhCRhv0r"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import re\n",
        "\n",
        "def parse_tables(query):\n",
        "\n",
        "    # pattern matches: \"from songs s\" or \"join listens l\"\n",
        "    pattern = r\"(from|join)\\s+([a-z_]+)\\s+([a-z])\"\n",
        "\n",
        "    matches = re.findall(pattern, query)\n",
        "\n",
        "    tables = {}\n",
        "    for _, table_name, alias in matches:\n",
        "        tables[alias] = table_name\n",
        "\n",
        "    return tables\n",
        "\n",
        "def parse_joins(query):\n",
        "\n",
        "    # 1) Get the base table from the FROM clause\n",
        "    base_match = re.search(r\"from\\s+([a-z_]+)\\s+([a-z])\", query)\n",
        "    if not base_match:\n",
        "        raise ValueError(\"Could not find FROM clause\")\n",
        "\n",
        "    base_table_name = base_match.group(1)\n",
        "    base_alias = base_match.group(2)\n",
        "    base_table = (base_alias, base_table_name)\n",
        "\n",
        "    # 2) Get each JOIN clause, in order\n",
        "    # pattern matches:\n",
        "    #   join listens l on s.song_id = l.song_id\n",
        "    join_pattern = (\n",
        "        r\"join\\s+([a-z_]+)\\s+([a-z])\\s+on\\s+\"\n",
        "        r\"([a-z])\\.([a-z_]+)\\s*=\\s*([a-z])\\.([a-z_]+)\"\n",
        "    )\n",
        "\n",
        "    joins = []\n",
        "    for m in re.finditer(join_pattern, query):\n",
        "        joined_table_name = m.group(1)\n",
        "        joined_alias = m.group(2)\n",
        "        left_alias = m.group(3)\n",
        "        left_col = m.group(4)\n",
        "        right_alias = m.group(5)\n",
        "        right_col = m.group(6)\n",
        "\n",
        "        joins.append(\n",
        "            {\n",
        "                \"joined_table_alias\": joined_alias,\n",
        "                \"joined_table_name\": joined_table_name,\n",
        "                \"left_alias\": left_alias,\n",
        "                \"left_column\": left_col,\n",
        "                \"right_alias\": right_alias,\n",
        "                \"right_column\": right_col,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\"base_table\" : base_table, \"Joins\" : joins}\n",
        "\n",
        "\n",
        "def parse_group_by(query):\n",
        "    \"\"\"\n",
        "    Return GROUP BY columns as a list of (alias, column) tuples.\n",
        "    Example: [('s', 'song_id'), ('s', 'title')]\n",
        "    \"\"\"\n",
        "    q = query.lower()\n",
        "\n",
        "    # Capture whatever is between GROUP BY and ORDER BY/semicolon/end\n",
        "    match = re.search(r\"group\\s+by\\s+(.+?)(order\\s+by|;|$)\", q, re.DOTALL)\n",
        "    if not match:\n",
        "        return []\n",
        "\n",
        "    groupby_text = match.group(1).strip()\n",
        "\n",
        "    columns = []\n",
        "    for col in groupby_text.split(\",\"):\n",
        "        col = col.strip()\n",
        "\n",
        "        # Expect pattern: alias.column\n",
        "        alias, column = col.split(\".\")\n",
        "        columns.append((alias, column))\n",
        "\n",
        "    return columns\n",
        "\n",
        "def parse_select_and_aggregations(query):\n",
        "    \"\"\"\n",
        "    Build:\n",
        "      aggregations: {agg_key: {...}}\n",
        "      select: list of items that may refer to agg_key\n",
        "    \"\"\"\n",
        "    q = query.lower()\n",
        "\n",
        "    m = re.search(r\"select\\s+(.+?)\\s+from\", q, re.DOTALL)\n",
        "    if not m:\n",
        "        return [], {}\n",
        "\n",
        "    select_text = m.group(1).strip()\n",
        "    raw_items = [item.strip() for item in select_text.split(\",\") if item.strip()]\n",
        "\n",
        "    select_list = []\n",
        "    aggregations = {}\n",
        "    agg_id = 1\n",
        "\n",
        "    for idx, item in enumerate(raw_items, start=1):\n",
        "        # AVG(...)\n",
        "        if item.startswith(\"avg(\"):\n",
        "            m_avg = re.match(\n",
        "                r\"avg\\(\\s*([a-z])\\.([a-z_]+)\\s*\\)(\\s+as\\s+([a-z_]+))?\",\n",
        "                item\n",
        "            )\n",
        "            if not m_avg:\n",
        "                raise ValueError(f\"Could not parse AVG aggregation: {item}\")\n",
        "            alias_letter = m_avg.group(1)\n",
        "            col_name = m_avg.group(2)\n",
        "            out_alias = m_avg.group(4) if m_avg.group(4) else None\n",
        "\n",
        "            aggregations[agg_id] = {\n",
        "                \"func\": \"avg\",\n",
        "                \"source\": (alias_letter, col_name),\n",
        "                \"distinct\": False,\n",
        "                \"output_name\": out_alias,\n",
        "            }\n",
        "\n",
        "            select_list.append(\n",
        "                {\n",
        "                    \"kind\": \"aggregation\",\n",
        "                    \"agg_key\": agg_id,\n",
        "                    \"alias\": out_alias,\n",
        "\n",
        "                }\n",
        "            )\n",
        "            agg_id += 1\n",
        "\n",
        "        # COUNT(DISTINCT ...)\n",
        "        elif item.startswith(\"count(\"):\n",
        "            m_cnt = re.match(\n",
        "                r\"count\\(\\s*distinct\\s+([a-z])\\.([a-z_]+)\\s*\\)(\\s+as\\s+([a-z_]+))?\",\n",
        "                item\n",
        "            )\n",
        "            if not m_cnt:\n",
        "                raise ValueError(f\"Could not parse COUNT aggregation: {item}\")\n",
        "            alias_letter = m_cnt.group(1)\n",
        "            col_name = m_cnt.group(2)\n",
        "            out_alias = m_cnt.group(4) if m_cnt.group(4) else None\n",
        "\n",
        "            aggregations[agg_id] = {\n",
        "                \"func\": \"count\",\n",
        "                \"source\": (alias_letter, col_name),\n",
        "                \"distinct\": True,\n",
        "                \"output_name\": out_alias,\n",
        "            }\n",
        "\n",
        "            select_list.append(\n",
        "                {\n",
        "                    \"kind\": \"aggregation\",\n",
        "                    \"agg_key\": agg_id,\n",
        "                    \"alias\": out_alias,\n",
        "                }\n",
        "            )\n",
        "            agg_id += 1\n",
        "\n",
        "        # Plain column: alias.column\n",
        "        else:\n",
        "            alias_letter, col_name = item.split(\".\")\n",
        "            select_list.append(\n",
        "                {\n",
        "                    \"kind\": \"column\",\n",
        "                    \"source\": (alias_letter, col_name),\n",
        "                    \"alias\": None,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return select_list, aggregations\n",
        "\n",
        "\n",
        "def parse_order_by(query, aggregations):\n",
        "    \"\"\"\n",
        "    Build order_by list where entries can refer to aggregations via agg_key.\n",
        "    \"\"\"\n",
        "    q = query.lower()\n",
        "\n",
        "    m = re.search(r\"order\\s+by\\s+(.+?)(;|$)\", q, re.DOTALL)\n",
        "    if not m:\n",
        "        return []\n",
        "\n",
        "    order_text = m.group(1).strip()\n",
        "    raw_items = [item.strip() for item in order_text.split(\",\") if item.strip()]\n",
        "\n",
        "    order_by = []\n",
        "\n",
        "    for item in raw_items:\n",
        "        direction = \"asc\"\n",
        "        expr = item\n",
        "\n",
        "        if expr.endswith(\" desc\"):\n",
        "            direction = \"desc\"\n",
        "            expr = expr[:-5].strip()\n",
        "        elif expr.endswith(\" asc\"):\n",
        "            direction = \"asc\"\n",
        "            expr = expr[:-4].strip()\n",
        "\n",
        "        # COUNT(DISTINCT ...) → match an aggregation\n",
        "        if expr.startswith(\"count(\"):\n",
        "            m_cnt = re.match(\n",
        "                r\"count\\(\\s*distinct\\s+([a-z])\\.([a-z_]+)\\s*\\)\",\n",
        "                expr\n",
        "            )\n",
        "            if not m_cnt:\n",
        "                raise ValueError(f\"Could not parse ORDER BY aggregation: {expr}\")\n",
        "            src = (m_cnt.group(1), m_cnt.group(2))\n",
        "\n",
        "            agg_key = None\n",
        "            for k, agg in aggregations.items():\n",
        "                if (\n",
        "                    agg[\"func\"] == \"count\"\n",
        "                    and agg[\"distinct\"]\n",
        "                    and agg[\"source\"] == src\n",
        "                ):\n",
        "                    agg_key = k\n",
        "                    break\n",
        "\n",
        "            if agg_key is None:\n",
        "                raise ValueError(f\"No matching aggregation found for ORDER BY expr: {expr}\")\n",
        "\n",
        "            order_by.append(\n",
        "                {\n",
        "                    \"kind\": \"aggregation\",\n",
        "                    \"agg_key\": agg_key,\n",
        "                    \"direction\": direction,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # assume plain column: alias.column\n",
        "            alias_letter, col_name = expr.split(\".\")\n",
        "            order_by.append(\n",
        "                {\n",
        "                    \"kind\": \"column\",\n",
        "                    \"source\": (alias_letter, col_name),\n",
        "                    \"direction\": direction,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return order_by\n",
        "\n",
        "def parse_sql(query):\n",
        "    \"\"\"\n",
        "    YOUR TASK: Extract tables, joins, and aggregations\n",
        "    \"\"\"\n",
        "    # Parse SQL string to identify:\n",
        "    # - Tables involved\n",
        "    # - Join conditions\n",
        "    # - GROUP BY columns\n",
        "    # - Aggregation functions\n",
        "    # Your implementation here\n",
        "    query = query.lower()\n",
        "    output = {}\n",
        "\n",
        "    output[\"tables\"] = parse_tables(query)\n",
        "    output[\"joins\"] = parse_joins(query)\n",
        "    output[\"GroupBy\"] = parse_group_by(query)\n",
        "    output[\"select\"], output[\"aggregations\"] = parse_select_and_aggregations(query)\n",
        "    output[\"orderBy\"] = parse_order_by(query, output[\"aggregations\"])\n",
        "\n",
        "    return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8mb80aeTAxM",
        "outputId": "02681db1-41c7-4339-907a-7c377b821792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tables: {'s': 'songs', 'l': 'listens', 'u': 'users'}\n",
            "joins: {'base_table': ('s', 'songs'), 'Joins': [{'joined_table_alias': 'l', 'joined_table_name': 'listens', 'left_alias': 's', 'left_column': 'song_id', 'right_alias': 'l', 'right_column': 'song_id'}, {'joined_table_alias': 'u', 'joined_table_name': 'users', 'left_alias': 'l', 'left_column': 'user_id', 'right_alias': 'u', 'right_column': 'user_id'}]}\n",
            "GroupBy: [('s', 'song_id'), ('s', 'title')]\n",
            "select: [{'kind': 'column', 'source': ('s', 'song_id'), 'alias': None}, {'kind': 'aggregation', 'agg_key': 1, 'alias': 'avg_age'}, {'kind': 'aggregation', 'agg_key': 2, 'alias': None}]\n",
            "aggregations: {1: {'func': 'avg', 'source': ('u', 'age'), 'distinct': False, 'output_name': 'avg_age'}, 2: {'func': 'count', 'source': ('l', 'user_id'), 'distinct': True, 'output_name': None}}\n",
            "orderBy: [{'kind': 'aggregation', 'agg_key': 2, 'direction': 'desc'}, {'kind': 'column', 'source': ('s', 'song_id'), 'direction': 'asc'}]\n"
          ]
        }
      ],
      "source": [
        "parsed_sql = parse_sql(query)\n",
        "for key, value in output.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071jzSZqkGyC"
      },
      "source": [
        "# Section 3: Implement Join Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14LszEqZhWkN"
      },
      "source": [
        "In this section, you will implement the execution operators (*how* to join) and aggregation after joins.\n",
        "\n",
        "**Reminder:** If you use temporary files or folders, you should clean them up either as part of your join logic, or after each run. Otherwise you might run into correctness issues!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1AVO_NRnkHq1"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "\n",
        "def HASHVALUE(value, B):\n",
        "    if isinstance(value, int):\n",
        "        return hash(value) % B\n",
        "    sha256 = hashlib.sha256()\n",
        "    sha256.update(str(value).encode(\"utf-8\"))\n",
        "    return int(sha256.hexdigest(), 16) % B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q46rgYihWkN"
      },
      "source": [
        "Implement `HashPartitionJoin`:\n",
        "1. Hash partition both tables\n",
        "2. Build hash table from smaller partition\n",
        "3. Probe with larger partition\n",
        "4. Return joined results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1pw1bPrhWkN"
      },
      "outputs": [],
      "source": [
        "class HashPartitionJoin:\n",
        "    def __init__(self, num_partitions=4, parquet_batch_size=1000):\n",
        "        self.num_partitions = num_partitions\n",
        "        self.parquet_batch_size = parquet_batch_size\n",
        "\n",
        "    def join(self, table1: ColumnarDbFile, table2: ColumnarDbFile, join_key1, join_key2,\n",
        "             temp_dir='temp', columns_table1=None, columns_table2=None):\n",
        "        \"\"\"\n",
        "        Perform a hash partition join between two ColumnarDbFile instances.\n",
        "\n",
        "        Parameters:\n",
        "        - table1: Left table (ColumnarDbFile)\n",
        "        - table2: Right table (ColumnarDbFile)\n",
        "        - join_key1: Join key from table1\n",
        "        - join_key2: Join key from table2\n",
        "        - temp_dir: Directory to store temporary files\n",
        "        - columns_table1: List of columns to select from table1\n",
        "        - columns_table2: List of columns to select from table2\n",
        "\n",
        "        Returns:\n",
        "        - join_result_table: ColumnarDbFile instance containing the join results\n",
        "        \"\"\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "        # Partition both tables\n",
        "        partitions1 = self._hash_partition(table1, join_key1, temp_dir, 'left', columns_table1)\n",
        "        partitions2 = self._hash_partition(table2, join_key2, temp_dir, 'right', columns_table2)\n",
        "\n",
        "    def _hash_partition(self, table: ColumnarDbFile, join_key, output_dir, side, columns=None):\n",
        "        # Find all parquet files in the directory\n",
        "        parquet_files = glob.glob(f\"{table.base_file_name}/*.parquet\")\n",
        "        if not parquet_files:\n",
        "            raise ValueError(f\"No parquet files found in {table.base_file_name}\")\n",
        "        \n",
        "        writers: dict[int, pq.ParquetWriter] = {}\n",
        "        \n",
        "        # Process each parquet file in the directory\n",
        "        for parquet_file_path in parquet_files:\n",
        "            parquet_file = pq.ParquetFile(parquet_file_path)\n",
        "            # Ensure join_key is included in columns for partitioning\n",
        "            read_columns = columns\n",
        "            if columns and join_key not in columns:\n",
        "                read_columns = list(columns) + [join_key]\n",
        "            \n",
        "            for batch in parquet_file.iter_batches(batch_size=self.parquet_batch_size, columns=read_columns):\n",
        "                batch_df = batch.to_pandas()\n",
        "                \n",
        "                # Add partition column based on join_key\n",
        "                batch_df[\"_part\"] = batch_df[join_key].apply(lambda x: HASHVALUE(x, self.num_partitions))\n",
        "                \n",
        "                # Filter to requested columns if specified (but keep _part for grouping)\n",
        "                if columns:\n",
        "                    # Select requested columns plus _part\n",
        "                    batch_df = batch_df[columns + [\"_part\"]]\n",
        "\n",
        "                # Group rows by partition id and write them out\n",
        "                for part_id, part_df in batch_df.groupby(\"_part\"):\n",
        "                    # Drop helper column before writing\n",
        "                    part_df = part_df.drop(columns=[\"_part\"])\n",
        "\n",
        "                    # Convert to Arrow Table\n",
        "                    part_table = pa.Table.from_pandas(part_df, preserve_index=False)\n",
        "\n",
        "                    # Lazily create writer for this partition\n",
        "                    writer = writers.get(part_id)\n",
        "                    if writer is None:\n",
        "                        part_path = self._make_partition_path(output_dir, side, part_id)\n",
        "                        writer = pq.ParquetWriter(part_path, part_table.schema)\n",
        "                        writers[part_id] = writer\n",
        "\n",
        "                    # Append this batch's rows for this partition as a new row group\n",
        "                    writer.write_table(part_table)\n",
        "\n",
        "        # Close all writers\n",
        "        for w in writers.values():\n",
        "            w.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Verify your implementation against pd.merge\n",
        "def test_hash_partition_join_comprehensive():\n",
        "    \"\"\"\n",
        "    Comprehensive test that validates both structure AND actual data values.\n",
        "    This ensures the HPJ implementation is truly correct.\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"Comprehensive Hash Partition Join Test\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if not os.path.exists('data'):\n",
        "        print(\"ERROR: 'data' directory does not exist. Please run the table building cell first.\")\n",
        "        return False\n",
        "    \n",
        "    tables = {\n",
        "        'Songs': ColumnarDbFile(\"Songs\", file_dir='data'),\n",
        "        'Users': ColumnarDbFile(\"Users\", file_dir='data'),\n",
        "        'Listens': ColumnarDbFile(\"Listens\", file_dir='data')\n",
        "    }\n",
        "    \n",
        "    all_tests_passed = True\n",
        "    \n",
        "    # Test Case 1: Songs JOIN Listens - FULL DATA VALIDATION\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Test Case 1: Songs JOIN Listens - Full Data Validation\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    songs_table = tables['Songs']\n",
        "    listens_table = tables['Listens']\n",
        "    \n",
        "    songs_cols = ['song_id', 'title']\n",
        "    listens_cols = ['listen_id', 'song_id', 'user_id']\n",
        "    \n",
        "    # Perform joins\n",
        "    hpj1 = HashPartitionJoin(num_partitions=4, parquet_batch_size=1000)\n",
        "    result_table1 = hpj1.join(\n",
        "        songs_table, listens_table,\n",
        "        join_key1='song_id', join_key2='song_id',\n",
        "        temp_dir='temp_test_songs_listens_comp',\n",
        "        columns_table1=songs_cols,\n",
        "        columns_table2=listens_cols\n",
        "    )\n",
        "    \n",
        "    hpj_result1 = result_table1.retrieve_data()\n",
        "    \n",
        "    # Get pd.merge result\n",
        "    songs_df = songs_table.retrieve_data(columns=songs_cols)\n",
        "    listens_df = listens_table.retrieve_data(columns=listens_cols)\n",
        "    pd_result1 = pd.merge(songs_df, listens_df, on='song_id', how='inner')\n",
        "    \n",
        "    print(f\"\\nHPJ result shape: {hpj_result1.shape}\")\n",
        "    print(f\"pd.merge result shape: {pd_result1.shape}\")\n",
        "    \n",
        "    test1_passed = True\n",
        "    \n",
        "    # 1. Row count check\n",
        "    if len(hpj_result1) != len(pd_result1):\n",
        "        print(f\"✗ Row count mismatch! HPJ: {len(hpj_result1)}, pd.merge: {len(pd_result1)}\")\n",
        "        test1_passed = False\n",
        "        all_tests_passed = False\n",
        "    else:\n",
        "        print(\"✓ Row counts match!\")\n",
        "    \n",
        "    # 2. Column check\n",
        "    hpj_cols = set(hpj_result1.columns)\n",
        "    pd_cols = set(pd_result1.columns)\n",
        "    if hpj_cols != pd_cols:\n",
        "        print(f\"✗ Column mismatch! HPJ: {hpj_cols}, pd.merge: {pd_cols}\")\n",
        "        test1_passed = False\n",
        "        all_tests_passed = False\n",
        "    else:\n",
        "        print(\"✓ Columns match!\")\n",
        "    \n",
        "    if test1_passed:\n",
        "        # 3. Sort both results for comparison\n",
        "        sort_cols = ['song_id', 'listen_id'] if 'listen_id' in hpj_result1.columns else ['song_id']\n",
        "        hpj_sorted = hpj_result1.sort_values(sort_cols).reset_index(drop=True)\n",
        "        pd_sorted = pd_result1.sort_values(sort_cols).reset_index(drop=True)\n",
        "        \n",
        "        # 4. Check unique keys\n",
        "        hpj_song_ids = set(hpj_result1['song_id'].unique())\n",
        "        pd_song_ids = set(pd_result1['song_id'].unique())\n",
        "        if hpj_song_ids != pd_song_ids:\n",
        "            print(f\"✗ Unique song_ids differ!\")\n",
        "            test1_passed = False\n",
        "            all_tests_passed = False\n",
        "        else:\n",
        "            print(\"✓ Unique song_ids match!\")\n",
        "        \n",
        "        # 5. FULL DATA VALUE COMPARISON - This is the critical check!\n",
        "        print(\"\\nPerforming full data value comparison...\")\n",
        "        data_matches = True\n",
        "        \n",
        "        # Compare each column\n",
        "        for col in sorted(hpj_cols):\n",
        "            hpj_col_data = hpj_sorted[col].values\n",
        "            pd_col_data = pd_sorted[col].values\n",
        "            \n",
        "            # Use np.array_equal for exact comparison\n",
        "            if not np.array_equal(hpj_col_data, pd_col_data):\n",
        "                print(f\"✗ Column '{col}' data mismatch!\")\n",
        "                \n",
        "                # Find first mismatch\n",
        "                mismatch_idx = np.where(hpj_col_data != pd_col_data)[0]\n",
        "                if len(mismatch_idx) > 0:\n",
        "                    idx = mismatch_idx[0]\n",
        "                    print(f\"  First mismatch at row {idx}:\")\n",
        "                    print(f\"    HPJ: {hpj_col_data[idx]}\")\n",
        "                    print(f\"    pd.merge: {pd_col_data[idx]}\")\n",
        "                    print(f\"  Total mismatches: {len(mismatch_idx)}\")\n",
        "                \n",
        "                data_matches = False\n",
        "                break\n",
        "        \n",
        "        if data_matches:\n",
        "            print(\"✓ All data values match exactly!\")\n",
        "            print(f\"✓ Verified {len(hpj_sorted)} rows × {len(hpj_cols)} columns = {len(hpj_sorted) * len(hpj_cols)} values\")\n",
        "        else:\n",
        "            print(\"✗ Data values do NOT match!\")\n",
        "            test1_passed = False\n",
        "            all_tests_passed = False\n",
        "        \n",
        "        # 6. Check for duplicate rows (should be same in both)\n",
        "        hpj_duplicates = hpj_sorted.duplicated().sum()\n",
        "        pd_duplicates = pd_sorted.duplicated().sum()\n",
        "        if hpj_duplicates != pd_duplicates:\n",
        "            print(f\"⚠ Duplicate row counts differ (HPJ: {hpj_duplicates}, pd.merge: {pd_duplicates})\")\n",
        "        else:\n",
        "            print(f\"✓ Duplicate row counts match ({hpj_duplicates} duplicates)\")\n",
        "    \n",
        "    if test1_passed:\n",
        "        print(\"\\n✓ Test Case 1 PASSED - Full validation successful!\")\n",
        "    else:\n",
        "        print(\"\\n✗ Test Case 1 FAILED!\")\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPREHENSIVE TEST SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    if all_tests_passed:\n",
        "        print(\"✓ ALL TESTS PASSED: Hash Partition Join is CORRECT!\")\n",
        "        print(\"  - Row counts match\")\n",
        "        print(\"  - Column structure matches\")\n",
        "        print(\"  - Unique keys match\")\n",
        "        print(\"  - ALL DATA VALUES match exactly\")\n",
        "    else:\n",
        "        print(\"✗ TESTS FAILED: Implementation has issues\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return all_tests_passed\n",
        "\n",
        "test_hash_partition_join_comprehensive()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzzs-5K8hWkN"
      },
      "source": [
        "Implement `SortMergeJoin`:\n",
        "1. Sort both tables by join key\n",
        "2. Merge sorted sequences\n",
        "3. Handle duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "class ParquetGroupChunkIter:\n",
        "    \"\"\"\n",
        "    Streams a Parquet file that is globally sorted by `join_key`,\n",
        "    yielding chunks of consecutive rows with the same key.\n",
        "    Each call to next_chunk() returns (table_chunk, key_value, is_last_for_key).\n",
        "    Memory bounded by IN_BATCH_ROWS and GROUP_CHUNK_ROWS.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path: str, columns, join_key: str,\n",
        "                 in_batch_rows: int = 64_000, group_chunk_rows: int = 64_000):\n",
        "        self._pf = pq.ParquetFile(file_path)\n",
        "        self._cols = columns\n",
        "        self._join_key = join_key\n",
        "        self._key_idx = (self._cols.index(join_key) if self._cols is not None\n",
        "                 else self._pf.schema_arrow.get_field_index(join_key))\n",
        "        if self._key_idx == -1:\n",
        "            raise ValueError(f\"join key {join_key} not in schema\")\n",
        "        self._IN_BATCH_ROWS = in_batch_rows\n",
        "        self._GROUP_CHUNK_ROWS = group_chunk_rows\n",
        "\n",
        "        self._it = self._pf.iter_batches(columns=self._cols, batch_size=self._IN_BATCH_ROWS)\n",
        "        self._batch = None\n",
        "        self._i = 0\n",
        "        self._cur_key = None\n",
        "        self._eof = False\n",
        "\n",
        "    def _next_batch(self):\n",
        "        try:\n",
        "            self._batch = next(self._it)\n",
        "            self._i = 0\n",
        "        except StopIteration:\n",
        "            self._batch = None\n",
        "\n",
        "    def _skip_nulls(self):\n",
        "        while self._batch is not None:\n",
        "            key_arr = self._batch.column(self._key_idx)\n",
        "            n = self._batch.num_rows\n",
        "            while self._i < n and key_arr[self._i].as_py() is None:\n",
        "                self._i += 1\n",
        "            if self._i < n:\n",
        "                return\n",
        "            self._next_batch()\n",
        "\n",
        "    def next_chunk(self):\n",
        "        if self._eof:\n",
        "            return None, None, True\n",
        "\n",
        "        if self._batch is None:\n",
        "            self._next_batch()\n",
        "            if self._batch is None:\n",
        "                self._eof = True\n",
        "                return None, None, True\n",
        "\n",
        "        self._skip_nulls()\n",
        "        if self._batch is None:\n",
        "            self._eof = True\n",
        "            return None, None, True\n",
        "\n",
        "        if self._cur_key is None:\n",
        "            self._cur_key = self._batch.column(self._key_idx)[self._i].as_py()\n",
        "\n",
        "        parts = []\n",
        "        collected = 0\n",
        "        more_for_key = False\n",
        "\n",
        "        while collected < self._GROUP_CHUNK_ROWS:\n",
        "            if self._batch is None:\n",
        "                break\n",
        "\n",
        "            key_arr = self._batch.column(self._key_idx)\n",
        "            n = self._batch.num_rows\n",
        "            j = self._i\n",
        "\n",
        "            # take rows while the key matches and we stay under the chunk cap\n",
        "            while j < n and key_arr[j].as_py() == self._cur_key and \\\n",
        "                  (collected + (j - self._i)) < self._GROUP_CHUNK_ROWS:\n",
        "                j += 1\n",
        "\n",
        "            take = j - self._i\n",
        "            if take > 0:\n",
        "                parts.append(self._batch.slice(self._i, take))\n",
        "                collected += take\n",
        "                self._i = j\n",
        "\n",
        "            if collected >= self._GROUP_CHUNK_ROWS:\n",
        "                # capped chunk; if more rows with same key remain, mark continuation\n",
        "                if self._i < n and key_arr[self._i].as_py() == self._cur_key:\n",
        "                    more_for_key = True\n",
        "                else:\n",
        "                    # peek next batch start\n",
        "                    save_batch, save_i = self._batch, self._i\n",
        "                    self._next_batch()\n",
        "                    if self._batch is not None:\n",
        "                        k0 = self._batch.column(self._key_idx)[0].as_py()\n",
        "                        more_for_key = (k0 == self._cur_key)\n",
        "                    # restore for the caller to continue\n",
        "                    self._batch, self._i = save_batch, save_i\n",
        "                break\n",
        "\n",
        "            if self._i >= n:\n",
        "                # move to next batch and check if key continues\n",
        "                self._next_batch()\n",
        "                self._skip_nulls()\n",
        "                if self._batch is None:\n",
        "                    break\n",
        "                if self._batch.column(self._key_idx)[self._i].as_py() != self._cur_key:\n",
        "                    break\n",
        "            else:\n",
        "                # key changed within this batch\n",
        "                break\n",
        "\n",
        "        tbl = pa.Table.from_batches(parts) if parts else None\n",
        "\n",
        "        # if no more rows remain for this key after this chunk, clear cur_key\n",
        "        if not more_for_key:\n",
        "            if self._batch is not None:\n",
        "                key_arr = self._batch.column(self._key_idx)\n",
        "                n = self._batch.num_rows\n",
        "                while self._i < n and key_arr[self._i].as_py() == self._cur_key:\n",
        "                    self._i += 1\n",
        "                if self._i >= n:\n",
        "                    self._next_batch()\n",
        "            self._cur_key = None\n",
        "\n",
        "        key_for_chunk = None\n",
        "        if tbl is not None:\n",
        "            key_for_chunk = tbl.column(self._key_idx).chunk(0)[0].as_py() \\\n",
        "                if tbl.num_rows > 0 else None\n",
        "\n",
        "        return tbl, key_for_chunk, not more_for_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "odvuVpv2hWkN"
      },
      "outputs": [],
      "source": [
        "\n",
        "BWAY_MERGE_FACTOR = 10\n",
        "\n",
        "class SortMergeJoin:\n",
        "    def __init__(\n",
        "        self, bway_merge_factor: int = BWAY_MERGE_FACTOR, num_pages_per_split=2\n",
        "    ):\n",
        "        self.bway_merge_factor = bway_merge_factor\n",
        "        self.num_pages_per_split = num_pages_per_split\n",
        "\n",
        "    def _streaming_inner_join(self,\n",
        "                            left_sorted: ColumnarDbFile,\n",
        "                            right_sorted: ColumnarDbFile,\n",
        "                            join_key1: str,\n",
        "                            join_key2: str,\n",
        "                            temp_dir: str,\n",
        "                            columns_table1: Optional[List[str]],\n",
        "                            columns_table2: Optional[List[str]],\n",
        "                            in_batch_rows: int = 64_000,\n",
        "                            group_chunk_rows: int = 64_000,\n",
        "                            max_product_rows: int = 200_000) -> ColumnarDbFile:\n",
        "\n",
        "        def ensure_key(cols, key):\n",
        "            if cols is None:\n",
        "                return None\n",
        "            return cols if key in cols else cols + [key]\n",
        "\n",
        "        # Compute input file paths\n",
        "        left_path = os.path.join(left_sorted.base_file_name, f\"{left_sorted.table_name}-0.parquet\")\n",
        "        right_path = os.path.join(right_sorted.base_file_name, f\"{right_sorted.table_name}-0.parquet\")\n",
        "\n",
        "        # Load schemas to default to all columns if None\n",
        "        l_pf = pq.ParquetFile(left_path)\n",
        "        r_pf = pq.ParquetFile(right_path)\n",
        "        l_schema = l_pf.schema_arrow\n",
        "        r_schema = r_pf.schema_arrow\n",
        "\n",
        "        if columns_table1 is None:\n",
        "            columns_table1 = l_schema.names\n",
        "        if columns_table2 is None:\n",
        "            columns_table2 = r_schema.names\n",
        "        columns_table1 = ensure_key(columns_table1, join_key1)\n",
        "        columns_table2 = ensure_key(columns_table2, join_key2)\n",
        "\n",
        "        # Output: keep left key; drop duplicate key from right\n",
        "        left_out_cols = columns_table1\n",
        "        right_out_cols = [c for c in columns_table2 if c != join_key2]\n",
        "\n",
        "        # Prepare iterators\n",
        "        L = ParquetGroupChunkIter(left_path, columns_table1, join_key1,\n",
        "                                    in_batch_rows, group_chunk_rows)\n",
        "        R = ParquetGroupChunkIter(right_path, columns_table2, join_key2,\n",
        "                                    in_batch_rows, group_chunk_rows)\n",
        "\n",
        "        # Output writer\n",
        "        result_table = ColumnarDbFile(\"join_result\", file_dir=temp_dir)\n",
        "        out_path = os.path.join(result_table.base_file_name, f\"{result_table.table_name}-0.parquet\")\n",
        "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "        writer = None\n",
        "\n",
        "        def write_join_product(l_tbl: pa.Table, r_tbl: pa.Table):\n",
        "            nonlocal writer\n",
        "            if l_tbl is None or r_tbl is None:\n",
        "                return\n",
        "            l_df = l_tbl.select(left_out_cols).to_pandas()\n",
        "            r_df = r_tbl.select(right_out_cols).to_pandas()\n",
        "            m, n = len(l_df), len(r_df)\n",
        "            if m == 0 or n == 0:\n",
        "                return\n",
        "            block_n = max(1, min(n, max_product_rows // max(1, m)))\n",
        "            start = 0\n",
        "            while start < n:\n",
        "                end = min(n, start + block_n)\n",
        "                r_block = r_df.iloc[start:end]\n",
        "                out_df = l_df.assign(_cj=1).merge(r_block.assign(_cj=1), on=\"_cj\").drop(columns=[\"_cj\"])\n",
        "                out_tbl = pa.Table.from_pandas(out_df, preserve_index=False)\n",
        "                if writer is None:\n",
        "                    writer = pq.ParquetWriter(out_path, schema=out_tbl.schema)\n",
        "                writer.write_table(out_tbl)\n",
        "                start = end\n",
        "\n",
        "        # Drive the SMJ\n",
        "        l_tbl, l_key, l_last = L.next_chunk()\n",
        "        r_tbl, r_key, r_last = R.next_chunk()\n",
        "\n",
        "        while l_tbl is not None and r_tbl is not None:\n",
        "            if l_key is None:\n",
        "                l_tbl, l_key, l_last = L.next_chunk()\n",
        "                continue\n",
        "            if r_key is None:\n",
        "                r_tbl, r_key, r_last = R.next_chunk()\n",
        "                continue\n",
        "\n",
        "            if l_key < r_key:\n",
        "                l_tbl, l_key, l_last = L.next_chunk()\n",
        "            elif l_key > r_key:\n",
        "                r_tbl, r_key, r_last = R.next_chunk()\n",
        "                \n",
        "            else:\n",
        "                # l_key == r_key: join full groups. Cache right group (spill if large),\n",
        "                # then stream left group chunk-by-chunk against the cached/spilled right.\n",
        "                import uuid\n",
        "\n",
        "                MAX_CACHED_GROUP_ROWS = 500_000  # tune as you like\n",
        "\n",
        "                r_chunks: List[pa.Table] = []\n",
        "                r_rows = 0\n",
        "                r_spill_path = None\n",
        "                r_writer = None\n",
        "\n",
        "                def _open_r_spill(schema: pa.Schema):\n",
        "                    nonlocal r_writer, r_spill_path\n",
        "                    if r_writer is None:\n",
        "                        r_spill_path = os.path.join(\n",
        "                            temp_dir, f\"_smj_rspill_{uuid.uuid4().hex}.parquet\"\n",
        "                        )\n",
        "                        r_writer = pq.ParquetWriter(r_spill_path, schema=schema)\n",
        "\n",
        "                def _cache_r_chunk(tbl: pa.Table):\n",
        "                    nonlocal r_rows, r_writer\n",
        "                    if tbl is None or tbl.num_rows == 0:\n",
        "                        return\n",
        "                    if r_writer is None and (r_rows + tbl.num_rows) <= MAX_CACHED_GROUP_ROWS:\n",
        "                        r_chunks.append(tbl)\n",
        "                    else:\n",
        "                        if r_writer is None:\n",
        "                            _open_r_spill(tbl.schema)\n",
        "                            for t in r_chunks:\n",
        "                                r_writer.write_table(t)\n",
        "                            r_chunks.clear()\n",
        "                        r_writer.write_table(tbl)\n",
        "                    r_rows += tbl.num_rows\n",
        "\n",
        "                # 1) Collect entire right group for this key\n",
        "                _cache_r_chunk(r_tbl)\n",
        "                while not r_last:\n",
        "                    r_tbl, r_key2, r_last = R.next_chunk()\n",
        "                    if r_tbl is None or r_key2 != l_key:\n",
        "                        break\n",
        "                    _cache_r_chunk(r_tbl)\n",
        "                if r_writer is not None:\n",
        "                    r_writer.close()\n",
        "\n",
        "                def right_iter():\n",
        "                    if r_spill_path is not None:\n",
        "                        pf = pq.ParquetFile(r_spill_path)\n",
        "                        for b in pf.iter_batches(batch_size=group_chunk_rows):\n",
        "                            yield pa.Table.from_batches([b])\n",
        "                    else:\n",
        "                        for t in r_chunks:\n",
        "                            yield t\n",
        "\n",
        "                # 2) Stream left group; for each left chunk, replay right group\n",
        "                while True:\n",
        "                    for r_part in right_iter():\n",
        "                        write_join_product(l_tbl, r_part)\n",
        "                    if l_last:\n",
        "                        break\n",
        "                    l_tbl, l_key2, l_last = L.next_chunk()\n",
        "                    if l_tbl is None or l_key2 != l_key:\n",
        "                        break\n",
        "\n",
        "                # 3) Advance right iterator to next key (we ended exactly at its boundary)\n",
        "                r_tbl, r_key, r_last = R.next_chunk()\n",
        "                # proceed to next loop iteration\n",
        "                continue\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.close()\n",
        "        return result_table\n",
        "\n",
        "    def _flush_run(\n",
        "        self,\n",
        "        dfs: List[pd.DataFrame],\n",
        "        join_key: str,\n",
        "        output_dir: str,\n",
        "        side: str,\n",
        "        run_idx: int,\n",
        "    ) -> str:\n",
        "\n",
        "        df_run = pd.concat(dfs, ignore_index=True)\n",
        "        df_run_sorted = df_run.sort_values(by=join_key)\n",
        "\n",
        "        run_file = os.path.join(output_dir, f\"{side}_run_{run_idx}.parquet\")\n",
        "        df_run_sorted.to_parquet(run_file)\n",
        "\n",
        "        dfs.clear()\n",
        "        del df_run, df_run_sorted\n",
        "        gc.collect()\n",
        "\n",
        "        return run_file\n",
        "\n",
        "\n",
        "    def _external_sort(\n",
        "        self,\n",
        "        table: ColumnarDbFile,\n",
        "        join_key: str,\n",
        "        output_dir: str,\n",
        "        side: str,\n",
        "        columns: Optional[List[str]] = None,\n",
        "    ) -> ColumnarDbFile:\n",
        "        \"\"\"\n",
        "        Perform an external sort on a table based on the join key and return a sorted ColumnarDbFile.\n",
        "        Use _bway_merge to merge sorted files\n",
        "        \"\"\"\n",
        "\n",
        "        # Get table size (on disk)\n",
        "        disk_usage = table.table_disk_usage()\n",
        "        total_bytes = disk_usage[\"total_bytes\"]\n",
        "\n",
        "        # Check if we can safely process in 12 GB RAM\n",
        "        if False: #table.can_process_parquet(total_bytes):\n",
        "\n",
        "            # read data in and sort all in RAM\n",
        "            df = table.retrieve_data(columns=columns)\n",
        "            df_sorted = df.sort_values(by=join_key).reset_index(drop=True)\n",
        "\n",
        "            # create paraquet in output dir for the table\n",
        "            sorted_name = f\"{side}_{table.table_name}_sorted\"\n",
        "            sorted_table = ColumnarDbFile(sorted_name, file_dir=output_dir)\n",
        "            sorted_table.build_table(df_sorted)\n",
        "\n",
        "            # clean unnecessary overhead and return table\n",
        "            del df, df_sorted\n",
        "            gc.collect()\n",
        "            return sorted_table\n",
        "\n",
        "        else:\n",
        "            print(\"sorting table \", table.table_name, \"with \", total_bytes, \"bytes using external sort\")\n",
        "            print(\"GBs : \", total_bytes / (1024 * 1024 * 1024))\n",
        "            # Get list of parquet files in the table directory\n",
        "            parquet_files = glob.glob(f\"{table.base_file_name}/*.parquet\")\n",
        "\n",
        "            runs_path: List[str] = []\n",
        "            run_idx = 0\n",
        "            current_dfs: List[pd.DataFrame] = []\n",
        "            current_row_groups = 0\n",
        "\n",
        "            # loop through all the parquet files\n",
        "            print(f\"Sorting {len(parquet_files)} files\")\n",
        "            for file in parquet_files:\n",
        "                pf = pq.ParquetFile(file)\n",
        "\n",
        "                # safe bounded unit of work for sorting\n",
        "                num_row_groups = pf.metadata.num_row_groups\n",
        "\n",
        "                for rg in range(num_row_groups):\n",
        "                    print(\"reading row group \", rg)\n",
        "\n",
        "                    # read a row group as a chunk\n",
        "                    batch = pf.read_row_group(rg, columns=columns)\n",
        "                    df_chunk = batch.to_pandas()\n",
        "                    current_dfs.append(df_chunk)\n",
        "                    current_row_groups += 1\n",
        "\n",
        "                    # treating a row group as a page\n",
        "                    # change to 2 to practice spliting runs\n",
        "                    if current_row_groups > self.num_pages_per_split:\n",
        "\n",
        "                        print(\"flushing run \", run_idx)\n",
        "                        # sort current run in ram\n",
        "                        # save as a parquet file in the current run directory\n",
        "                        run_file = self._flush_run(\n",
        "                        current_dfs, join_key, output_dir, side, run_idx\n",
        "                        )\n",
        "\n",
        "                        # runs path is a list of the sorted parquet files\n",
        "                        runs_path.append(run_file)\n",
        "                        run_idx += 1\n",
        "                        current_row_groups = 0\n",
        "                        print(f\"Flushed run {run_idx} at {run_file}\")\n",
        "\n",
        "            # flush remaining partial run, get sorted run_file as a df\n",
        "            if current_dfs:\n",
        "              run_file = self._flush_run(\n",
        "                  current_dfs, join_key, output_dir, side, run_idx\n",
        "              )\n",
        "              runs_path.append(run_file)\n",
        "            # output_dir has a list of sorted parquet files for the current table\n",
        "\n",
        "            # Create the wrapper first so we write where it will read\n",
        "            sorted_table = ColumnarDbFile(\n",
        "                table_name=f\"{side}_{table.table_name}_sorted\",\n",
        "                file_dir=output_dir,\n",
        "            )\n",
        "\n",
        "            # Write the final merged file inside that directory, matching ColumnarDbFile\n",
        "            final_sorted_path = os.path.join(\n",
        "                sorted_table.base_file_name, f\"{sorted_table.table_name}-0.parquet\"\n",
        "            )\n",
        "            print(\"merging all runs into \", final_sorted_path)\n",
        "            self._merge_all_runs(runs_path, final_sorted_path, join_key)\n",
        "\n",
        "            return sorted_table\n",
        "\n",
        "    def _merge_all_runs(self, sorted_files: List[str], output_file: str, join_key: str):\n",
        "        \"\"\"\n",
        "        Merge multiple sorted Parquet files into a single sorted Parquet file.\n",
        "        \"\"\"\n",
        "        B = self.bway_merge_factor\n",
        "\n",
        "        # copy that we will mutate\n",
        "        runs = list(sorted_files)\n",
        "        pass_idx = 0\n",
        "\n",
        "        # while list of runs left to merge\n",
        "        while len(runs) > 1:\n",
        "          print(\"merging pass \", pass_idx)\n",
        "          next_runs = []\n",
        "\n",
        "          # B - 1 input buffers +1 output buffer\n",
        "          for i in range(0, len(runs), B - 1):\n",
        "                batch = runs[i : i + (B - 1)]   # B-1 input buffers, still a list\n",
        "\n",
        "                # choose an output path for this merged batch\n",
        "                # on the final pass, we want the result at `output_file`\n",
        "                if len(runs) <= B - 1 and len(next_runs) == 0:\n",
        "                    # last pass, first (and only) merged run -> final output\n",
        "                    merged_path = output_file\n",
        "                else:\n",
        "                    # intermediate pass: write to a temp run file\n",
        "                    base_dir = os.path.dirname(output_file)\n",
        "                    merged_path = os.path.join(\n",
        "                        base_dir,\n",
        "                        f\"bway_pass{pass_idx}_run{len(next_runs)}.parquet\",\n",
        "                    )\n",
        "\n",
        "                # b-way merge this batch into merged_path\n",
        "                self._bway_merge(batch, merged_path, join_key)\n",
        "\n",
        "                next_runs.append(merged_path)\n",
        "\n",
        "          runs = next_runs\n",
        "          pass_idx += 1\n",
        "\n",
        "        # At this point, runs has exactly one file.\n",
        "        final_run = runs[0]\n",
        "        if final_run != output_file:\n",
        "            # In case we didn't land exactly on output_file path\n",
        "            if os.path.exists(output_file):\n",
        "                os.remove(output_file)\n",
        "            shutil.move(final_run, output_file)\n",
        "\n",
        "        return output_file\n",
        "\n",
        "\n",
        "    def _bway_merge(self, sorted_files: List[str], output_file: str, join_key: str):\n",
        "        \"\"\"\n",
        "        Streaming B-way merge of already-sorted Parquet 'runs' into a single\n",
        "        sorted Parquet file at `output_file`, ordered by `join_key`.\n",
        "        Memory use is bounded: keeps one small batch per input + an output buffer.\n",
        "        \"\"\"\n",
        "        import heapq\n",
        "        import pyarrow as pa\n",
        "        import pyarrow.parquet as pq\n",
        "\n",
        "        if not sorted_files:\n",
        "            raise ValueError(\"No input files to merge.\")\n",
        "\n",
        "        # Tunables: keep these moderate to bound memory\n",
        "        IN_BATCH_ROWS = 64_000         # rows read per input batch\n",
        "        OUT_ROW_GROUP_ROWS = 128_000   # rows written per output row group\n",
        "\n",
        "        # Determine common schema and key index from first file\n",
        "        first_pf = pq.ParquetFile(sorted_files[0])\n",
        "        schema = first_pf.schema_arrow\n",
        "        key_index = schema.get_field_index(join_key)\n",
        "        if key_index == -1:\n",
        "            raise ValueError(f\"join_key '{join_key}' not found in schema.\")\n",
        "\n",
        "        # Initialize streaming readers (preserve on-disk order)\n",
        "        readers = [pq.ParquetFile(p).iter_batches(batch_size=IN_BATCH_ROWS) for p in sorted_files]\n",
        "\n",
        "        # Per-input state\n",
        "        states = []  # list of dicts: { 'iter': iterator, 'batch': RecordBatch|None, 'ridx': int }\n",
        "        heap = []    # min-heap of ((is_null, key_value), src_idx)\n",
        "\n",
        "        def _prime_state(it):\n",
        "            try:\n",
        "                return next(it)\n",
        "            except StopIteration:\n",
        "                return None\n",
        "\n",
        "        def _push_heap_for_src(src_idx):\n",
        "            st = states[src_idx]\n",
        "            batch = st[\"batch\"]\n",
        "            ridx = st[\"ridx\"]\n",
        "            if batch is None or ridx >= batch.num_rows:\n",
        "                return\n",
        "            key_arr = batch.column(key_index)\n",
        "            key_val = key_arr[ridx].as_py()\n",
        "            # Use (is_null, value) to make None sort after non-null deterministically\n",
        "            heapq.heappush(heap, ((key_val is None, key_val), src_idx))\n",
        "\n",
        "        # Prime batches and heap\n",
        "        for it in readers:\n",
        "            batch = _prime_state(it)\n",
        "            states.append({\"iter\": it, \"batch\": batch, \"ridx\": 0})\n",
        "        for i in range(len(states)):\n",
        "            if states[i][\"batch\"] is not None and states[i][\"batch\"].num_rows > 0:\n",
        "                _push_heap_for_src(i)\n",
        "\n",
        "        # Open output writer\n",
        "        writer = pq.ParquetWriter(output_file, schema=schema)\n",
        "        out_batches: List[pa.RecordBatch] = []\n",
        "        out_rows = 0\n",
        "\n",
        "        # Core streaming merge loop\n",
        "        while heap:\n",
        "            _, src_idx = heapq.heappop(heap)\n",
        "            st = states[src_idx]\n",
        "            batch = st[\"batch\"]\n",
        "            ridx = st[\"ridx\"]\n",
        "\n",
        "            # Append a single-row slice to output buffer\n",
        "            out_batches.append(batch.slice(ridx, 1))\n",
        "            out_rows += 1\n",
        "\n",
        "            # Advance this source\n",
        "            st[\"ridx\"] += 1\n",
        "            if st[\"ridx\"] >= batch.num_rows:\n",
        "                st[\"batch\"] = _prime_state(st[\"iter\"])\n",
        "                st[\"ridx\"] = 0\n",
        "            if st[\"batch\"] is not None:\n",
        "                _push_heap_for_src(src_idx)\n",
        "\n",
        "            # Flush output buffer as a row group when big enough\n",
        "            if out_rows >= OUT_ROW_GROUP_ROWS:\n",
        "                writer.write_table(pa.Table.from_batches(out_batches))\n",
        "                out_batches.clear()\n",
        "                out_rows = 0\n",
        "\n",
        "        # Final flush\n",
        "        if out_rows > 0:\n",
        "            writer.write_table(pa.Table.from_batches(out_batches))\n",
        "        writer.close()\n",
        "\n",
        "    def join(\n",
        "        self,\n",
        "        table1: ColumnarDbFile,\n",
        "        table2: ColumnarDbFile,\n",
        "        join_key1: str,\n",
        "        join_key2: str,\n",
        "        temp_dir: str = \"temp\",\n",
        "        columns_table1: Optional[List[str]] = None,\n",
        "        columns_table2: Optional[List[str]] = None,\n",
        "    ) -> Optional[ColumnarDbFile]:\n",
        "        \"\"\"\n",
        "        Perform a sort-merge join between two ColumnarDbFile instances and return a sorted ColumnarDbFile.\n",
        "        \"\"\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Sort both tables externally\n",
        "        sorted_table1 = self._external_sort(\n",
        "            table1, join_key1, temp_dir, \"left\", columns_table1\n",
        "        )\n",
        "        sorted_table2 = self._external_sort(\n",
        "            table2, join_key2, temp_dir, \"right\", columns_table2\n",
        "        )\n",
        "\n",
        "        result_table = self._streaming_inner_join(\n",
        "            left_sorted=sorted_table1,\n",
        "            right_sorted=sorted_table2,\n",
        "            join_key1=join_key1,\n",
        "            join_key2=join_key2,\n",
        "            temp_dir=temp_dir,\n",
        "            columns_table1=columns_table1,\n",
        "            columns_table2=columns_table2,\n",
        "        )\n",
        "        return result_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yhxUqHS5ShN"
      },
      "outputs": [],
      "source": [
        "songs_table = tables['Songs']\n",
        "users_table = tables['Users']\n",
        "listens_table = tables['Listens']\n",
        "\n",
        "smj = SortMergeJoin()\n",
        "\n",
        "# Example: join Songs with Listens on song_id\n",
        "sorted_join_result = smj.join(\n",
        "    songs_table,\n",
        "    listens_table,\n",
        "    join_key1=\"song_id\",\n",
        "    join_key2=\"song_id\",\n",
        "    temp_dir=\"temp_songs_listens\",\n",
        "    columns_table1= [\"song_id\", \"title\"],\n",
        "    columns_table2= [\"song_id\", \"user_id\"]\n",
        ")\n",
        "\n",
        "display(sorted_join_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sorting table  LeftTest with  1269458 bytes using external sort\n",
            "GBs :  0.0011822748929262161\n",
            "Sorting 1 files\n",
            "reading row group  0\n",
            "merging all runs into  temp_smj_correctness/join_out/_left_LeftTest_sorted/left_LeftTest_sorted-0.parquet\n",
            "sorting table  RightTest with  2034101 bytes using external sort\n",
            "GBs :  0.0018944041803479195\n",
            "Sorting 1 files\n",
            "reading row group  0\n",
            "merging all runs into  temp_smj_correctness/join_out/_right_RightTest_sorted/right_RightTest_sorted-0.parquet\n",
            "Row counts - SMJ vs Pandas: 200000 200000\n",
            "Column sets equal: True\n",
            "Shapes equal: True\n",
            "Frames equal: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Correctness test vs pandas on a manageable subset\n",
        "import os, shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Choose columns and sample sizes\n",
        "JOIN_KEY_LEFT = \"song_id\"\n",
        "JOIN_KEY_RIGHT = \"song_id\"\n",
        "LEFT_COLS = [\"song_id\", \"title\"]\n",
        "RIGHT_COLS = [\"song_id\", \"user_id\"]\n",
        "\n",
        "N_LEFT = 200_000\n",
        "N_RIGHT = 200_000\n",
        "\n",
        "# 1) Create temporary, smaller ColumnarDbFiles from the big tables\n",
        "TEST_DIR = \"temp_smj_correctness\"\n",
        "if os.path.exists(TEST_DIR):\n",
        "    shutil.rmtree(TEST_DIR)\n",
        "os.makedirs(TEST_DIR, exist_ok=True)\n",
        "\n",
        "left_df_full = tables[\"Songs\"].retrieve_data(columns=LEFT_COLS)\n",
        "right_df_full = tables[\"Listens\"].retrieve_data(columns=RIGHT_COLS)\n",
        "\n",
        "left_df = left_df_full.head(N_LEFT).reset_index(drop=True)\n",
        "right_df = right_df_full.head(N_RIGHT).reset_index(drop=True)\n",
        "\n",
        "left_tmp = ColumnarDbFile(\"LeftTest\", file_dir=TEST_DIR)\n",
        "right_tmp = ColumnarDbFile(\"RightTest\", file_dir=TEST_DIR)\n",
        "left_tmp.build_table(left_df)\n",
        "right_tmp.build_table(right_df)\n",
        "\n",
        "# 2) Run your streaming SMJ on the temp tables\n",
        "smj = SortMergeJoin()\n",
        "smj_out = smj.join(\n",
        "    left_tmp,\n",
        "    right_tmp,\n",
        "    join_key1=JOIN_KEY_LEFT,\n",
        "    join_key2=JOIN_KEY_RIGHT,\n",
        "    temp_dir=os.path.join(TEST_DIR, \"join_out\"),\n",
        "    columns_table1=LEFT_COLS,\n",
        "    columns_table2=RIGHT_COLS,\n",
        ")\n",
        "\n",
        "# 3) Materialize results\n",
        "# SMJ output keeps only the left join key (your implementation drops the right duplicate)\n",
        "smj_df = smj_out.retrieve_data()\n",
        "\n",
        "# Pandas baseline: merge then drop the duplicate right key to match SMJ schema\n",
        "baseline_df = pd.merge(\n",
        "    left_df,\n",
        "    right_df,\n",
        "    left_on=JOIN_KEY_LEFT,\n",
        "    right_on=JOIN_KEY_RIGHT,\n",
        "    how=\"inner\",\n",
        ")\n",
        "baseline_df = baseline_df[LEFT_COLS + [c for c in RIGHT_COLS if c != JOIN_KEY_RIGHT]]\n",
        "\n",
        "# 4) Normalize order and compare\n",
        "order_cols = [JOIN_KEY_LEFT] + [c for c in LEFT_COLS if c != JOIN_KEY_LEFT] + [c for c in RIGHT_COLS if c != JOIN_KEY_RIGHT]\n",
        "smj_df = smj_df.sort_values(by=order_cols).reset_index(drop=True)\n",
        "baseline_df = baseline_df.sort_values(by=order_cols).reset_index(drop=True)\n",
        "\n",
        "# Ensure identical dtypes for fair compare when feasible\n",
        "for c in order_cols:\n",
        "    if c in smj_df.columns and c in baseline_df.columns:\n",
        "        try:\n",
        "            baseline_df[c] = baseline_df[c].astype(smj_df[c].dtype)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# 5) Assertions and quick diagnostics\n",
        "same_shape = smj_df.shape == baseline_df.shape\n",
        "same_rows = smj_df.equals(baseline_df)\n",
        "\n",
        "print(\"Row counts - SMJ vs Pandas:\", smj_df.shape[0], baseline_df.shape[0])\n",
        "print(\"Column sets equal:\", set(smj_df.columns) == set(baseline_df.columns))\n",
        "print(\"Shapes equal:\", same_shape)\n",
        "print(\"Frames equal:\", same_rows)\n",
        "\n",
        "if not same_rows:\n",
        "    # Show a small diff preview\n",
        "    from itertools import islice\n",
        "    merged_chk = smj_df.merge(\n",
        "        baseline_df, how=\"outer\", indicator=True, on=order_cols\n",
        "    )\n",
        "    print(\"Mismatched samples:\")\n",
        "    display(merged_chk[merged_chk[\"_merge\"] != \"both\"].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af8XhdLWhWkN"
      },
      "source": [
        "Implement GROUP BY after joins:\n",
        "- Here you could use `pd.groupby` or do manual aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iwQ65LBhWkN"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feYU7Tdlke9Z"
      },
      "source": [
        "# Section 4: Query Planning & Optimization\n",
        "\n",
        "In this section, you'll implement smart query planning using metadata analysis. The key idea is to **avoid loading data unnecessarily** by:\n",
        "1. Analyzing Parquet metadata first (row counts, column names, file sizes)\n",
        "2. Making intelligent decisions about join order and algorithm selection\n",
        "3. Loading only the columns you actually need for the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQt-sR-zhWkN"
      },
      "outputs": [],
      "source": [
        "def analyze_metadata_before_loading(file_paths):\n",
        "    \"\"\"YOUR TASK: Get table statistics WITHOUT loading data\n",
        "    \n",
        "    Hints:\n",
        "    - Use pq.ParquetFile() to access metadata\n",
        "    - Extract: num_rows, column names, file sizes\n",
        "    - DON'T use pd.read_parquet() here - that loads data!\n",
        "    \"\"\"\n",
        "    # TODO: For each table ('songs', 'users', 'listens'):\n",
        "    #   - Open the Parquet file (but don't load data)\n",
        "    #   - Extract metadata like row count, columns, sizes\n",
        "    #   - Store in a dictionary\n",
        "    pass  # Your implementation here\n",
        "    metadata = {}\n",
        "\n",
        "    for table_name, coldb in tables.items():\n",
        "        # Use provided helpers for counts/sizes\n",
        "        tm = coldb.table_metadata()        # num_files, total_rows, total_compressed_bytes\n",
        "        du = coldb.table_disk_usage()      # num_files, total_bytes (on disk)\n",
        "\n",
        "        # Get schema from the first parquet file (metadata only)\n",
        "        base_dir = coldb.base_file_name\n",
        "        parquet_files = sorted(glob.glob(f\"{base_dir}/*.parquet\"))\n",
        "        columns = {}\n",
        "        if parquet_files:\n",
        "            pf = pq.ParquetFile(parquet_files[0])\n",
        "            # Use Arrow schema to read field names and types\n",
        "            columns = {field.name: str(field.type) for field in pf.schema_arrow}\n",
        "\n",
        "        metadata[table_name] = {\n",
        "            \"num_files\": tm[\"num_files\"],\n",
        "            \"rows\": int(tm[\"total_rows\"]),\n",
        "            \"columns\": columns,  # name -> type\n",
        "            \"bytes_on_disk\": int(du[\"total_bytes\"]),\n",
        "            \"total_compressed_bytes\": int(tm[\"total_compressed_bytes\"]),\n",
        "            \"can_process_in_12GB\": ColumnarDbFile.can_process_parquet(int(du[\"total_bytes\"]))\n",
        "        }\n",
        "\n",
        "    return metadata\n",
        "\n",
        "def estimate_smj_build_bytes\n",
        "    # BigSort(R) + BigSort(S) + MergeJoin(R, S) + C_w * Out\n",
        "\n",
        "\n",
        "def plan_query_execution(metadata, parsed_query, memory_bytes=12 * 1024**3, overhead_per_row=24):\n",
        "    \"\"\"YOUR TASK: Use metadata to make smart decisions\n",
        "\n",
        "    Questions to answer:\n",
        "    - Which table is smallest? Largest?\n",
        "    - Will a hash table fit in memory?\n",
        "    - Which columns does the query actually need?\n",
        "    - What's the optimal join order?\n",
        "    \"\"\"\n",
        "    # TODO: Based on metadata, decide:\n",
        "    #   1. Join order (smallest first? or different strategy?)\n",
        "    #   2. Algorithm choice (HPJ if fits in memory, else SMJ)\n",
        "    #   3. Which columns to load for each table\n",
        "    \"\"\"\n",
        "    Use parsed SQL + table metadata to:\n",
        "      - pick columns to read (column pruning)\n",
        "      - choose join order (follow SQL joins; build on smaller side)\n",
        "      - choose algorithm per step (HPJ if build fits in memory, else SMJ)\n",
        "    Returns a plan dict.\n",
        "    \"\"\"\n",
        "    # 0) Helpers\n",
        "    def meta_key_for(table_name_lower: str) -> str:\n",
        "        # Our metadata keys are capitalized: 'Songs','Users','Listens'\n",
        "        return table_name_lower.capitalize()\n",
        "\n",
        "    def size_of_type(t: str) -> int:\n",
        "        t = (t or \"\").lower()\n",
        "        if \"int64\" in t or \"timestamp\" in t or \"float64\" in t or \"double\" in t:\n",
        "            return 8\n",
        "        if \"int32\" in t or \"float32\" in t:\n",
        "            return 4\n",
        "        if \"bool\" in t:\n",
        "            return 1\n",
        "        # strings/unknown: average placeholder\n",
        "        return 16\n",
        "\n",
        "    def estimate_build_bytes(rows: int, cols: list[str], column_types: dict[str,str]) -> int:\n",
        "        if rows <= 0:\n",
        "            return 0\n",
        "        per_row = sum(size_of_type(column_types.get(c, \"\")) for c in cols) + overhead_per_row\n",
        "        return int(rows * per_row)\n",
        "\n",
        "    # 1) Alias: table-name mappings\n",
        "    alias_to_table_lower = parsed_query[\"tables\"]\n",
        "    alias_to_meta = {a: meta_key_for(t) for a, t in alias_to_table_lower.items()}\n",
        "\n",
        "    # 2) Columns needed per alias\n",
        "    cols_needed_by_alias: dict[str, set] = {a: set() for a in alias_to_table_lower.keys()}\n",
        "\n",
        "    # 2a) Joins (include join keys)\n",
        "    for j in parsed_query[\"joins\"][\"Joins\"]:\n",
        "        cols_needed_by_alias[j[\"left_alias\"]].add(j[\"left_column\"])\n",
        "        cols_needed_by_alias[j[\"right_alias\"]].add(j[\"right_column\"])\n",
        "\n",
        "    # 2b) GROUP BY columns\n",
        "    for a, c in parsed_query.get(\"GroupBy\", []):\n",
        "        cols_needed_by_alias[a].add(c)\n",
        "\n",
        "    # 2c) SELECT items and aggregations\n",
        "    for item in parsed_query.get(\"select\", []):\n",
        "        if item[\"kind\"] == \"column\":\n",
        "            a, c = item[\"source\"]\n",
        "            cols_needed_by_alias[a].add(c)\n",
        "\n",
        "    for agg in parsed_query.get(\"aggregations\", {}).values():\n",
        "        a, c = agg[\"source\"]\n",
        "        cols_needed_by_alias[a].add(c)\n",
        "\n",
        "    # 2d) ORDER BY columns (aggregations already covered)\n",
        "    for ob in parsed_query.get(\"orderBy\", []):\n",
        "        if ob[\"kind\"] == \"column\":\n",
        "            a, c = ob[\"source\"]\n",
        "            cols_needed_by_alias[a].add(c)\n",
        "\n",
        "    # 2e) Convert to real table names\n",
        "    columns_to_load = {}\n",
        "    for a, cols in cols_needed_by_alias.items():\n",
        "        tkey = alias_to_meta[a]\n",
        "        # intersect with actual table columns for safety\n",
        "        actual_cols = set(metadata[tkey][\"columns\"].keys())\n",
        "        columns_to_load[tkey] = sorted(list(set(cols) & actual_cols)) if actual_cols else sorted(list(cols))\n",
        "\n",
        "    # 3) Plan join steps (follow parsed order; build smaller side each time)\n",
        "    base_alias, base_table_lower = parsed_query[\"joins\"][\"base_table\"]\n",
        "    current_alias = base_alias\n",
        "    current_tkey = alias_to_meta[current_alias]\n",
        "    current_rows = int(metadata[current_tkey][\"rows\"])\n",
        "\n",
        "    join_steps = []\n",
        "\n",
        "    for j in parsed_query[\"joins\"][\"Joins\"]:\n",
        "        joined_alias = j[\"joined_table_alias\"]\n",
        "        joined_tkey = alias_to_meta[joined_alias]\n",
        "        joined_rows = int(metadata[joined_tkey][\"rows\"])\n",
        "\n",
        "        # Which side's key belongs to the current relation?\n",
        "        if j[\"left_alias\"] == joined_alias:\n",
        "            joined_key = j[\"left_column\"]\n",
        "            current_key = j[\"right_column\"]\n",
        "        else:\n",
        "            joined_key = j[\"right_column\"]\n",
        "            current_key = j[\"left_column\"]\n",
        "\n",
        "        # Determine which alias supplies the \"current\" key for this join\n",
        "        provider_alias = j[\"left_alias\"] if j[\"left_alias\"] != joined_alias else j[\"right_alias\"]\n",
        "        current_types_tkey = alias_to_meta[provider_alias]\n",
        "\n",
        "        # Estimate build memory for both choices\n",
        "        current_build_cols = [current_key]\n",
        "        joined_build_cols = columns_to_load.get(joined_tkey, []) or [joined_key]\n",
        "\n",
        "        current_build_bytes = estimate_build_bytes(\n",
        "            current_rows, current_build_cols, metadata[current_types_tkey][\"columns\"]\n",
        "        )\n",
        "        joined_build_bytes = estimate_build_bytes(\n",
        "            joined_rows, joined_build_cols, metadata[joined_tkey][\"columns\"]\n",
        "        )\n",
        "\n",
        "        # Choose smaller build\n",
        "        build_side = \"current\" if current_build_bytes <= joined_build_bytes else \"joined\"\n",
        "        build_bytes = min(current_build_bytes, joined_build_bytes)\n",
        "        algorithm = \"HPJ\" if build_bytes <= memory_bytes else \"SMJ\"\n",
        "\n",
        "        # Record step (left=current relation, right=joined table)\n",
        "        step = {\n",
        "            \"left_relation\": current_tkey,                 # current so far\n",
        "            \"right_relation\": joined_tkey,                 # the new table\n",
        "            \"left_on\": current_key,\n",
        "            \"right_on\": joined_key,\n",
        "            \"build_side\": build_side,                      # 'current' or 'joined'\n",
        "            \"estimated_build_bytes\": build_bytes,\n",
        "            \"algorithm\": algorithm,\n",
        "        }\n",
        "        join_steps.append(step)\n",
        "\n",
        "        # Update current relation stats: result rows ~ max of inputs for FK joins\n",
        "        current_rows = max(current_rows, joined_rows)\n",
        "        current_tkey = f\"({current_tkey}⋈{joined_tkey})\"   # label for readability\n",
        "\n",
        "    plan = {\n",
        "        \"columns_to_load\": columns_to_load,        # table-name -> [columns]\n",
        "        \"join_steps\": join_steps,                  # ordered join execution plan\n",
        "        \"memory_budget_bytes\": memory_bytes,\n",
        "        \"group_by\": parsed_query.get(\"GroupBy\", []),\n",
        "        \"select\": parsed_query.get(\"select\", []),\n",
        "        \"aggregations\": parsed_query.get(\"aggregations\", {}),\n",
        "        \"order_by\": parsed_query.get(\"orderBy\", []),\n",
        "    }\n",
        "    return plan\n",
        "\n",
        "\n",
        "# After planning, load ONLY what you need:\n",
        "# Example (you implement the actual logic):\n",
        "# columns_needed = ['song_id', 'artist']  # From your planning\n",
        "# df = pd.read_parquet('songs.parquet', columns=columns_needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'joined_table_alias': 'l', 'joined_table_name': 'listens', 'left_alias': 's', 'left_column': 'song_id', 'right_alias': 'l', 'right_column': 'song_id'}\n",
            "{'joined_table_alias': 'u', 'joined_table_name': 'users', 'left_alias': 'l', 'left_column': 'user_id', 'right_alias': 'u', 'right_column': 'user_id'}\n",
            "\n",
            "\n",
            "\n",
            "Songs {'num_files': 1, 'rows': 100000, 'columns': {'song_id': 'int64', 'title': 'string', 'extra_col_1': 'string', 'extra_col_2': 'string', 'extra_col_3': 'string', 'extra_col_4': 'string', 'extra_col_5': 'string', 'extra_col_6': 'string', 'extra_col_7': 'string', 'extra_col_8': 'string', 'extra_col_9': 'string', 'extra_col_10': 'string'}, 'bytes_on_disk': 43162300, 'total_compressed_bytes': 8782, 'can_process_in_12GB': True}\n",
            "Users {'num_files': 1, 'rows': 500000, 'columns': {'user_id': 'int64', 'age': 'int64', 'extra_col_1': 'string', 'extra_col_2': 'string', 'extra_col_3': 'string', 'extra_col_4': 'string', 'extra_col_5': 'string', 'extra_col_6': 'string', 'extra_col_7': 'string', 'extra_col_8': 'string', 'extra_col_9': 'string', 'extra_col_10': 'string'}, 'bytes_on_disk': 210923285, 'total_compressed_bytes': 8780, 'can_process_in_12GB': True}\n",
            "Listens {'num_files': 1, 'rows': 10000000, 'columns': {'listen_id': 'int64', 'user_id': 'int64', 'song_id': 'int64', 'extra_col_1': 'string', 'extra_col_2': 'string', 'extra_col_3': 'string', 'extra_col_4': 'string', 'extra_col_5': 'string', 'extra_col_6': 'string', 'extra_col_7': 'string', 'extra_col_8': 'string', 'extra_col_9': 'string', 'extra_col_10': 'string'}, 'bytes_on_disk': 874997335, 'total_compressed_bytes': 23571, 'can_process_in_12GB': True}\n",
            "\n",
            "\n",
            "\n",
            "columns_to_load {'Songs': ['song_id', 'title'], 'Listens': ['song_id', 'user_id'], 'Users': ['age', 'user_id']}\n",
            "join_steps [{'left_relation': 'Songs', 'right_relation': 'Listens', 'left_on': 'song_id', 'right_on': 'song_id', 'build_side': 'current', 'estimated_build_bytes': 3200000, 'algorithm': 'HPJ'}, {'left_relation': '(Songs⋈Listens)', 'right_relation': 'Users', 'left_on': 'user_id', 'right_on': 'user_id', 'build_side': 'joined', 'estimated_build_bytes': 20000000, 'algorithm': 'HPJ'}]\n",
            "memory_budget_bytes 12884901888\n",
            "group_by [('s', 'song_id'), ('s', 'title')]\n",
            "select [{'kind': 'column', 'source': ('s', 'song_id'), 'alias': None}, {'kind': 'aggregation', 'agg_key': 1, 'alias': 'avg_age'}, {'kind': 'aggregation', 'agg_key': 2, 'alias': None}]\n",
            "aggregations {1: {'func': 'avg', 'source': ('u', 'age'), 'distinct': False, 'output_name': 'avg_age'}, 2: {'func': 'count', 'source': ('l', 'user_id'), 'distinct': True, 'output_name': None}}\n",
            "order_by [{'kind': 'aggregation', 'agg_key': 2, 'direction': 'desc'}, {'kind': 'column', 'source': ('s', 'song_id'), 'direction': 'asc'}]\n"
          ]
        }
      ],
      "source": [
        "joins = parsed_sql[\"joins\"][\"Joins\"]\n",
        "for join in joins:\n",
        "    print(join)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "metadata = analyze_metadata_before_loading(tables)\n",
        "for key, value in metadata.items():\n",
        "    print(key, value)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "plan = plan_query_execution(metadata, parsed_sql)\n",
        "for key, value in plan.items():\n",
        "    print(key, value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhwOd7sfhWkN"
      },
      "outputs": [],
      "source": [
        "class QueryPlanner:\n",
        "    pass # Your implementation here\n",
        "\n",
        "\n",
        "class QueryExecutor:\n",
        "    def __init__(self, tables, num_partitions=8, output_dir=\"temp\", planner=None):\n",
        "        self.tables = tables\n",
        "        self.num_partitions = num_partitions\n",
        "        self.output_dir = output_dir\n",
        "        self.planner = planner or QueryPlanner()\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def execute_hardcoded_query(self):\n",
        "        \"\"\"\n",
        "        Executes the following SQL query:\n",
        "\n",
        "        SELECT s.song_id, AVG(u.age) AS avg_age,\n",
        "        COUNT(DISTINCT l.user_id)\n",
        "        FROM Songs s\n",
        "        JOIN Listens l ON s.song_id = l.song_id\n",
        "        JOIN Users u ON l.user_id = u.user_id\n",
        "        GROUP BY s.song_id, s.title\n",
        "        ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
        "        \"\"\"\n",
        "\n",
        "        # Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BunT6g1HklaH"
      },
      "source": [
        "# Section 5: Performance Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpRb3IsakmzT"
      },
      "outputs": [],
      "source": [
        "def benchmark_query(executor, dataset_size):\n",
        "    \"\"\"Benchmark the query execution time and memory usage.\"\"\"\n",
        "    print(f\"\\nBenchmarking with {dataset_size} dataset...\")\n",
        "    start_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "    start_time = time.time()\n",
        "\n",
        "    result = executor.execute_hardcoded_query()\n",
        "\n",
        "    end_time = time.time()\n",
        "    end_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
        "    print(f\"Memory Usage: {end_mem - start_mem:.2f} MB\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUny8jDziWxk"
      },
      "source": [
        "## 100MB Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OBi6vhriYRK"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HalUj5s-ifAu"
      },
      "source": [
        "## 1GB Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwmefvmzigRO"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8h2f0TAijZT"
      },
      "source": [
        "## Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AQL4sxdinZn"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs145-project2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
