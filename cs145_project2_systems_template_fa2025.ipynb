{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/berkyalcinkaya/cs145-project2-systems/blob/main/cs145_project2_systems_template_fa2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab (Main)\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/berkyalcinkaya/cs145-project2-systems/blob/berk/cs145_project2_systems_template_fa2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab (berk)\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM8u06dhhieu"
   },
   "source": [
    "## Collaborators\n",
    "\n",
    "1.   Berk Yalcinkaya\n",
    "2.   Nick Allen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-T7CuUFejs1R"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kSLvQ6xjjqQh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import time\n",
    "import psutil\n",
    "import heapq\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable, Dict, Union, Any, Tuple\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "from IPython.display import display\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import memory_profiler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_parquet_files():\n",
    "    for file in glob.glob(\"*.parquet\"):\n",
    "        os.remove(file)\n",
    "    return\n",
    "\n",
    "clear_parquet_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwVW6zMghmMq"
   },
   "source": [
    "# Section 0: Generate Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y0yOxXShWkL"
   },
   "source": [
    "This section has already been implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Djt3vqewhA76"
   },
   "outputs": [],
   "source": [
    "def generate_songs_chunk(start, size, string_length=100):\n",
    "    data = {\n",
    "        \"song_id\": range(start, start + size),\n",
    "        \"title\": [f\"Song_{i}\" for i in range(start, start + size)],\n",
    "    }\n",
    "    base_strings = generate_base_strings(size, string_length)\n",
    "    for i in range(1, 11):\n",
    "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def generate_users_chunk(start, size, string_length=100):\n",
    "    data = {\n",
    "        \"user_id\": range(start, start + size),\n",
    "        \"age\": [18 + ((start + i) % 60) for i in range(size)],\n",
    "    }\n",
    "    base_strings = generate_base_strings(size, string_length)\n",
    "    for i in range(1, 11):\n",
    "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def generate_listens_chunk(start, size, num_users, num_songs, string_length=16):\n",
    "    data = {\n",
    "        \"listen_id\": range(start, start + size),\n",
    "        \"user_id\": np.random.randint(0, num_users, size=size),\n",
    "        \"song_id\": np.random.randint(0, num_songs, size=size),\n",
    "    }\n",
    "    base_strings = generate_base_strings(size, string_length)\n",
    "    for i in range(1, 11):\n",
    "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def generate_base_strings(num_records, string_length):\n",
    "    chars = np.array(list(\"ab\"))\n",
    "    random_indices = np.random.randint(0, len(chars), size=(num_records, string_length))\n",
    "    char_array = chars[random_indices]\n",
    "    return np.array(list(map(\"\".join, char_array)))\n",
    "\n",
    "\n",
    "def _write_parquet_streamed(\n",
    "    filename,\n",
    "    total_rows,\n",
    "    make_chunk_fn,\n",
    "    chunk_size=250_000,\n",
    "    compression=\"snappy\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream DataFrame chunks to a single Parquet file with one ParquetWriter.\n",
    "    - schema_df: optional small DataFrame to lock schema; if None we'll infer from the first chunk.\n",
    "    \"\"\"\n",
    "    written = 0\n",
    "\n",
    "    first_chunk = make_chunk_fn(0, min(chunk_size, total_rows))\n",
    "    first_table = pa.Table.from_pandas(first_chunk, preserve_index=False)\n",
    "    writer = pq.ParquetWriter(filename, first_table.schema, compression=compression)\n",
    "    writer.write_table(first_table)\n",
    "\n",
    "    written += len(first_chunk)\n",
    "    del first_chunk\n",
    "    gc.collect()\n",
    "\n",
    "    while written < total_rows:\n",
    "        take = min(chunk_size, total_rows - written)\n",
    "        chunk_df = make_chunk_fn(written, take)\n",
    "        writer.write_table(pa.Table.from_pandas(chunk_df, preserve_index=False))\n",
    "        written += take\n",
    "        del chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def generate_test_data(target_size=\"100MB\"):\n",
    "    \"\"\"\n",
    "    Generate datasets with proper foreign key relationships.\n",
    "\n",
    "    Target COMPRESSED Parquet file sizes on disk:\n",
    "    100MB total compressed:\n",
    "        - Songs: 10K rows → ~5MB (5% of total)\n",
    "        - Users: 50K rows → ~20MB (20% of total)\n",
    "        - Listens: 1M rows → ~75MB (75% of total)\n",
    "    1GB total compressed:\n",
    "        - Songs: 100K rows → ~50MB (5% of total)\n",
    "        - Users: 500K rows → ~200MB (20% of total)\n",
    "        - Listens: 10M rows → ~750MB (75% of total)\n",
    "\n",
    "    Each table needs:\n",
    "        - Primary key column(s)\n",
    "        - 10 additional string columns of k characters each\n",
    "        - For Users: add 'age' column (random 18-80)\n",
    "\n",
    "    CRITICAL: Listens table must have valid foreign keys!\n",
    "    Every song_id must exist in Songs\n",
    "    Every user_id must exist in Users\n",
    "    \"\"\"\n",
    "\n",
    "    assert target_size in [\"100MB\", \"1GB\", \"10GB\"]\n",
    "    if target_size == \"100MB\":\n",
    "        num_songs = 10_000\n",
    "        num_users = 50_000\n",
    "        num_listens = 1_000_000\n",
    "\n",
    "        songs_chunk = 10_000\n",
    "        users_chunk = 50_000\n",
    "        listens_chunk = 1_000_000\n",
    "    elif target_size == \"1GB\":\n",
    "        num_songs = 100_000\n",
    "        num_users = 500_000\n",
    "        num_listens = 10_000_000\n",
    "\n",
    "        songs_chunk = 10_000\n",
    "        users_chunk = 50_000\n",
    "        listens_chunk = 1_000_000\n",
    "    else: \n",
    "        num_songs = 1_000_000\n",
    "        num_users = 5_000_000\n",
    "        num_listens = 100_000_000\n",
    "\n",
    "        songs_chunk = 10_000\n",
    "        users_chunk = 50_000\n",
    "        listens_chunk = 1_000_000\n",
    "\n",
    "    print(\"Writing Songs\")\n",
    "    _write_parquet_streamed(\n",
    "        filename=f\"songs_{target_size}.parquet\",\n",
    "        total_rows=num_songs,\n",
    "        make_chunk_fn=lambda start, size: generate_songs_chunk(start, size),\n",
    "        chunk_size=songs_chunk,\n",
    "    )\n",
    "\n",
    "    print(\"Writing Users\")\n",
    "    _write_parquet_streamed(\n",
    "        filename=f\"users_{target_size}.parquet\",\n",
    "        total_rows=num_users,\n",
    "        make_chunk_fn=lambda start, size: generate_users_chunk(start, size),\n",
    "        chunk_size=users_chunk,\n",
    "    )\n",
    "\n",
    "    print(\"Writing Listens\")\n",
    "    _write_parquet_streamed(\n",
    "        filename=f\"listens_{target_size}.parquet\",\n",
    "        total_rows=num_listens,\n",
    "        make_chunk_fn=lambda start, size: generate_listens_chunk(\n",
    "            start, size, num_users, num_songs\n",
    "        ),\n",
    "        chunk_size=listens_chunk,\n",
    "    )\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0b: Define Memory and Performance Benchmarking Functions\n",
    "- Memory will be monitored using the memory_profiler function: %%memit above a cell monitors memory usage of entire cell, %memit monitors the memory usage of a single line\n",
    "- CPU performance will be measured with a custom decorator defined below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure and print the execution time of a function.\n",
    "\n",
    "    Usage:\n",
    "        @timer\n",
    "        def my_function(...):\n",
    "            ...\n",
    "\n",
    "    When the decorated function is called, it will print the elapsed time in seconds with a descriptive message.\n",
    "\n",
    "    Returns:\n",
    "        The result of the wrapped function, after printing its runtime.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        print(f\"Method '{func.__name__}' took {end_time - start_time:.4f} seconds.\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5qnpQghhWkL",
    "outputId": "40816b75-44cb-4338-8610-e9838e59970a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100MB data already generated\n",
      "1GB data already generated\n",
      "10GB data already generated\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "if not os.path.exists(\"listens_100MB.parquet\"):\n",
    "    generate_test_data(\"100MB\")\n",
    "else:\n",
    "    print(\"100MB data already generated\")\n",
    "if not os.path.exists(\"listens_1GB.parquet\"):\n",
    "    generate_test_data('1GB')\n",
    "else:\n",
    "    print(\"1GB data already generated\")\n",
    "if not os.path.exists(\"listens_10GB.parquet\"):\n",
    "    generate_test_data('10GB')\n",
    "else:\n",
    "    print(\"10GB data already generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEiGGznFhtxo"
   },
   "source": [
    "# Section 1: Parquet-based Columnar Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BGrkP5PhWkM"
   },
   "source": [
    "Implement Parquet-based storage for the tables\n",
    "- For simplicity, store all data for a table in a single Parquet file and use a single DataFrame object as a buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b0o8zkpGhWkM"
   },
   "outputs": [],
   "source": [
    "# see ed: https://edstem.org/us/courses/87394/discussion/7251811 for advice on writing to a parquet without loading existing into RAM\n",
    "# a ColumnarDbFile is actually a directory with an arbitrary number of parquet files inside\n",
    "# Append writes a new file with the next postfix\n",
    "# Retrieve reads all parquet files and concatenates them together, done natively by pandas\n",
    "class ColumnarDbFile:\n",
    "    def __init__(self, table_name, file_dir='data', file_pfx=''):\n",
    "        self.file_pfx = file_pfx\n",
    "        self.table_name = table_name\n",
    "        self.file_dir = file_dir\n",
    "        #os.makedirs(self.file_dir, exist_ok=True)\n",
    "        self.base_file_name = f\"{self.file_dir}/{self.file_pfx}_{self.table_name}\"\n",
    "        os.makedirs(self.base_file_name, exist_ok=True)\n",
    "        \n",
    "        # Streaming state\n",
    "        self._streaming = False\n",
    "        self._stream_writer = None\n",
    "        self._stream_file_path = None\n",
    "\n",
    "    def build_table(self, data):\n",
    "        \"\"\"Build and save table data to Parquet.\"\"\"\n",
    "        assert self._get_num_parquets() == 0\n",
    "        target_path = f\"{self.base_file_name}/{self.table_name}-0.parquet\"\n",
    "        # If data is a string and is a valid file path, copy it\n",
    "        if isinstance(data, str) and os.path.isfile(data):\n",
    "            shutil.copy(data, target_path)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            data.to_parquet(target_path)\n",
    "        else:\n",
    "            raise ValueError(\"data must be a pandas DataFrame or a valid file path string\")\n",
    "        return\n",
    "\n",
    "    def retrieve_data(self, columns=None, sample=None):\n",
    "        \"\"\"Create pd.DataFrame by reading from Parquet\"\"\"\n",
    "        if sample is not None:\n",
    "            return next(self.iter_pages(sample, columns=columns, as_pandas=True))\n",
    "        else:\n",
    "            return pd.read_parquet(self.base_file_name, columns=columns)\n",
    "\n",
    "    def append_data(self, data):\n",
    "        \"\"\"Append new data to Parquet\n",
    "        \n",
    "        Behavior depends on streaming mode:\n",
    "        - If streaming (start_stream() called): writes to a single parquet file via ParquetWriter\n",
    "        - Otherwise: creates a new parquet file for each call\n",
    "        \"\"\"\n",
    "        if self._streaming:\n",
    "            # Convert DataFrame to PyArrow Table\n",
    "            table = pa.Table.from_pandas(data, preserve_index=False)\n",
    "            \n",
    "            # Lazy writer creation: create on first append with schema\n",
    "            if self._stream_writer is None:\n",
    "                self._stream_writer = pq.ParquetWriter(self._stream_file_path, table.schema)\n",
    "            \n",
    "            # Write to stream\n",
    "            self._stream_writer.write_table(table)\n",
    "        else:\n",
    "            # Original behavior: create new file\n",
    "            data.to_parquet(self.get_new_parquet_file())\n",
    "        return\n",
    "\n",
    "    def get_new_parquet_file(self):\n",
    "        '''return a path to a new file with name uniqueness'''\n",
    "        return f\"{self.base_file_name}/{self.table_name}-{self._get_num_parquets()}.parquet\"\n",
    "\n",
    "    def _get_num_parquets(self):\n",
    "        return len(self.get_all_parquet_paths())\n",
    "\n",
    "    def get_all_parquet_paths(self):\n",
    "        return glob.glob(f\"{self.base_file_name}/*.parquet\")\n",
    "    \n",
    "    def start_stream(self):\n",
    "        \"\"\"Start streaming mode for efficient batch writes.\n",
    "        \n",
    "        After calling this, append_data() will write to a single parquet file\n",
    "        using ParquetWriter (streaming) instead of creating separate files.\n",
    "        Must call stop_stream() when done to properly close the writer.\n",
    "        \n",
    "        If called multiple times, closes any existing writer and starts a new stream.\n",
    "        \n",
    "        Can also be used as a context manager:\n",
    "            with output_db:\n",
    "                output_db.append_data(df1)\n",
    "                output_db.append_data(df2)\n",
    "            # Automatically stops streaming\n",
    "        \"\"\"\n",
    "        # Close existing writer if streaming was already active\n",
    "        if self._streaming and self._stream_writer is not None:\n",
    "            self._stream_writer.close()\n",
    "        \n",
    "        # Initialize streaming state\n",
    "        self._streaming = True\n",
    "        self._stream_file_path = self.get_new_parquet_file()\n",
    "        self._stream_writer = None  # Will be created lazily on first append_data()\n",
    "    \n",
    "    def stop_stream(self):\n",
    "        \"\"\"Stop streaming mode and close the ParquetWriter.\n",
    "        \n",
    "        Safe to call multiple times or if streaming was never started.\n",
    "        \"\"\"\n",
    "        if self._stream_writer is not None:\n",
    "            self._stream_writer.close()\n",
    "            self._stream_writer = None\n",
    "        \n",
    "        self._streaming = False\n",
    "        self._stream_file_path = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry: start streaming mode.\"\"\"\n",
    "        self.start_stream()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit: stop streaming mode.\"\"\"\n",
    "        self.stop_stream()\n",
    "        return False  # Don't suppress exceptions\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Destructor: ensure stream is closed if not explicitly stopped.\"\"\"\n",
    "        # Safety net: close writer if streaming was left open\n",
    "        if self._streaming and self._stream_writer is not None:\n",
    "            try:\n",
    "                self._stream_writer.close()\n",
    "            except:\n",
    "                pass  # Ignore errors during cleanup\n",
    "\n",
    "    def table_metadata(self):\n",
    "        \"\"\"Return total rows and total byte size of the table without loading data.\"\"\"\n",
    "        parquet_files = glob.glob(f\"{self.base_file_name}/*.parquet\")\n",
    "\n",
    "        total_rows = 0\n",
    "        total_bytes = 0\n",
    "\n",
    "        for file in parquet_files:\n",
    "            pf = pq.ParquetFile(file)\n",
    "            meta = pf.metadata\n",
    "\n",
    "            total_rows += meta.num_rows\n",
    "            total_bytes += meta.serialized_size  # includes footer + metadata\n",
    "\n",
    "        return {\n",
    "            \"num_files\": len(parquet_files),\n",
    "            \"total_rows\": total_rows,\n",
    "            \"total_compressed_bytes\": total_bytes,\n",
    "        }\n",
    "\n",
    "    def table_disk_usage(self):\n",
    "        parquet_files = glob.glob(f\"{self.base_file_name}/*.parquet\")\n",
    "\n",
    "        total_bytes = sum(os.path.getsize(f) for f in parquet_files)\n",
    "\n",
    "        return {\n",
    "            \"num_files\": len(parquet_files),\n",
    "            \"total_bytes\": total_bytes\n",
    "        }\n",
    "\n",
    "    def iter_pages(self, rows_per_batch: int = 100_000, columns=None, as_pandas=True):\n",
    "        for path in self.get_all_parquet_paths():        \n",
    "            pf = pq.ParquetFile(path)\n",
    "            for batch in pf.iter_batches(batch_size=rows_per_batch, columns=columns):\n",
    "                yield batch.to_pandas() if as_pandas else batch\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        # load both tables into memory\n",
    "        df1 = pd.read_parquet(self.base_file_name)\n",
    "        df2 = pd.read_parquet(other.base_file_name)\n",
    "\n",
    "        # check for row-wise equality\n",
    "        return df1.equals(df2)\n",
    "\n",
    "    @staticmethod\n",
    "    def fits_in_12GB(bytes_needed: int) -> bool:\n",
    "        TWELVE_GB = 12 * 1024**3\n",
    "        return bytes_needed <= TWELVE_GB\n",
    "\n",
    "    @staticmethod\n",
    "    def can_process_parquet(bytes_on_disk: int, compression_factor: int = 5) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if a Parquet dataset of `bytes_on_disk` can be processed\n",
    "        within 12 GB of RAM, after accounting for decompression expansion.\n",
    "        \"\"\"\n",
    "        estimated_ram = bytes_on_disk * compression_factor\n",
    "        TWELVE_GB = 12 * 1024**3\n",
    "        return estimated_ram <= TWELVE_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tables...\n",
      "Removing existing data directory\n",
      "Tables built successfully.\n",
      "peak memory: 141.44 MiB, increment: 2.53 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "print(\"Building tables...\")\n",
    "if os.path.exists('data'):\n",
    "    print(\"Removing existing data directory\")\n",
    "    shutil.rmtree('data')\n",
    "\n",
    "sizes = [\"100MB\", \"1GB\", \"10GB\"]\n",
    "tables = {}\n",
    "for size in sizes:\n",
    "    for table_name in [\"Songs\", \"Users\", \"Listens\"]:\n",
    "        key = f\"{table_name}_{size}\"\n",
    "        tables[key] = ColumnarDbFile(f\"{table_name}_{size}\", file_dir='data')\n",
    "        parquet_path = f\"{table_name.lower()}_{size}.parquet\"\n",
    "        assert os.path.exists(parquet_path)\n",
    "        tables[key].build_table(parquet_path)\n",
    "\n",
    "print(\"Tables built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "LfutqrA2hWkM",
    "outputId": "3fdee8d3-19d1-404a-cbb8-312fc4ff8485"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Song_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Song_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Song_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Song_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Song_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>Song_9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>Song_9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>Song_9997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>Song_9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>Song_9999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      song_id      title\n",
       "0           0     Song_0\n",
       "1           1     Song_1\n",
       "2           2     Song_2\n",
       "3           3     Song_3\n",
       "4           4     Song_4\n",
       "...       ...        ...\n",
       "9995     9995  Song_9995\n",
       "9996     9996  Song_9996\n",
       "9997     9997  Song_9997\n",
       "9998     9998  Song_9998\n",
       "9999     9999  Song_9999\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve data\n",
    "tables['Songs_100MB'].retrieve_data(columns = ['song_id', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ZiPY1Hs9hWkM",
    "outputId": "d1bad27a-9419-41f8-94b9-1aa6abe5d6bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listen_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>song_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19936</td>\n",
       "      <td>7687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37756</td>\n",
       "      <td>9045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>35676</td>\n",
       "      <td>3593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18861</td>\n",
       "      <td>2977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9826</td>\n",
       "      <td>4653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>999995</td>\n",
       "      <td>15502</td>\n",
       "      <td>4168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>999996</td>\n",
       "      <td>1562</td>\n",
       "      <td>1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>999997</td>\n",
       "      <td>5838</td>\n",
       "      <td>2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>999998</td>\n",
       "      <td>35276</td>\n",
       "      <td>1541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>999999</td>\n",
       "      <td>24936</td>\n",
       "      <td>6419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        listen_id  user_id  song_id\n",
       "0               0    19936     7687\n",
       "1               1    37756     9045\n",
       "2               2    35676     3593\n",
       "3               3    18861     2977\n",
       "4               4     9826     4653\n",
       "...           ...      ...      ...\n",
       "999995     999995    15502     4168\n",
       "999996     999996     1562     1217\n",
       "999997     999997     5838     2871\n",
       "999998     999998    35276     1541\n",
       "999999     999999    24936     6419\n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables['Listens_100MB'].retrieve_data(columns = ['listen_id', 'user_id', 'song_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtLVO3cChWkM"
   },
   "source": [
    "Analyze and report on:\n",
    "- Space efficiency compared to row storage\n",
    "  - e.g. Compare file sizes on disk: How much disk space does Parquet use vs. a row storage format like CSV?\n",
    "- Compression ratios achieved with Parquet\n",
    "  - e.g. Compare Parquet’s uncompressed encoded size (reported in its metadata) to its compressed on-disk size to compute compression ratios.\n",
    "  - You could also report the memory expansion factor: how much larger the dataset becomes when loaded into a `pd.DataFrame` compared to the compressed file size.\n",
    "- Read/write performance characteristics\n",
    "  - e.g. Read performance: How long does it take to read all columns from Parquet vs. CSV?\n",
    "  - e.g. Columnar advantage: How long does it take to read selective columns from Parquet vs. reading all columns?\n",
    "  - e.g. Write performance: How long does it take to write data to Parquet vs. CSV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7kfNisQFhWkM"
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def analyze(size=\"100MB\"):\n",
    "    \"\"\"Analyze storage efficiency, compression, and read/write performance.\"\"\"\n",
    "\n",
    "    table_files = {\n",
    "        \"Songs\": f\"songs_{size}.parquet\",\n",
    "        \"Users\": f\"users_{size}.parquet\",\n",
    "        \"Listens\": f\"listens_{size}.parquet\",\n",
    "    }\n",
    "\n",
    "    report_rows = []\n",
    "\n",
    "    for table_name, parquet_file in table_files.items():\n",
    "        parquet_path = Path(parquet_file)\n",
    "\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        mem_usage_bytes = df.memory_usage(deep=True).sum() # memory usage of the dataframe\n",
    "        parquet_size_bytes = parquet_path.stat().st_size # size of the parquet file on disk\n",
    "\n",
    "        parquet_file_obj = pq.ParquetFile(parquet_path)\n",
    "        metadata = parquet_file_obj.metadata\n",
    "        uncompressed_bytes = 0\n",
    "\n",
    "        # iterate over all row groups and columns to get the total uncompressed size of the parquet file\n",
    "        for rg_idx in range(metadata.num_row_groups):\n",
    "            row_group = metadata.row_group(rg_idx)\n",
    "            for col_idx in range(row_group.num_columns):\n",
    "                column_meta = row_group.column(col_idx)\n",
    "                if column_meta.total_uncompressed_size is not None:\n",
    "                    uncompressed_bytes += column_meta.total_uncompressed_size\n",
    "\n",
    "        # calculate compression ratio and memory expansion\n",
    "        compression_ratio = (\n",
    "            uncompressed_bytes / parquet_size_bytes\n",
    "        )\n",
    "        memory_expansion = (\n",
    "            mem_usage_bytes / parquet_size_bytes\n",
    "        )\n",
    "\n",
    "        # test reading speed of parquet file vs csv, for all columns and selective columns\n",
    "        # pick 1 less than the total number of columns to test reading selective columns\n",
    "        subset_columns = list(df.columns)[0:len(df.columns)-1]\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmpdir_path = Path(tmpdir)\n",
    "\n",
    "            csv_path = tmpdir_path / f\"{parquet_path.stem}.csv\"\n",
    "            start = time.perf_counter()\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            write_csv_time = time.perf_counter() - start\n",
    "            csv_size_bytes = csv_path.stat().st_size\n",
    "\n",
    "            parquet_tmp_path = tmpdir_path / f\"{parquet_path.stem}.parquet\"\n",
    "            start = time.perf_counter()\n",
    "            df.to_parquet(parquet_tmp_path, index=False)\n",
    "            write_parquet_time = time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            _ = pd.read_parquet(parquet_path)\n",
    "            read_parquet_all = time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            _ = pd.read_csv(csv_path)\n",
    "            read_csv_all = time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            _ = pd.read_parquet(parquet_path, columns=subset_columns)\n",
    "            read_parquet_subset = time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            _ = pd.read_csv(csv_path, usecols=subset_columns)\n",
    "            read_csv_subset = time.perf_counter() - start\n",
    "\n",
    "        size_saving_pct = (\n",
    "            100.0 * (1 - parquet_size_bytes / csv_size_bytes)\n",
    "        )\n",
    "\n",
    "        # append the results to the report\n",
    "        report_rows.append(\n",
    "            {\n",
    "                \"table\": table_name,\n",
    "                \"parquet_size_mb\": parquet_size_bytes / (1024 ** 2),\n",
    "                \"csv_size_mb\": csv_size_bytes / (1024 ** 2),\n",
    "                \"size_saving_pct\": size_saving_pct,\n",
    "                \"compression_ratio\": compression_ratio,\n",
    "                \"memory_expansion\": memory_expansion,\n",
    "                \"read_parquet_all_s\": read_parquet_all,\n",
    "                \"read_csv_all_s\": read_csv_all,\n",
    "                \"read_parquet_subset_s\": read_parquet_subset,\n",
    "                \"read_csv_subset_s\": read_csv_subset,\n",
    "                \"write_parquet_s\": write_parquet_time,\n",
    "                \"write_csv_s\": write_csv_time,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "    summary = pd.DataFrame(report_rows)\n",
    "    print(\"Analysis Summary for Tables of Size \" + size + \" (sizes in MB, times in seconds):\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "Xkw4z1GMhXsH",
    "outputId": "39eebc07-7084-40c3-b28d-25ac2ecb8dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Summary for Tables of Size 100MB (sizes in MB, times in seconds):\n",
      "Method 'analyze' took 9.9829 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>parquet_size_mb</th>\n",
       "      <th>csv_size_mb</th>\n",
       "      <th>size_saving_pct</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>memory_expansion</th>\n",
       "      <th>read_parquet_all_s</th>\n",
       "      <th>read_csv_all_s</th>\n",
       "      <th>read_parquet_subset_s</th>\n",
       "      <th>read_csv_subset_s</th>\n",
       "      <th>write_parquet_s</th>\n",
       "      <th>write_csv_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Songs</td>\n",
       "      <td>4.271927</td>\n",
       "      <td>9.773173</td>\n",
       "      <td>56.289255</td>\n",
       "      <td>2.415910</td>\n",
       "      <td>3.473430</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0.073592</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.062106</td>\n",
       "      <td>0.027642</td>\n",
       "      <td>0.097124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Users</td>\n",
       "      <td>20.347857</td>\n",
       "      <td>48.579238</td>\n",
       "      <td>58.114089</td>\n",
       "      <td>2.471382</td>\n",
       "      <td>3.529207</td>\n",
       "      <td>0.038628</td>\n",
       "      <td>0.340593</td>\n",
       "      <td>0.034948</td>\n",
       "      <td>0.308032</td>\n",
       "      <td>0.090254</td>\n",
       "      <td>0.443893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Listens</td>\n",
       "      <td>79.926873</td>\n",
       "      <td>178.866784</td>\n",
       "      <td>55.314860</td>\n",
       "      <td>2.432530</td>\n",
       "      <td>8.042059</td>\n",
       "      <td>0.256465</td>\n",
       "      <td>1.740107</td>\n",
       "      <td>0.293358</td>\n",
       "      <td>1.624935</td>\n",
       "      <td>0.574689</td>\n",
       "      <td>2.668126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     table  parquet_size_mb  csv_size_mb  size_saving_pct  compression_ratio  \\\n",
       "0    Songs         4.271927     9.773173        56.289255           2.415910   \n",
       "1    Users        20.347857    48.579238        58.114089           2.471382   \n",
       "2  Listens        79.926873   178.866784        55.314860           2.432530   \n",
       "\n",
       "   memory_expansion  read_parquet_all_s  read_csv_all_s  \\\n",
       "0          3.473430            0.007974        0.073592   \n",
       "1          3.529207            0.038628        0.340593   \n",
       "2          8.042059            0.256465        1.740107   \n",
       "\n",
       "   read_parquet_subset_s  read_csv_subset_s  write_parquet_s  write_csv_s  \n",
       "0               0.007648           0.062106         0.027642     0.097124  \n",
       "1               0.034948           0.308032         0.090254     0.443893  \n",
       "2               0.293358           1.624935         0.574689     2.668126  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(analyze(size=\"100MB\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across all three tables, Parquet is markedly more space-efficient than row-oriented CSV: file sizes drop by ~55–58% (e.g., Listens 79.9 MB vs 178.9 MB), corresponding to compression ratios of about 2.4–2.5× relative to the uncompressed Parquet data. Loading into pandas expands data substantially beyond disk size (≈3.5× for Songs/Users and ≈8× for Listens), highlighting the in-memory cost of wide/large tables. Performance-wise, Parquet reads are much faster than CSV both for full scans and column subsets—about 7–9× speedup for full reads and ~5–9× for selective reads—while Parquet writes are also faster, roughly 3.5–5× quicker than writing CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8U3edewiDBa"
   },
   "source": [
    "# Section 2: Parse SQL Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_KoWHLohWkM"
   },
   "source": [
    "In this section, you should implement logic to parse the following SQL query:\n",
    "```sql\n",
    "    SELECT s.song_id, AVG(u.age) AS avg_age,\n",
    "       COUNT(DISTINCT l.user_id) AS count_distinct_users,\n",
    "    FROM Songs s\n",
    "    JOIN Listens l ON s.song_id = l.song_id\n",
    "    JOIN Users u ON l.user_id = u.user_id\n",
    "    GROUP BY s.song_id, s.title\n",
    "    ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
    "```\n",
    "\n",
    "You should manually extract the components from the provided query (i.e. you don't need to implement a general SQL parser, just handle this specific query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TT3jWKFYhWkN"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT s.song_id, AVG(u.age) AS avg_age,\n",
    "COUNT(DISTINCT l.user_id)\n",
    "FROM Songs s\n",
    "JOIN Listens l ON s.song_id = l.song_id\n",
    "JOIN Users u ON l.user_id = u.user_id\n",
    "GROUP BY s.song_id, s.title\n",
    "ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "J1PmMhCRhv0r"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import re\n",
    "\n",
    "def parse_tables(query):\n",
    "\n",
    "    # pattern matches: \"from songs s\" or \"join listens l\"\n",
    "    pattern = r\"(from|join)\\s+([a-z_]+)\\s+([a-z])\"\n",
    "\n",
    "    matches = re.findall(pattern, query)\n",
    "\n",
    "    tables = {}\n",
    "    for _, table_name, alias in matches:\n",
    "        tables[alias] = table_name\n",
    "\n",
    "    return tables\n",
    "\n",
    "def parse_joins(query):\n",
    "\n",
    "    # 1) Get the base table from the FROM clause\n",
    "    base_match = re.search(r\"from\\s+([a-z_]+)\\s+([a-z])\", query)\n",
    "    if not base_match:\n",
    "        raise ValueError(\"Could not find FROM clause\")\n",
    "\n",
    "    base_table_name = base_match.group(1)\n",
    "    base_alias = base_match.group(2)\n",
    "    base_table = (base_alias, base_table_name)\n",
    "\n",
    "    # 2) Get each JOIN clause, in order\n",
    "    # pattern matches:\n",
    "    #   join listens l on s.song_id = l.song_id\n",
    "    join_pattern = (\n",
    "        r\"join\\s+([a-z_]+)\\s+([a-z])\\s+on\\s+\"\n",
    "        r\"([a-z])\\.([a-z_]+)\\s*=\\s*([a-z])\\.([a-z_]+)\"\n",
    "    )\n",
    "\n",
    "    joins = []\n",
    "    for m in re.finditer(join_pattern, query):\n",
    "        joined_table_name = m.group(1)\n",
    "        joined_alias = m.group(2)\n",
    "        left_alias = m.group(3)\n",
    "        left_col = m.group(4)\n",
    "        right_alias = m.group(5)\n",
    "        right_col = m.group(6)\n",
    "\n",
    "        joins.append(\n",
    "            {\n",
    "                \"joined_table_alias\": joined_alias,\n",
    "                \"joined_table_name\": joined_table_name,\n",
    "                \"left_alias\": left_alias,\n",
    "                \"left_column\": left_col,\n",
    "                \"right_alias\": right_alias,\n",
    "                \"right_column\": right_col,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\"base_table\" : base_table, \"Joins\" : joins}\n",
    "\n",
    "\n",
    "def parse_group_by(query):\n",
    "    \"\"\"\n",
    "    Return GROUP BY columns as a list of (alias, column) tuples.\n",
    "    Example: [('s', 'song_id'), ('s', 'title')]\n",
    "    \"\"\"\n",
    "    q = query.lower()\n",
    "\n",
    "    # Capture whatever is between GROUP BY and ORDER BY/semicolon/end\n",
    "    match = re.search(r\"group\\s+by\\s+(.+?)(order\\s+by|;|$)\", q, re.DOTALL)\n",
    "    if not match:\n",
    "        return []\n",
    "\n",
    "    groupby_text = match.group(1).strip()\n",
    "\n",
    "    columns = []\n",
    "    for col in groupby_text.split(\",\"):\n",
    "        col = col.strip()\n",
    "\n",
    "        # Expect pattern: alias.column\n",
    "        alias, column = col.split(\".\")\n",
    "        columns.append((alias, column))\n",
    "\n",
    "    return columns\n",
    "\n",
    "def parse_select_and_aggregations(query):\n",
    "    \"\"\"\n",
    "    Build:\n",
    "      aggregations: {agg_key: {...}}\n",
    "      select: list of items that may refer to agg_key\n",
    "    \"\"\"\n",
    "    q = query.lower()\n",
    "\n",
    "    m = re.search(r\"select\\s+(.+?)\\s+from\", q, re.DOTALL)\n",
    "    if not m:\n",
    "        return [], {}\n",
    "\n",
    "    select_text = m.group(1).strip()\n",
    "    raw_items = [item.strip() for item in select_text.split(\",\") if item.strip()]\n",
    "\n",
    "    select_list = []\n",
    "    aggregations = {}\n",
    "    agg_id = 1\n",
    "\n",
    "    for idx, item in enumerate(raw_items, start=1):\n",
    "        # AVG(...)\n",
    "        if item.startswith(\"avg(\"):\n",
    "            m_avg = re.match(\n",
    "                r\"avg\\(\\s*([a-z])\\.([a-z_]+)\\s*\\)(\\s+as\\s+([a-z_]+))?\",\n",
    "                item\n",
    "            )\n",
    "            if not m_avg:\n",
    "                raise ValueError(f\"Could not parse AVG aggregation: {item}\")\n",
    "            alias_letter = m_avg.group(1)\n",
    "            col_name = m_avg.group(2)\n",
    "            out_alias = m_avg.group(4) if m_avg.group(4) else None\n",
    "\n",
    "            aggregations[agg_id] = {\n",
    "                \"func\": \"avg\",\n",
    "                \"source\": (alias_letter, col_name),\n",
    "                \"distinct\": False,\n",
    "                \"output_name\": out_alias,\n",
    "            }\n",
    "\n",
    "            select_list.append(\n",
    "                {\n",
    "                    \"kind\": \"aggregation\",\n",
    "                    \"agg_key\": agg_id,\n",
    "                    \"alias\": out_alias,\n",
    "\n",
    "                }\n",
    "            )\n",
    "            agg_id += 1\n",
    "\n",
    "        # COUNT(DISTINCT ...)\n",
    "        elif item.startswith(\"count(\"):\n",
    "            m_cnt = re.match(\n",
    "                r\"count\\(\\s*distinct\\s+([a-z])\\.([a-z_]+)\\s*\\)(\\s+as\\s+([a-z_]+))?\",\n",
    "                item\n",
    "            )\n",
    "            if not m_cnt:\n",
    "                raise ValueError(f\"Could not parse COUNT aggregation: {item}\")\n",
    "            alias_letter = m_cnt.group(1)\n",
    "            col_name = m_cnt.group(2)\n",
    "            out_alias = m_cnt.group(4) if m_cnt.group(4) else None\n",
    "\n",
    "            aggregations[agg_id] = {\n",
    "                \"func\": \"count\",\n",
    "                \"source\": (alias_letter, col_name),\n",
    "                \"distinct\": True,\n",
    "                \"output_name\": out_alias,\n",
    "            }\n",
    "\n",
    "            select_list.append(\n",
    "                {\n",
    "                    \"kind\": \"aggregation\",\n",
    "                    \"agg_key\": agg_id,\n",
    "                    \"alias\": out_alias,\n",
    "                }\n",
    "            )\n",
    "            agg_id += 1\n",
    "\n",
    "        # Plain column: alias.column\n",
    "        else:\n",
    "            alias_letter, col_name = item.split(\".\")\n",
    "            select_list.append(\n",
    "                {\n",
    "                    \"kind\": \"column\",\n",
    "                    \"source\": (alias_letter, col_name),\n",
    "                    \"alias\": None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return select_list, aggregations\n",
    "\n",
    "\n",
    "def parse_order_by(query, aggregations):\n",
    "    \"\"\"\n",
    "    Build order_by list where entries can refer to aggregations via agg_key.\n",
    "    \"\"\"\n",
    "    q = query.lower()\n",
    "\n",
    "    m = re.search(r\"order\\s+by\\s+(.+?)(;|$)\", q, re.DOTALL)\n",
    "    if not m:\n",
    "        return []\n",
    "\n",
    "    order_text = m.group(1).strip()\n",
    "    raw_items = [item.strip() for item in order_text.split(\",\") if item.strip()]\n",
    "\n",
    "    order_by = []\n",
    "\n",
    "    for item in raw_items:\n",
    "        direction = \"asc\"\n",
    "        expr = item\n",
    "\n",
    "        if expr.endswith(\" desc\"):\n",
    "            direction = \"desc\"\n",
    "            expr = expr[:-5].strip()\n",
    "        elif expr.endswith(\" asc\"):\n",
    "            direction = \"asc\"\n",
    "            expr = expr[:-4].strip()\n",
    "\n",
    "        # COUNT(DISTINCT ...) → match an aggregation\n",
    "        if expr.startswith(\"count(\"):\n",
    "            m_cnt = re.match(\n",
    "                r\"count\\(\\s*distinct\\s+([a-z])\\.([a-z_]+)\\s*\\)\",\n",
    "                expr\n",
    "            )\n",
    "            if not m_cnt:\n",
    "                raise ValueError(f\"Could not parse ORDER BY aggregation: {expr}\")\n",
    "            src = (m_cnt.group(1), m_cnt.group(2))\n",
    "\n",
    "            agg_key = None\n",
    "            for k, agg in aggregations.items():\n",
    "                if (\n",
    "                    agg[\"func\"] == \"count\"\n",
    "                    and agg[\"distinct\"]\n",
    "                    and agg[\"source\"] == src\n",
    "                ):\n",
    "                    agg_key = k\n",
    "                    break\n",
    "\n",
    "            if agg_key is None:\n",
    "                raise ValueError(f\"No matching aggregation found for ORDER BY expr: {expr}\")\n",
    "\n",
    "            order_by.append(\n",
    "                {\n",
    "                    \"kind\": \"aggregation\",\n",
    "                    \"agg_key\": agg_key,\n",
    "                    \"direction\": direction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # assume plain column: alias.column\n",
    "            alias_letter, col_name = expr.split(\".\")\n",
    "            order_by.append(\n",
    "                {\n",
    "                    \"kind\": \"column\",\n",
    "                    \"source\": (alias_letter, col_name),\n",
    "                    \"direction\": direction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return order_by\n",
    "\n",
    "@timer\n",
    "def parse_sql(query):\n",
    "    \"\"\"\n",
    "    YOUR TASK: Extract tables, joins, and aggregations\n",
    "    \"\"\"\n",
    "    # Parse SQL string to identify:\n",
    "    # - Tables involved\n",
    "    # - Join conditions\n",
    "    # - GROUP BY columns\n",
    "    # - Aggregation functions\n",
    "    # Your implementation here\n",
    "    query = query.lower()\n",
    "    output = {}\n",
    "\n",
    "    output[\"tables\"] = parse_tables(query)\n",
    "    output[\"joins\"] = parse_joins(query)\n",
    "    output[\"GroupBy\"] = parse_group_by(query)\n",
    "    output[\"select\"], output[\"aggregations\"] = parse_select_and_aggregations(query)\n",
    "    output[\"orderBy\"] = parse_order_by(query, output[\"aggregations\"])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8mb80aeTAxM",
    "outputId": "02681db1-41c7-4339-907a-7c377b821792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 'parse_sql' took 0.0024 seconds.\n",
      "tables: {'s': 'songs', 'l': 'listens', 'u': 'users'}\n",
      "joins: {'base_table': ('s', 'songs'), 'Joins': [{'joined_table_alias': 'l', 'joined_table_name': 'listens', 'left_alias': 's', 'left_column': 'song_id', 'right_alias': 'l', 'right_column': 'song_id'}, {'joined_table_alias': 'u', 'joined_table_name': 'users', 'left_alias': 'l', 'left_column': 'user_id', 'right_alias': 'u', 'right_column': 'user_id'}]}\n",
      "GroupBy: [('s', 'song_id'), ('s', 'title')]\n",
      "select: [{'kind': 'column', 'source': ('s', 'song_id'), 'alias': None}, {'kind': 'aggregation', 'agg_key': 1, 'alias': 'avg_age'}, {'kind': 'aggregation', 'agg_key': 2, 'alias': None}]\n",
      "aggregations: {1: {'func': 'avg', 'source': ('u', 'age'), 'distinct': False, 'output_name': 'avg_age'}, 2: {'func': 'count', 'source': ('l', 'user_id'), 'distinct': True, 'output_name': None}}\n",
      "orderBy: [{'kind': 'aggregation', 'agg_key': 2, 'direction': 'desc'}, {'kind': 'column', 'source': ('s', 'song_id'), 'direction': 'asc'}]\n"
     ]
    }
   ],
   "source": [
    "output = parse_sql(query)\n",
    "for key, value in output.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "071jzSZqkGyC"
   },
   "source": [
    "# Section 3: Implement Join Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14LszEqZhWkN"
   },
   "source": [
    "In this section, you will implement the execution operators (*how* to join) and aggregation after joins.\n",
    "\n",
    "**Reminder:** If you use temporary files or folders, you should clean them up either as part of your join logic, or after each run. Otherwise you might run into correctness issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1AVO_NRnkHq1"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def HASHVALUE(value, B):\n",
    "    if isinstance(value, int):\n",
    "        return hash(value) % B\n",
    "    sha256 = hashlib.sha256()\n",
    "    sha256.update(str(value).encode(\"utf-8\"))\n",
    "    return int(sha256.hexdigest(), 16) % B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_partition(\n",
    "    table: ColumnarDbFile,\n",
    "    hash_keys: List[str],\n",
    "    num_partitions: int,\n",
    "    parquet_batch_size: int,\n",
    "    hash_value_fn: Callable[[object, int], int],\n",
    "    make_partition_path_fn: Callable[[int], str],\n",
    "    columns: Optional[List[str]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hash-partition `table` into `num_partitions` Parquet files.\n",
    "\n",
    "    - `hash_keys` is a list of column names (one or more).\n",
    "    - If len(hash_keys) > 1, we build a temporary concatenated column `_hash_key`\n",
    "      and hash on that.\n",
    "    - `hash_value_fn(key, num_partitions)` returns an int in [0, num_partitions).\n",
    "    - `columns` are the columns to write into each partition.\n",
    "      All `hash_keys` are automatically included in `columns`.\n",
    "    \"\"\"\n",
    "    is_multi_col = len(hash_keys) > 1\n",
    "    hash_col_name = \"_hash_key\" if is_multi_col else hash_keys[0]\n",
    "\n",
    "    # Normalize and ensure hash_keys are included in the columns we read & write\n",
    "    if columns:\n",
    "        for col in hash_keys:\n",
    "            if col not in columns:\n",
    "                columns.append(col)\n",
    "\n",
    "    writers: Dict[int, pq.ParquetWriter] = {}\n",
    "    for batch_df in table.iter_pages(columns=columns, rows_per_batch=parquet_batch_size):\n",
    "        # If multiple hash columns, build a temporary concatenated key column\n",
    "        if is_multi_col:\n",
    "            batch_df[hash_col_name] = (\n",
    "                batch_df[hash_keys]\n",
    "                .astype(str)\n",
    "                .agg(\"|\".join, axis=1)\n",
    "            )\n",
    "\n",
    "        # Compute partition id\n",
    "        batch_df[\"_part\"] = batch_df[hash_col_name].apply(\n",
    "            lambda x: hash_value_fn(x, num_partitions)\n",
    "        )\n",
    "\n",
    "        if is_multi_col:\n",
    "            batch_df = batch_df.drop(columns=hash_col_name)\n",
    "        if columns:\n",
    "            batch_df = batch_df[columns + [\"_part\"]]\n",
    "\n",
    "        # Group rows by partition and write each group\n",
    "        for part_id, part_df in batch_df.groupby(\"_part\"):\n",
    "            part_df = part_df.drop(columns=[\"_part\"])\n",
    "\n",
    "            part_table = pa.Table.from_pandas(part_df, preserve_index=False)\n",
    "\n",
    "            writer = writers.get(part_id)\n",
    "            if writer is None:\n",
    "                part_path = make_partition_path_fn(part_id)\n",
    "                writer = pq.ParquetWriter(part_path, part_table.schema)\n",
    "                writers[part_id] = writer\n",
    "\n",
    "            writer.write_table(part_table)\n",
    "\n",
    "    for w in writers.values():\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q46rgYihWkN"
   },
   "source": [
    "Implement `HashPartitionJoin`:\n",
    "1. Hash partition both tables\n",
    "2. Build hash table from smaller partition\n",
    "3. Probe with larger partition\n",
    "4. Return joined results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastHashPartitionJoin:\n",
    "    def __init__(self, num_partitions=4, parquet_batch_size=100_000, use_streaming=False, time_it=True):\n",
    "        self.num_partitions = num_partitions\n",
    "        self.parquet_batch_size = parquet_batch_size\n",
    "        self.use_streaming = use_streaming\n",
    "        self.time_it = time_it\n",
    "    \n",
    "    @timer\n",
    "    def join(self, table1: ColumnarDbFile, table2: ColumnarDbFile, join_key1, join_key2,\n",
    "             temp_dir='temp', columns_table1=None, columns_table2=None):\n",
    "        \"\"\"\n",
    "        Perform an optimized hash partition join between two ColumnarDbFile instances.\n",
    "\n",
    "        Speed ups:\n",
    "        - load smaller table into memory and build hash map with pandas groupby, larger table is probed in batches\n",
    "        - vectorized join using numpy operations: see _vectorized_join method for more details\n",
    "        \"\"\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        # Partition both tables\n",
    "        self._hash_partition(table1, join_key1, temp_dir, 'left', columns_table1)\n",
    "        self._hash_partition(table2, join_key2, temp_dir, 'right', columns_table2)\n",
    "\n",
    "        output = ColumnarDbFile(f\"hpj_{table1.table_name}_{table2.table_name}\")\n",
    "        \n",
    "        # Clean up any existing files in the output directory\n",
    "        if os.path.exists(output.base_file_name):\n",
    "            for file_path in output.get_all_parquet_paths():\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        if self.use_streaming:\n",
    "            output.start_stream()\n",
    "\n",
    "        for part_id in range(self.num_partitions):\n",
    "            left_path = self._make_partition_path(temp_dir, \"left\", part_id)\n",
    "            right_path = self._make_partition_path(temp_dir, \"right\", part_id)\n",
    "\n",
    "            if not (os.path.exists(left_path) and os.path.exists(right_path)):\n",
    "                continue\n",
    "            \n",
    "            # Process this partition with batched reading\n",
    "            self._process_partition(\n",
    "                left_path, right_path, join_key1, join_key2, output\n",
    "            )\n",
    "\n",
    "        if self.use_streaming:\n",
    "            output.stop_stream()\n",
    "        \n",
    "        shutil.rmtree(temp_dir)\n",
    "        return output\n",
    "\n",
    "    def _process_partition(self, left_path, right_path, join_key1, join_key2, output):\n",
    "        \"\"\"\n",
    "        Process a partition an individual partition from left and right\n",
    "        Determine which side is smaller and build hash map from that side\n",
    "        Probe with larger side\n",
    "        \"\"\"\n",
    "        # Get metadata to determine which side is smaller\n",
    "        left_size = pq.ParquetFile(left_path).metadata.num_rows\n",
    "        right_size = pq.ParquetFile(right_path).metadata.num_rows\n",
    "        \n",
    "        if left_size <= right_size:\n",
    "            # Build hash map from left, probe with right\n",
    "            self._build_and_probe(left_path, right_path, join_key1, join_key2, \n",
    "                                  output, left_is_build=True)\n",
    "        else:\n",
    "            # Build hash map from right, probe with left\n",
    "            self._build_and_probe(right_path, left_path, join_key2, join_key1, \n",
    "                                  output, left_is_build=False)\n",
    "\n",
    "    def _build_and_probe(self, build_path, probe_path, build_key, probe_key, \n",
    "                         output, left_is_build):\n",
    "        \"\"\"\n",
    "        Build hash map from build side and probe with probe side using batched reading.\n",
    "        \"\"\"\n",
    "        # Build hash map from the smaller side (build side)\n",
    "        hash_map = self._build_hash_map(build_path, build_key)\n",
    "        \n",
    "        # Probe with the larger side in batches\n",
    "        probe_file = pq.ParquetFile(probe_path)\n",
    "        build_df = pq.read_table(build_path).to_pandas()\n",
    "        \n",
    "        for probe_batch in probe_file.iter_batches(batch_size=self.parquet_batch_size):\n",
    "            probe_df = probe_batch.to_pandas()\n",
    "            \n",
    "            # Vectorized join using numpy operations\n",
    "            joined_df = self._vectorized_join(\n",
    "                build_df, probe_df, hash_map, probe_key, left_is_build\n",
    "            )\n",
    "            \n",
    "            if not joined_df.empty:\n",
    "                output.append_data(joined_df)\n",
    "            \n",
    "            # Explicit memory cleanup\n",
    "            del probe_df\n",
    "            del joined_df\n",
    "            gc.collect()\n",
    "        \n",
    "        del build_df\n",
    "        del hash_map\n",
    "        gc.collect()\n",
    "\n",
    "    def _build_hash_map(self, file_path, key_column):\n",
    "        \"\"\"\n",
    "        Build an optimized hash map using numpy arrays for better performance.\n",
    "        Returns a dictionary mapping keys to numpy arrays of indices.\n",
    "        \"\"\"\n",
    "        df = pq.read_table(file_path).to_pandas()\n",
    "        \n",
    "        # Group indices by key using pandas groupby (much faster than manual loop)\n",
    "        grouped = df.reset_index().groupby(key_column)['index'].apply(np.array).to_dict()\n",
    "        \n",
    "        return grouped\n",
    "\n",
    "    def _vectorized_join(self, build_df, probe_df, hash_map, probe_key, left_is_build):\n",
    "        \"\"\"\n",
    "        Primary optimization using a vectorized join with vectorized join:\n",
    "        1. Get probe keys and find indeces of matches in hash map  \n",
    "        2. Establish a parrallel index for build and probe tables\n",
    "        3. Build result from parallel indices using advanced pandas indexing\n",
    "        \"\"\"\n",
    "        probe_keys = probe_df[probe_key].values\n",
    "        \n",
    "        build_indices = []\n",
    "        probe_indices = []\n",
    "        \n",
    "        # Build index for build and probe tables\n",
    "        for probe_idx, key in enumerate(probe_keys):\n",
    "            if key in hash_map:\n",
    "                build_idxs = hash_map[key]\n",
    "                build_indices.extend(build_idxs)\n",
    "                probe_indices.extend([probe_idx] * len(build_idxs))\n",
    "        \n",
    "        if not build_indices:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        build_indices = np.array(build_indices)\n",
    "        probe_indices = np.array(probe_indices)\n",
    "        \n",
    "        # Build result using advanced indexing\n",
    "        # advanced works as follows here:\n",
    "        # build_df.iloc[build_indices] -> get rows from build_df where index is in build_indices\n",
    "        # probe_df.iloc[probe_indices] -> get rows from probe_df where index is in probe_indices\n",
    "        # these lists are parallel, meaning that the row in position i in build_indices is joined \n",
    "        # with the row in position i in probe_indices\n",
    "        if left_is_build:\n",
    "            left_result = build_df.iloc[build_indices].reset_index(drop=True)\n",
    "            right_result = probe_df.iloc[probe_indices].reset_index(drop=True)\n",
    "        else:\n",
    "            left_result = probe_df.iloc[probe_indices].reset_index(drop=True)\n",
    "            right_result = build_df.iloc[build_indices].reset_index(drop=True)\n",
    "        \n",
    "        # Drop duplicate columns from right side (keeping left)\n",
    "        common_columns = set(left_result.columns) & set(right_result.columns)\n",
    "        if common_columns:\n",
    "            right_result = right_result.drop(columns=list(common_columns))\n",
    "\n",
    "        result = pd.concat([left_result, right_result], axis=1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _make_partition_path(self, output_dir, side, part_id):\n",
    "        return f\"{output_dir}/{side}_part{part_id}.parquet\"\n",
    "\n",
    "    @timer\n",
    "    def _hash_partition(self, table: ColumnarDbFile, join_key, output_dir, side, columns=None):\n",
    "        make_partition_path_fn = partial(self._make_partition_path, output_dir, side)\n",
    "        hash_partition(table, [join_key], self.num_partitions, self.parquet_batch_size,\n",
    "                       HASHVALUE, make_partition_path_fn, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berk/opt/miniforge3/envs/cs145-project2/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method '_hash_partition' took 0.0606 seconds.\n",
      "Method '_hash_partition' took 3.0541 seconds.\n",
      "Method '_build_hash_map' took 0.2388 seconds.\n",
      "Method '_build_hash_map' took 0.2324 seconds.\n",
      "Method '_build_hash_map' took 0.2306 seconds.\n",
      "Method '_build_hash_map' took 0.2346 seconds.\n",
      "Method 'join' took 11.6230 seconds.\n",
      "peak memory: 1486.02 MiB, increment: 1254.59 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "SIZE = \"1GB\" #[\"100MB\", \"1GB\", \"10GB\"]\n",
    "SAMPLE = 100\n",
    "USE_STREAMING = True\n",
    "\n",
    "songs_table = tables[f'Songs_{SIZE}']\n",
    "listens_table = tables[f'Listens_{SIZE}']\n",
    "\n",
    "# Select specific columns from each table\n",
    "songs_cols = ['song_id', 'title']\n",
    "listens_cols = ['listen_id', 'song_id', 'user_id']\n",
    "\n",
    "# Create HashPartitionJoin instance\n",
    "hpj1 = FastHashPartitionJoin(\n",
    "    num_partitions=4, \n",
    "    parquet_batch_size=1000000,\n",
    "    use_streaming=USE_STREAMING  \n",
    ")\n",
    "\n",
    "# Perform the join\n",
    "result_songs_listens = hpj1.join(\n",
    "    table1=songs_table,           \n",
    "    table2=listens_table,         \n",
    "    join_key1='song_id',          \n",
    "    join_key2='song_id',          \n",
    "    temp_dir='temp_songs_listens',\n",
    "    columns_table1=songs_cols,    \n",
    "    columns_table2=listens_cols   \n",
    ")\n",
    "\n",
    "result_df = result_songs_listens.retrieve_data(sample=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Comprehensive Hash Partition Join Test\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Test: Songs JOIN Listens \n",
      "======================================================================\n",
      "Method '_hash_partition' took 0.0205 seconds.\n",
      "Method '_hash_partition' took 0.3476 seconds.\n",
      "Method '_build_hash_map' took 0.0250 seconds.\n",
      "Method '_build_hash_map' took 0.0250 seconds.\n",
      "Method '_build_hash_map' took 0.0271 seconds.\n",
      "Method '_build_hash_map' took 0.0277 seconds.\n",
      "Method 'join' took 2.0240 seconds.\n",
      "\n",
      "HPJ result shape: (1000000, 4)\n",
      "pd.merge result shape: (1000000, 4)\n",
      "Row counts match!\n",
      "Columns match!\n",
      "Unique song_ids match!\n",
      "\n",
      "Performing full data value comparison...\n",
      "All data values match exactly!\n",
      "Verified 1000000 rows × 4 columns = 4000000 values\n",
      "Duplicate row counts match (0 duplicates)\n",
      "\n",
      " Test PASSED\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE TEST SUMMARY\n",
      "======================================================================\n",
      "✓ ALL TESTS PASSED: Hash Partition Join is CORRECT!\n",
      "  - Row counts match\n",
      "  - Column structure matches\n",
      "  - Unique keys match\n",
      "  - ALL DATA VALUES match exactly\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_STREAMING = True\n",
    "SIZE = \"100MB\"\n",
    "# Optional: Verify your implementation against pd.merge\n",
    "def test_hash_partition_join_comprehensive():\n",
    "    \"\"\"\n",
    "    Comprehensive test that validates both structure AND actual data values.\n",
    "    This ensures the HPJ implementation is truly correct.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"Comprehensive Hash Partition Join Test\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_tests_passed = True\n",
    "    \n",
    "    # Test: Songs JOIN Listens - FULL DATA VALIDATION\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Test: Songs JOIN Listens \")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    songs_table = tables[f'Songs_{SIZE}']\n",
    "    listens_table = tables[f'Listens_{SIZE}']\n",
    "    \n",
    "    songs_cols = ['song_id', 'title']\n",
    "    listens_cols = ['listen_id', 'song_id', 'user_id']\n",
    "    \n",
    "    # Perform joins\n",
    "    hpj1 = FastHashPartitionJoin(num_partitions=4, parquet_batch_size=100_000, use_streaming=USE_STREAMING)\n",
    "    result_table1 = hpj1.join(\n",
    "        songs_table, listens_table,\n",
    "        join_key1='song_id', join_key2='song_id',\n",
    "        temp_dir='temp_test_songs_listens_comp',\n",
    "        columns_table1=songs_cols,\n",
    "        columns_table2=listens_cols\n",
    "    )\n",
    "    \n",
    "    hpj_result1 = result_table1.retrieve_data()\n",
    "    \n",
    "    # Get pd.merge result\n",
    "    songs_df = songs_table.retrieve_data(columns=songs_cols)\n",
    "    listens_df = listens_table.retrieve_data(columns=listens_cols)\n",
    "    pd_result1 = pd.merge(songs_df, listens_df, on='song_id', how='inner')\n",
    "    \n",
    "    print(f\"\\nHPJ result shape: {hpj_result1.shape}\")\n",
    "    print(f\"pd.merge result shape: {pd_result1.shape}\")\n",
    "    \n",
    "    test1_passed = True\n",
    "    \n",
    "    # 1. Row count check\n",
    "    if len(hpj_result1) != len(pd_result1):\n",
    "        print(f\"Row count mismatch -- HPJ: {len(hpj_result1)}, pd.merge: {len(pd_result1)}\")\n",
    "        test1_passed = False\n",
    "        all_tests_passed = False\n",
    "    else:\n",
    "        print(\"Row counts match!\")\n",
    "    \n",
    "    # 2. Column check\n",
    "    hpj_cols = set(hpj_result1.columns)\n",
    "    pd_cols = set(pd_result1.columns)\n",
    "    if hpj_cols != pd_cols:\n",
    "        print(f\"Column mismatch -- HPJ: {hpj_cols}, pd.merge: {pd_cols}\")\n",
    "        test1_passed = False\n",
    "        all_tests_passed = False\n",
    "    else:\n",
    "        print(\"Columns match!\")\n",
    "    \n",
    "    if test1_passed:\n",
    "        # 3. Sort both results for comparison\n",
    "        sort_cols = ['song_id', 'listen_id'] if 'listen_id' in hpj_result1.columns else ['song_id']\n",
    "        hpj_sorted = hpj_result1.sort_values(sort_cols).reset_index(drop=True)\n",
    "        pd_sorted = pd_result1.sort_values(sort_cols).reset_index(drop=True)\n",
    "        \n",
    "        # 4. Check unique keys\n",
    "        hpj_song_ids = set(hpj_result1['song_id'].unique())\n",
    "        pd_song_ids = set(pd_result1['song_id'].unique())\n",
    "        if hpj_song_ids != pd_song_ids:\n",
    "            print(f\"Unique song_ids differ!\")\n",
    "            test1_passed = False\n",
    "            all_tests_passed = False\n",
    "        else:\n",
    "            print(\"Unique song_ids match!\")\n",
    "        \n",
    "        # 5. FULL DATA VALUE COMPARISON - This is the critical check!\n",
    "        print(\"\\nPerforming full data value comparison...\")\n",
    "        data_matches = True\n",
    "        \n",
    "        # Compare each column\n",
    "        for col in sorted(hpj_cols):\n",
    "            hpj_col_data = hpj_sorted[col].values\n",
    "            pd_col_data = pd_sorted[col].values\n",
    "            \n",
    "            # Use np.array_equal for exact comparison\n",
    "            if not np.array_equal(hpj_col_data, pd_col_data):\n",
    "                print(f\"Column '{col}' data mismatch\")\n",
    "                \n",
    "                # Find first mismatch\n",
    "                mismatch_idx = np.where(hpj_col_data != pd_col_data)[0]\n",
    "                if len(mismatch_idx) > 0:\n",
    "                    idx = mismatch_idx[0]\n",
    "                    print(f\"  First mismatch at row {idx}:\")\n",
    "                    print(f\"    HPJ: {hpj_col_data[idx]}\")\n",
    "                    print(f\"    pd.merge: {pd_col_data[idx]}\")\n",
    "                    print(f\"  Total mismatches: {len(mismatch_idx)}\")\n",
    "                \n",
    "                data_matches = False\n",
    "                break\n",
    "        \n",
    "        if data_matches:\n",
    "            print(\"All data values match exactly!\")\n",
    "            print(f\"Verified {len(hpj_sorted)} rows × {len(hpj_cols)} columns = {len(hpj_sorted) * len(hpj_cols)} values\")\n",
    "        else:\n",
    "            print(\"✗ Data values do NOT match!\")\n",
    "            test1_passed = False\n",
    "            all_tests_passed = False\n",
    "        \n",
    "        # 6. Check for duplicate rows (should be same in both)\n",
    "        hpj_duplicates = hpj_sorted.duplicated().sum()\n",
    "        pd_duplicates = pd_sorted.duplicated().sum()\n",
    "        if hpj_duplicates != pd_duplicates:\n",
    "            print(f\"Duplicate row counts differ (HPJ: {hpj_duplicates}, pd.merge: {pd_duplicates})\")\n",
    "        else:\n",
    "            print(f\"Duplicate row counts match ({hpj_duplicates} duplicates)\")\n",
    "    \n",
    "    if test1_passed:\n",
    "        print(\"\\n Test PASSED\")\n",
    "    else:\n",
    "        print(\"\\n Test FAILED!\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE TEST SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    if all_tests_passed:\n",
    "        print(\"✓ ALL TESTS PASSED: Hash Partition Join is CORRECT!\")\n",
    "        print(\"  - Row counts match\")\n",
    "        print(\"  - Column structure matches\")\n",
    "        print(\"  - Unique keys match\")\n",
    "        print(\"  - ALL DATA VALUES match exactly\")\n",
    "    else:\n",
    "        print(\"✗ TESTS FAILED: Implementation has issues\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return all_tests_passed\n",
    "\n",
    "test_hash_partition_join_comprehensive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hzzs-5K8hWkN"
   },
   "source": [
    "Implement `SortMergeJoin`:\n",
    "1. Sort both tables by join key\n",
    "2. Merge sorted sequences\n",
    "3. Handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetGroupChunkIter:\n",
    "    \"\"\"\n",
    "    Streams a Parquet file that is globally sorted by `join_key`,\n",
    "    yielding chunks of consecutive rows with the same key.\n",
    "    Each call to next_chunk() returns (table_chunk, key_value, is_last_for_key).\n",
    "    Memory bounded by IN_BATCH_ROWS and GROUP_CHUNK_ROWS.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: str, columns, join_key: str,\n",
    "                 in_batch_rows: int = 64_000, group_chunk_rows: int = 64_000):\n",
    "        self._pf = pq.ParquetFile(file_path)\n",
    "        self._cols = columns\n",
    "        self._join_key = join_key\n",
    "        self._key_idx = (self._cols.index(join_key) if self._cols is not None\n",
    "                 else self._pf.schema_arrow.get_field_index(join_key))\n",
    "        if self._key_idx == -1:\n",
    "            raise ValueError(f\"join key {join_key} not in schema\")\n",
    "        self._IN_BATCH_ROWS = in_batch_rows\n",
    "        self._GROUP_CHUNK_ROWS = group_chunk_rows\n",
    "\n",
    "        self._it = self._pf.iter_batches(columns=self._cols, batch_size=self._IN_BATCH_ROWS)\n",
    "        self._batch = None\n",
    "        self._i = 0\n",
    "        self._cur_key = None\n",
    "        self._eof = False\n",
    "\n",
    "    def _next_batch(self):\n",
    "        try:\n",
    "            self._batch = next(self._it)\n",
    "            self._i = 0\n",
    "        except StopIteration:\n",
    "            self._batch = None\n",
    "\n",
    "    def _skip_nulls(self):\n",
    "        while self._batch is not None:\n",
    "            key_arr = self._batch.column(self._key_idx)\n",
    "            n = self._batch.num_rows\n",
    "            while self._i < n and key_arr[self._i].as_py() is None:\n",
    "                self._i += 1\n",
    "            if self._i < n:\n",
    "                return\n",
    "            self._next_batch()\n",
    "\n",
    "    def next_chunk(self):\n",
    "        if self._eof:\n",
    "            return None, None, True\n",
    "\n",
    "        if self._batch is None:\n",
    "            self._next_batch()\n",
    "            if self._batch is None:\n",
    "                self._eof = True\n",
    "                return None, None, True\n",
    "\n",
    "        self._skip_nulls()\n",
    "        if self._batch is None:\n",
    "            self._eof = True\n",
    "            return None, None, True\n",
    "\n",
    "        if self._cur_key is None:\n",
    "            self._cur_key = self._batch.column(self._key_idx)[self._i].as_py()\n",
    "\n",
    "        parts = []\n",
    "        collected = 0\n",
    "        more_for_key = False\n",
    "\n",
    "        while collected < self._GROUP_CHUNK_ROWS:\n",
    "            if self._batch is None:\n",
    "                break\n",
    "\n",
    "            key_arr = self._batch.column(self._key_idx)\n",
    "            n = self._batch.num_rows\n",
    "            j = self._i\n",
    "\n",
    "            # take rows while the key matches and we stay under the chunk cap\n",
    "            while j < n and key_arr[j].as_py() == self._cur_key and \\\n",
    "                  (collected + (j - self._i)) < self._GROUP_CHUNK_ROWS:\n",
    "                j += 1\n",
    "\n",
    "            take = j - self._i\n",
    "            if take > 0:\n",
    "                parts.append(self._batch.slice(self._i, take))\n",
    "                collected += take\n",
    "                self._i = j\n",
    "\n",
    "            if collected >= self._GROUP_CHUNK_ROWS:\n",
    "                # capped chunk; if more rows with same key remain, mark continuation\n",
    "                if self._i < n and key_arr[self._i].as_py() == self._cur_key:\n",
    "                    more_for_key = True\n",
    "                else:\n",
    "                    # peek next batch start\n",
    "                    save_batch, save_i = self._batch, self._i\n",
    "                    self._next_batch()\n",
    "                    if self._batch is not None:\n",
    "                        k0 = self._batch.column(self._key_idx)[0].as_py()\n",
    "                        more_for_key = (k0 == self._cur_key)\n",
    "                    # restore for the caller to continue\n",
    "                    self._batch, self._i = save_batch, save_i\n",
    "                break\n",
    "\n",
    "            if self._i >= n:\n",
    "                # move to next batch and check if key continues\n",
    "                self._next_batch()\n",
    "                self._skip_nulls()\n",
    "                if self._batch is None:\n",
    "                    break\n",
    "                if self._batch.column(self._key_idx)[self._i].as_py() != self._cur_key:\n",
    "                    break\n",
    "            else:\n",
    "                # key changed within this batch\n",
    "                break\n",
    "\n",
    "        tbl = pa.Table.from_batches(parts) if parts else None\n",
    "\n",
    "        # if no more rows remain for this key after this chunk, clear cur_key\n",
    "        if not more_for_key:\n",
    "            if self._batch is not None:\n",
    "                key_arr = self._batch.column(self._key_idx)\n",
    "                n = self._batch.num_rows\n",
    "                while self._i < n and key_arr[self._i].as_py() == self._cur_key:\n",
    "                    self._i += 1\n",
    "                if self._i >= n:\n",
    "                    self._next_batch()\n",
    "            self._cur_key = None\n",
    "\n",
    "        key_for_chunk = None\n",
    "        if tbl is not None:\n",
    "            key_for_chunk = tbl.column(self._key_idx).chunk(0)[0].as_py() \\\n",
    "                if tbl.num_rows > 0 else None\n",
    "\n",
    "        return tbl, key_for_chunk, not more_for_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "odvuVpv2hWkN"
   },
   "outputs": [],
   "source": [
    "\n",
    "BWAY_MERGE_FACTOR = 10\n",
    "\n",
    "class SortMergeJoin:\n",
    "    def __init__(\n",
    "        self, bway_merge_factor: int = BWAY_MERGE_FACTOR, num_pages_per_split=100, verbose=False\n",
    "    ):\n",
    "        self.bway_merge_factor = bway_merge_factor\n",
    "        self.num_pages_per_split = num_pages_per_split\n",
    "        self.ascending = True\n",
    "        self.tiebreak_key = None\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @timer\n",
    "    def _streaming_inner_join(self,\n",
    "                            left_sorted: ColumnarDbFile,\n",
    "                            right_sorted: ColumnarDbFile,\n",
    "                            join_key1: str,\n",
    "                            join_key2: str,\n",
    "                            temp_dir: str,\n",
    "                            columns_table1: Optional[List[str]],\n",
    "                            columns_table2: Optional[List[str]],\n",
    "                            in_batch_rows: int = 64_000,\n",
    "                            group_chunk_rows: int = 64_000,\n",
    "                            max_product_rows: int = 200_000) -> ColumnarDbFile:\n",
    "\n",
    "        def ensure_key(cols, key):\n",
    "            if cols is None:\n",
    "                return None\n",
    "            return cols if key in cols else cols + [key]\n",
    "\n",
    "        # Compute input file paths\n",
    "        left_path = os.path.join(left_sorted.base_file_name, f\"{left_sorted.table_name}-0.parquet\")\n",
    "        right_path = os.path.join(right_sorted.base_file_name, f\"{right_sorted.table_name}-0.parquet\")\n",
    "\n",
    "        # Load schemas to default to all columns if None\n",
    "        l_pf = pq.ParquetFile(left_path)\n",
    "        r_pf = pq.ParquetFile(right_path)\n",
    "        l_schema = l_pf.schema_arrow\n",
    "        r_schema = r_pf.schema_arrow\n",
    "\n",
    "        if columns_table1 is None:\n",
    "            columns_table1 = l_schema.names\n",
    "        if columns_table2 is None:\n",
    "            columns_table2 = r_schema.names\n",
    "        columns_table1 = ensure_key(columns_table1, join_key1)\n",
    "        columns_table2 = ensure_key(columns_table2, join_key2)\n",
    "\n",
    "        # Output: keep left key; drop duplicate key from right\n",
    "        left_out_cols = columns_table1\n",
    "        right_out_cols = [c for c in columns_table2 if c != join_key2]\n",
    "\n",
    "        # Prepare iterators\n",
    "        L = ParquetGroupChunkIter(left_path, columns_table1, join_key1,\n",
    "                                    in_batch_rows, group_chunk_rows)\n",
    "        R = ParquetGroupChunkIter(right_path, columns_table2, join_key2,\n",
    "                                    in_batch_rows, group_chunk_rows)\n",
    "\n",
    "        # Output writer\n",
    "        result_table = ColumnarDbFile(\"join_result\", file_dir=temp_dir)\n",
    "        out_path = os.path.join(result_table.base_file_name, f\"{result_table.table_name}-0.parquet\")\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        writer = None\n",
    "\n",
    "        def write_join_product(l_tbl: pa.Table, r_tbl: pa.Table):\n",
    "            nonlocal writer\n",
    "            if l_tbl is None or r_tbl is None:\n",
    "                return\n",
    "            l_df = l_tbl.select(left_out_cols).to_pandas()\n",
    "            r_df = r_tbl.select(right_out_cols).to_pandas()\n",
    "            m, n = len(l_df), len(r_df)\n",
    "            if m == 0 or n == 0:\n",
    "                return\n",
    "            block_n = max(1, min(n, max_product_rows // max(1, m)))\n",
    "            start = 0\n",
    "            while start < n:\n",
    "                end = min(n, start + block_n)\n",
    "                r_block = r_df.iloc[start:end]\n",
    "                out_df = l_df.assign(_cj=1).merge(r_block.assign(_cj=1), on=\"_cj\").drop(columns=[\"_cj\"])\n",
    "                out_tbl = pa.Table.from_pandas(out_df, preserve_index=False)\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(out_path, schema=out_tbl.schema)\n",
    "                writer.write_table(out_tbl)\n",
    "                start = end\n",
    "\n",
    "        # Drive the SMJ\n",
    "        l_tbl, l_key, l_last = L.next_chunk()\n",
    "        r_tbl, r_key, r_last = R.next_chunk()\n",
    "\n",
    "        while l_tbl is not None and r_tbl is not None:\n",
    "            if l_key is None:\n",
    "                l_tbl, l_key, l_last = L.next_chunk()\n",
    "                continue\n",
    "            if r_key is None:\n",
    "                r_tbl, r_key, r_last = R.next_chunk()\n",
    "                continue\n",
    "\n",
    "            if l_key < r_key:\n",
    "                l_tbl, l_key, l_last = L.next_chunk()\n",
    "            elif l_key > r_key:\n",
    "                r_tbl, r_key, r_last = R.next_chunk()\n",
    "                \n",
    "            else:\n",
    "                # l_key == r_key: join full groups. Cache right group (spill if large),\n",
    "                # then stream left group chunk-by-chunk against the cached/spilled right.\n",
    "                MAX_CACHED_GROUP_ROWS = 500_000  # tune as you like\n",
    "\n",
    "                r_chunks: List[pa.Table] = []\n",
    "                r_rows = 0\n",
    "                r_spill_path = None\n",
    "                r_writer = None\n",
    "\n",
    "                def _open_r_spill(schema: pa.Schema):\n",
    "                    nonlocal r_writer, r_spill_path\n",
    "                    if r_writer is None:\n",
    "                        r_spill_path = os.path.join(\n",
    "                            temp_dir, f\"_smj_rspill_{uuid.uuid4().hex}.parquet\"\n",
    "                        )\n",
    "                        r_writer = pq.ParquetWriter(r_spill_path, schema=schema)\n",
    "\n",
    "                def _cache_r_chunk(tbl: pa.Table):\n",
    "                    nonlocal r_rows, r_writer\n",
    "                    if tbl is None or tbl.num_rows == 0:\n",
    "                        return\n",
    "                    if r_writer is None and (r_rows + tbl.num_rows) <= MAX_CACHED_GROUP_ROWS:\n",
    "                        r_chunks.append(tbl)\n",
    "                    else:\n",
    "                        if r_writer is None:\n",
    "                            _open_r_spill(tbl.schema)\n",
    "                            for t in r_chunks:\n",
    "                                r_writer.write_table(t)\n",
    "                            r_chunks.clear()\n",
    "                        r_writer.write_table(tbl)\n",
    "                    r_rows += tbl.num_rows\n",
    "\n",
    "                # 1) Collect entire right group for this key\n",
    "                _cache_r_chunk(r_tbl)\n",
    "                while not r_last:\n",
    "                    r_tbl, r_key2, r_last = R.next_chunk()\n",
    "                    if r_tbl is None or r_key2 != l_key:\n",
    "                        break\n",
    "                    _cache_r_chunk(r_tbl)\n",
    "                if r_writer is not None:\n",
    "                    r_writer.close()\n",
    "\n",
    "                def right_iter():\n",
    "                    if r_spill_path is not None:\n",
    "                        pf = pq.ParquetFile(r_spill_path)\n",
    "                        for b in pf.iter_batches(batch_size=group_chunk_rows):\n",
    "                            yield pa.Table.from_batches([b])\n",
    "                    else:\n",
    "                        for t in r_chunks:\n",
    "                            yield t\n",
    "\n",
    "                # 2) Stream left group; for each left chunk, replay right group\n",
    "                while True:\n",
    "                    for r_part in right_iter():\n",
    "                        write_join_product(l_tbl, r_part)\n",
    "                    if l_last:\n",
    "                        break\n",
    "                    l_tbl, l_key2, l_last = L.next_chunk()\n",
    "                    if l_tbl is None or l_key2 != l_key:\n",
    "                        break\n",
    "\n",
    "                # 3) Advance right iterator to next key (we ended exactly at its boundary)\n",
    "                r_tbl, r_key, r_last = R.next_chunk()\n",
    "                # proceed to next loop iteration\n",
    "                continue\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "        return result_table\n",
    "\n",
    "    def _flush_run(\n",
    "        self,\n",
    "        dfs: List[pd.DataFrame],\n",
    "        join_key: str,\n",
    "        output_dir: str,\n",
    "        side: str,\n",
    "        run_idx: int,\n",
    "        ascending: bool = True,\n",
    "    ) -> str:\n",
    "\n",
    "        df_run = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Sort by join_key, then tiebreak if set, tiebreak key is always sorted asceding\n",
    "        if self.tiebreak_key and self.tiebreak_key in df_run.columns:\n",
    "            df_run_sorted = df_run.sort_values(by=[join_key, self.tiebreak_key], ascending=[ascending, True]) \n",
    "        else:\n",
    "            df_run_sorted = df_run.sort_values(by=join_key, ascending=ascending)\n",
    "\n",
    "        run_file = os.path.join(output_dir, f\"{side}_run_{run_idx}.parquet\")\n",
    "        df_run_sorted.to_parquet(run_file)\n",
    "\n",
    "        dfs.clear()\n",
    "        del df_run, df_run_sorted\n",
    "        gc.collect()\n",
    "\n",
    "        return run_file\n",
    "\n",
    "    @timer\n",
    "    def _external_sort(\n",
    "        self,\n",
    "        table: ColumnarDbFile,\n",
    "        join_key: str,\n",
    "        output_dir: str,\n",
    "        side: str,\n",
    "        columns: Optional[List[str]] = None,\n",
    "        ascending: bool = True,\n",
    "        tiebreak_key = None\n",
    "    ) -> ColumnarDbFile:\n",
    "        \"\"\"\n",
    "        Perform an external sort on a table based on the join key and return a sorted ColumnarDbFile.\n",
    "        Use _bway_merge to merge sorted files\n",
    "        \"\"\"\n",
    "        self.tiebreak_key = tiebreak_key\n",
    "\n",
    "        # Get table size (on disk)\n",
    "        disk_usage = table.table_disk_usage()\n",
    "        total_bytes = disk_usage[\"total_bytes\"]\n",
    "\n",
    "        # Check if we can safely process in 12 GB RAM\n",
    "        if table.can_process_parquet(total_bytes):\n",
    "\n",
    "            # read data in and sort all in RAM\n",
    "            df = table.retrieve_data(columns=columns)\n",
    "\n",
    "            if self.tiebreak_key and self.tiebreak_key in df.columns:\n",
    "                df_sorted = df.sort_values(by=[join_key, self.tiebreak_key], ascending=[ascending, True]).reset_index(drop=True)\n",
    "            else:\n",
    "                df_sorted = df.sort_values(by=join_key, ascending=ascending).reset_index(drop=True)\n",
    "\n",
    "            # create paraquet in output dir for the table\n",
    "            sorted_name = f\"{side}_{table.table_name}_sorted\"\n",
    "            sorted_table = ColumnarDbFile(sorted_name, file_dir=output_dir)\n",
    "            sorted_table.build_table(df_sorted)\n",
    "\n",
    "            # clean unnecessary overhead and return table\n",
    "            del df, df_sorted\n",
    "            gc.collect()\n",
    "            return sorted_table\n",
    "\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"sorting table \", table.table_name, \"with \", total_bytes, \"bytes using external sort\")\n",
    "                print(\"GBs : \", total_bytes / (1024 * 1024 * 1024))\n",
    "            # Get list of parquet files in the table directory\n",
    "            parquet_files = glob.glob(f\"{table.base_file_name}/*.parquet\")\n",
    "\n",
    "            runs_path: List[str] = []\n",
    "            run_idx = 0\n",
    "            current_dfs: List[pd.DataFrame] = []\n",
    "            current_row_groups = 0\n",
    "\n",
    "            # loop through all the parquet files\n",
    "            if self.verbose:\n",
    "                print(f\"Sorting {len(parquet_files)} files\")\n",
    "            for file in parquet_files:\n",
    "                pf = pq.ParquetFile(file)\n",
    "\n",
    "                # safe bounded unit of work for sorting\n",
    "                num_row_groups = pf.metadata.num_row_groups\n",
    "\n",
    "                for rg in range(num_row_groups):\n",
    "                    if self.verbose:\n",
    "                        print(\"reading row group \", rg)\n",
    "\n",
    "                    # read a row group as a chunk\n",
    "                    batch = pf.read_row_group(rg, columns=columns)\n",
    "                    df_chunk = batch.to_pandas()\n",
    "                    current_dfs.append(df_chunk)\n",
    "                    current_row_groups += 1\n",
    "\n",
    "                    # treating a row group as a page\n",
    "                    # change to 2 to practice spliting runs\n",
    "                    if current_row_groups > self.num_pages_per_split:\n",
    "                        \n",
    "                        if self.verbose:\n",
    "                            print(\"flushing run \", run_idx)\n",
    "                        # sort current run in ram\n",
    "                        # save as a parquet file in the current run directory\n",
    "                        run_file = self._flush_run(\n",
    "                        current_dfs, join_key, output_dir, side, run_idx, ascending=ascending\n",
    "                        )\n",
    "\n",
    "                        # runs path is a list of the sorted parquet files\n",
    "                        runs_path.append(run_file)\n",
    "                        run_idx += 1\n",
    "                        current_row_groups = 0\n",
    "                        if self.verbose:\n",
    "                            print(f\"Flushed run {run_idx} at {run_file}\")\n",
    "\n",
    "            # flush remaining partial run, get sorted run_file as a df\n",
    "            if current_dfs:\n",
    "              run_file = self._flush_run(\n",
    "                  current_dfs, join_key, output_dir, side, run_idx, ascending=ascending\n",
    "              )\n",
    "              runs_path.append(run_file)\n",
    "            # output_dir has a list of sorted parquet files for the current table\n",
    "\n",
    "            # Create the wrapper first so we write where it will read\n",
    "            sorted_table = ColumnarDbFile(\n",
    "                table_name=f\"{side}_{table.table_name}_sorted\",\n",
    "                file_dir=output_dir,\n",
    "            )\n",
    "\n",
    "            # Write the final merged file inside that directory, matching ColumnarDbFile\n",
    "            final_sorted_path = os.path.join(\n",
    "                sorted_table.base_file_name, f\"{sorted_table.table_name}-0.parquet\"\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(\"merging all runs into \", final_sorted_path)\n",
    "            self._merge_all_runs(runs_path, final_sorted_path, join_key, ascending=ascending)\n",
    "\n",
    "            return sorted_table\n",
    "\n",
    "    @timer\n",
    "    def _merge_all_runs(self, sorted_files: List[str], output_file: str, join_key: str, ascending: bool = True) :\n",
    "        \"\"\"\n",
    "        Merge multiple sorted Parquet files into a single sorted Parquet file.\n",
    "        \"\"\"\n",
    "        B = self.bway_merge_factor\n",
    "\n",
    "        # copy that we will mutate\n",
    "        runs = list(sorted_files)\n",
    "        pass_idx = 0\n",
    "\n",
    "        # while list of runs left to merge\n",
    "        while len(runs) > 1:\n",
    "          if self.verbose:\n",
    "            print(\"merging pass \", pass_idx)\n",
    "          next_runs = []\n",
    "\n",
    "          # B - 1 input buffers +1 output buffer\n",
    "          for i in range(0, len(runs), B - 1):\n",
    "                batch = runs[i : i + (B - 1)]   # B-1 input buffers, still a list\n",
    "\n",
    "                # choose an output path for this merged batch\n",
    "                # on the final pass, we want the result at `output_file`\n",
    "                if len(runs) <= B - 1 and len(next_runs) == 0:\n",
    "                    # last pass, first (and only) merged run -> final output\n",
    "                    merged_path = output_file\n",
    "                else:\n",
    "                    # intermediate pass: write to a temp run file\n",
    "                    base_dir = os.path.dirname(output_file)\n",
    "                    merged_path = os.path.join(\n",
    "                        base_dir,\n",
    "                        f\"bway_pass{pass_idx}_run{len(next_runs)}.parquet\",\n",
    "                    )\n",
    "\n",
    "                # b-way merge this batch into merged_path\n",
    "                self._bway_merge(batch, merged_path, join_key, ascending=ascending)\n",
    "\n",
    "                next_runs.append(merged_path)\n",
    "\n",
    "          runs = next_runs\n",
    "          pass_idx += 1\n",
    "\n",
    "        # At this point, runs has exactly one file.\n",
    "        final_run = runs[0]\n",
    "        if final_run != output_file:\n",
    "            # In case we didn't land exactly on output_file path\n",
    "            if os.path.exists(output_file):\n",
    "                os.remove(output_file)\n",
    "            shutil.move(final_run, output_file)\n",
    "\n",
    "        return output_file\n",
    "\n",
    "\n",
    "    def _bway_merge(self, sorted_files: List[str], output_file: str, join_key: str, ascending: bool = True):\n",
    "        \"\"\"\n",
    "        Streaming B-way merge of already-sorted Parquet 'runs' into a single\n",
    "        sorted Parquet file at `output_file`, ordered by `join_key`.\n",
    "        Memory use is bounded: keeps one small batch per input + an output buffer.\n",
    "        \"\"\"\n",
    "        if not sorted_files:\n",
    "            raise ValueError(\"No input files to merge.\")\n",
    "\n",
    "        # Tunables: keep these moderate to bound memory\n",
    "        IN_BATCH_ROWS = 64_000         # rows read per input batch\n",
    "        OUT_ROW_GROUP_ROWS = 128_000   # rows written per output row group\n",
    "\n",
    "        # Determine common schema and key index from first file\n",
    "        first_pf = pq.ParquetFile(sorted_files[0])\n",
    "        schema = first_pf.schema_arrow\n",
    "        key_index = schema.get_field_index(join_key)\n",
    "        if key_index == -1:\n",
    "            raise ValueError(f\"join_key '{join_key}' not found in schema.\")\n",
    "\n",
    "        # Initialize streaming readers (preserve on-disk order)\n",
    "        readers = [pq.ParquetFile(p).iter_batches(batch_size=IN_BATCH_ROWS) for p in sorted_files]\n",
    "\n",
    "        # Per-input state\n",
    "        states = []  # list of dicts: { 'iter': iterator, 'batch': RecordBatch|None, 'ridx': int }\n",
    "        heap = []    # min-heap of ((is_null, key_value), src_idx)\n",
    "\n",
    "        # Get tiebreak index if set\n",
    "        tiebreak_index = None\n",
    "        if self.tiebreak_key:\n",
    "            tiebreak_index = schema.get_field_index(self.tiebreak_key)\n",
    "\n",
    "        def _prime_state(it):\n",
    "            try:\n",
    "                return next(it)\n",
    "            except StopIteration:\n",
    "                return None\n",
    "\n",
    "        def _push_heap_for_src(src_idx):\n",
    "            st = states[src_idx]\n",
    "            batch = st[\"batch\"]\n",
    "            ridx = st[\"ridx\"]\n",
    "            if batch is None or ridx >= batch.num_rows:\n",
    "                return\n",
    "            key_arr = batch.column(key_index)\n",
    "            key_val = key_arr[ridx].as_py()\n",
    "            \n",
    "            # Get tiebreaker value if set\n",
    "            tie_val = None\n",
    "            if tiebreak_index is not None:\n",
    "                tie_arr = batch.column(tiebreak_index)\n",
    "                tie_val = tie_arr[ridx].as_py()\n",
    "            \n",
    "            # Compute heap key based on sort direction\n",
    "            if self.ascending:\n",
    "                heap_key = (key_val is None, key_val, tie_val)\n",
    "            else:\n",
    "                # DESC on primary key, ASC on tiebreaker\n",
    "                heap_key = (key_val is not None, \n",
    "                        -key_val if isinstance(key_val, (int, float)) else key_val,\n",
    "                        tie_val)\n",
    "            \n",
    "            heapq.heappush(heap, (heap_key, src_idx))\n",
    "\n",
    "        # Prime batches and heap\n",
    "        for it in readers:\n",
    "            batch = _prime_state(it)\n",
    "            states.append({\"iter\": it, \"batch\": batch, \"ridx\": 0})\n",
    "        for i in range(len(states)):\n",
    "            if states[i][\"batch\"] is not None and states[i][\"batch\"].num_rows > 0:\n",
    "                _push_heap_for_src(i)\n",
    "\n",
    "        # Open output writer\n",
    "        writer = pq.ParquetWriter(output_file, schema=schema)\n",
    "        out_batches: List[pa.RecordBatch] = []\n",
    "        out_rows = 0\n",
    "\n",
    "        # Core streaming merge loop\n",
    "        while heap:\n",
    "            _, src_idx = heapq.heappop(heap)\n",
    "            st = states[src_idx]\n",
    "            batch = st[\"batch\"]\n",
    "            ridx = st[\"ridx\"]\n",
    "\n",
    "            # Append a single-row slice to output buffer\n",
    "            out_batches.append(batch.slice(ridx, 1))\n",
    "            out_rows += 1\n",
    "\n",
    "            # Advance this source\n",
    "            st[\"ridx\"] += 1\n",
    "            if st[\"ridx\"] >= batch.num_rows:\n",
    "                st[\"batch\"] = _prime_state(st[\"iter\"])\n",
    "                st[\"ridx\"] = 0\n",
    "            if st[\"batch\"] is not None:\n",
    "                _push_heap_for_src(src_idx)\n",
    "\n",
    "            # Flush output buffer as a row group when big enough\n",
    "            if out_rows >= OUT_ROW_GROUP_ROWS:\n",
    "                writer.write_table(pa.Table.from_batches(out_batches))\n",
    "                out_batches.clear()\n",
    "                out_rows = 0\n",
    "\n",
    "        # Final flush\n",
    "        if out_rows > 0:\n",
    "            writer.write_table(pa.Table.from_batches(out_batches))\n",
    "        writer.close()\n",
    "\n",
    "    @timer\n",
    "    def join(\n",
    "        self,\n",
    "        table1: ColumnarDbFile,\n",
    "        table2: ColumnarDbFile,\n",
    "        join_key1: str,\n",
    "        join_key2: str,\n",
    "        temp_dir: str = \"temp\",\n",
    "        columns_table1: Optional[List[str]] = None,\n",
    "        columns_table2: Optional[List[str]] = None,\n",
    "        ASC: bool = True,\n",
    "    ) -> Optional[ColumnarDbFile]:\n",
    "        \"\"\"\n",
    "        Perform a sort-merge join between two ColumnarDbFile instances and return a sorted ColumnarDbFile.\n",
    "        \"\"\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        self.ascending = ASC\n",
    "\n",
    "        # Sort both tables externally\n",
    "        sorted_table1 = self._external_sort(\n",
    "            table1, join_key1, temp_dir, \"left\", columns_table1, ascending=ASC\n",
    "        )\n",
    "        sorted_table2 = self._external_sort(\n",
    "            table2, join_key2, temp_dir, \"right\", columns_table2, ascending=ASC\n",
    "        )\n",
    "\n",
    "        result_table = self._streaming_inner_join(\n",
    "            left_sorted=sorted_table1,\n",
    "            right_sorted=sorted_table2,\n",
    "            join_key1=join_key1,\n",
    "            join_key2=join_key2,\n",
    "            temp_dir=temp_dir,\n",
    "            columns_table1=columns_table1,\n",
    "            columns_table2=columns_table2,\n",
    "        )\n",
    "\n",
    "        return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "4yhxUqHS5ShN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method '_external_sort' took 0.0553 seconds.\n",
      "Method '_external_sort' took 1.3795 seconds.\n",
      "Method '_streaming_inner_join' took 91.9589 seconds.\n",
      "Method 'join' took 93.4050 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ColumnarDbFile at 0x362f2d8e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1144.31 MiB, increment: 1004.33 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "SIZE = \"1GB\"\n",
    "songs_table = tables[f'Songs_{SIZE}']\n",
    "users_table = tables[f'Users_{SIZE}']\n",
    "listens_table = tables[f'Listens_{SIZE}']\n",
    "\n",
    "smj = SortMergeJoin(num_pages_per_split=1000, verbose=False)\n",
    "\n",
    "# Example: join Songs with Listens on song_id\n",
    "sorted_join_result = smj.join(\n",
    "    songs_table,\n",
    "    listens_table,\n",
    "    join_key1=\"song_id\",\n",
    "    join_key2=\"song_id\",\n",
    "    temp_dir=\"temp_songs_listens\",\n",
    "    columns_table1= [\"song_id\", \"title\"],\n",
    "    columns_table2= [\"song_id\", \"user_id\"]\n",
    ")\n",
    "\n",
    "display(sorted_join_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method '_external_sort' took 0.0625 seconds.\n",
      "Method '_external_sort' took 0.0451 seconds.\n",
      "Method '_streaming_inner_join' took 8.8478 seconds.\n",
      "Method 'join' took 8.9557 seconds.\n",
      "Row counts - SMJ vs Pandas: 200000 200000\n",
      "Column sets equal: True\n",
      "Shapes equal: True\n",
      "Frames equal: True\n"
     ]
    }
   ],
   "source": [
    "# Correctness test vs pandas on a manageable subset\n",
    "SIZE = \"100MB\"\n",
    "\n",
    "# Choose columns and sample sizes\n",
    "JOIN_KEY_LEFT = \"song_id\"\n",
    "JOIN_KEY_RIGHT = \"song_id\"\n",
    "LEFT_COLS = [\"song_id\", \"title\"]\n",
    "RIGHT_COLS = [\"song_id\", \"user_id\"]\n",
    "\n",
    "N_LEFT = 200_000\n",
    "N_RIGHT = 200_000\n",
    "\n",
    "# 1) Create temporary, smaller ColumnarDbFiles from the big tables\n",
    "TEST_DIR = \"temp_smj_correctness\"\n",
    "if os.path.exists(TEST_DIR):\n",
    "    shutil.rmtree(TEST_DIR)\n",
    "os.makedirs(TEST_DIR, exist_ok=True)\n",
    "\n",
    "left_df_full = tables[f\"Songs_{SIZE}\"].retrieve_data(columns=LEFT_COLS)\n",
    "right_df_full = tables[f\"Listens_{SIZE}\"].retrieve_data(columns=RIGHT_COLS)\n",
    "\n",
    "left_df = left_df_full.head(N_LEFT).reset_index(drop=True)\n",
    "right_df = right_df_full.head(N_RIGHT).reset_index(drop=True)\n",
    "\n",
    "left_tmp = ColumnarDbFile(\"LeftTest\", file_dir=TEST_DIR)\n",
    "right_tmp = ColumnarDbFile(\"RightTest\", file_dir=TEST_DIR)\n",
    "left_tmp.build_table(left_df)\n",
    "right_tmp.build_table(right_df)\n",
    "\n",
    "# 2) Run your streaming SMJ on the temp tables\n",
    "smj = SortMergeJoin()\n",
    "smj_out = smj.join(\n",
    "    left_tmp,\n",
    "    right_tmp,\n",
    "    join_key1=JOIN_KEY_LEFT,\n",
    "    join_key2=JOIN_KEY_RIGHT,\n",
    "    temp_dir=os.path.join(TEST_DIR, \"join_out\"),\n",
    "    columns_table1=LEFT_COLS,\n",
    "    columns_table2=RIGHT_COLS,\n",
    ")\n",
    "\n",
    "# 3) Materialize results\n",
    "# SMJ output keeps only the left join key (your implementation drops the right duplicate)\n",
    "smj_df = smj_out.retrieve_data()\n",
    "\n",
    "# Pandas baseline: merge then drop the duplicate right key to match SMJ schema\n",
    "baseline_df = pd.merge(\n",
    "    left_df,\n",
    "    right_df,\n",
    "    left_on=JOIN_KEY_LEFT,\n",
    "    right_on=JOIN_KEY_RIGHT,\n",
    "    how=\"inner\",\n",
    ")\n",
    "baseline_df = baseline_df[LEFT_COLS + [c for c in RIGHT_COLS if c != JOIN_KEY_RIGHT]]\n",
    "\n",
    "# 4) Normalize order and compare\n",
    "order_cols = [JOIN_KEY_LEFT] + [c for c in LEFT_COLS if c != JOIN_KEY_LEFT] + [c for c in RIGHT_COLS if c != JOIN_KEY_RIGHT]\n",
    "smj_df = smj_df.sort_values(by=order_cols).reset_index(drop=True)\n",
    "baseline_df = baseline_df.sort_values(by=order_cols).reset_index(drop=True)\n",
    "\n",
    "# Ensure identical dtypes for fair compare when feasible\n",
    "for c in order_cols:\n",
    "    if c in smj_df.columns and c in baseline_df.columns:\n",
    "        try:\n",
    "            baseline_df[c] = baseline_df[c].astype(smj_df[c].dtype)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# 5) Assertions and quick diagnostics\n",
    "same_shape = smj_df.shape == baseline_df.shape\n",
    "same_rows = smj_df.equals(baseline_df)\n",
    "\n",
    "print(\"Row counts - SMJ vs Pandas:\", smj_df.shape[0], baseline_df.shape[0])\n",
    "print(\"Column sets equal:\", set(smj_df.columns) == set(baseline_df.columns))\n",
    "print(\"Shapes equal:\", same_shape)\n",
    "print(\"Frames equal:\", same_rows)\n",
    "\n",
    "if not same_rows:\n",
    "    # Show a small diff preview\n",
    "    merged_chk = smj_df.merge(\n",
    "        baseline_df, how=\"outer\", indicator=True, on=order_cols\n",
    "    )\n",
    "    print(\"Mismatched samples:\")\n",
    "    display(merged_chk[merged_chk[\"_merge\"] != \"both\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af8XhdLWhWkN"
   },
   "source": [
    "Implement GROUP BY after joins:\n",
    "- Here you could use `pd.groupby` or do manual aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "4iwQ65LBhWkN"
   },
   "outputs": [],
   "source": [
    "# GROUP BY s.song_id, s.title\n",
    "class HashGroupbyAverageAndDistinct():\n",
    "    def __init__(self, num_partitions, parquet_batch_size, use_streaming=False):\n",
    "        self.num_partitions = num_partitions\n",
    "        self.parquet_batch_size = parquet_batch_size\n",
    "        self.use_streaming = use_streaming\n",
    "    \n",
    "    def _make_partition_path(self, temp_dir, part_id):\n",
    "        return os.path.join(temp_dir, f\"group_part{part_id}.parquet\")\n",
    "    \n",
    "    @timer\n",
    "    def groupby_average_distinct(self,\n",
    "                        table: ColumnarDbFile, \n",
    "                        groupby_cols: List[str],\n",
    "                        average_col: str, \n",
    "                        average_col_name: str, \n",
    "                        distinct_col: str,\n",
    "                        distinct_col_name: str,\n",
    "                        select_cols: List[str], \n",
    "                        temp_dir='groupby_temp') -> ColumnarDbFile:\n",
    "        \"\"\"\n",
    "        Perform:\n",
    "            SELECT select_cols..., AVG(average_col) AS average_col_name, COUNT(DISTINCT distinct_col)\n",
    "            FROM table\n",
    "            GROUP BY groupby_cols...\n",
    "        \n",
    "        Hash partitioning on (concatenation of) groupby_cols, and then in-memory aggregation per partition\n",
    "\n",
    "        Assumptions:\n",
    "        - groupby_col is non-empty\n",
    "        - select_cols is a subset of groupby_col\n",
    "        - Per-partition hash table fits in memory\n",
    "        \n",
    "        Uses self.use_streaming to determine whether to use ParquetWriter streaming\n",
    "        for efficient batch writes.\n",
    "        \"\"\"\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        \n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        # hash on groupby columns (safe: same group always shares this)\n",
    "\n",
    "        hash_partition(\n",
    "            table=table,\n",
    "            hash_keys=groupby_cols,  # hash on groupby columns (safe: same group always shares this)\n",
    "            num_partitions=self.num_partitions,\n",
    "            parquet_batch_size=self.parquet_batch_size,\n",
    "            hash_value_fn=HASHVALUE,\n",
    "            make_partition_path_fn=partial(self._make_partition_path, temp_dir),\n",
    "            columns= list(set(groupby_cols + [average_col, distinct_col])),\n",
    "        )\n",
    "\n",
    "        output_db = ColumnarDbFile(f\"{table.table_name}_groupby_avg\")\n",
    "        \n",
    "        # Clean up any existing files in the output directory\n",
    "        if os.path.exists(output_db.base_file_name):\n",
    "            for file_path in output_db.get_all_parquet_paths():\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        # Start streaming if enabled\n",
    "        if self.use_streaming:\n",
    "            output_db.start_stream()\n",
    "        for part_id in range(self.num_partitions):\n",
    "            part_path = self._make_partition_path(temp_dir, part_id)\n",
    "            if not os.path.exists(part_path):\n",
    "                continue\n",
    "\n",
    "            # In-memory hash table for this partition:\n",
    "            # key: tuple of groupby_col values\n",
    "            # value: Tuple of (sum of average_col, count of average_col, set of distinct distinct_col values)\n",
    "            SUM_IDX = 0\n",
    "            COUNT_IDX = 1\n",
    "            DISTINCT_SET_IDX = 2\n",
    "            agg_map: Dict[Any, Tuple[float, int, set]] = {}\n",
    "\n",
    "            pf = pq.ParquetFile(part_path)\n",
    "            for batch in pf.iter_batches(batch_size=self.parquet_batch_size):\n",
    "                df = batch.to_pandas()\n",
    "\n",
    "                grouped = (\n",
    "                    df.groupby(groupby_cols)\n",
    "                    .agg(\n",
    "                        sum_avg=(average_col, \"sum\"),\n",
    "                        cnt_avg=(average_col, \"count\"),  # SQL AVG ignores NULLs\n",
    "                        distinct_set=(distinct_col, lambda s: set(s.dropna()))  # SQL ignores NULL in COUNT DISTINCT\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for key_tuple, row in grouped.iterrows():\n",
    "                    if not isinstance(key_tuple, tuple):\n",
    "                        key_tuple = (key_tuple,)\n",
    "\n",
    "                    state = agg_map.setdefault(key_tuple, [0.0, 0, set()])\n",
    "                    state[SUM_IDX] += row[\"sum_avg\"]\n",
    "                    state[COUNT_IDX] += row[\"cnt_avg\"]\n",
    "                    state[DISTINCT_SET_IDX] |= row[\"distinct_set\"]\n",
    "\n",
    "\n",
    "            # Turn the per-partition hash table into a DataFrame and append\n",
    "            if agg_map:\n",
    "                # Pre-compute column index mapping to avoid repeated index() calls\n",
    "                col_idx_map = {col: groupby_cols.index(col) for col in select_cols}\n",
    "                \n",
    "                # Build output efficiently using list comprehensions\n",
    "                out_rows = []\n",
    "                for key_tuple, state in agg_map.items():\n",
    "                    # Ensure key_tuple is a tuple (handles single vs multi-column)\n",
    "                    if not isinstance(key_tuple, tuple):\n",
    "                        key_tuple = (key_tuple,)\n",
    "                    \n",
    "                    row_dict = {col: key_tuple[col_idx_map[col]] for col in select_cols}\n",
    "                    row_dict[average_col_name] = state[SUM_IDX] / state[COUNT_IDX]\n",
    "                    row_dict[distinct_col_name] = len(state[DISTINCT_SET_IDX])\n",
    "                    out_rows.append(row_dict)\n",
    "\n",
    "                if out_rows:\n",
    "                    out_df = pd.DataFrame(out_rows)\n",
    "                    output_db.append_data(out_df)\n",
    "\n",
    "        # Stop streaming if it was enabled\n",
    "        if self.use_streaming:\n",
    "            output_db.stop_stream()\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "        return output_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 'groupby_average_distinct' took 0.6927 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>avg_user_id</th>\n",
       "      <th>distinct_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>23262.696078</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>25417.038835</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>22609.904762</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>24474.144330</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>22391.166667</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9983</td>\n",
       "      <td>23652.714286</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9987</td>\n",
       "      <td>25471.716981</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9991</td>\n",
       "      <td>24112.888889</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9995</td>\n",
       "      <td>26495.407407</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>26898.800000</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      song_id   avg_user_id  distinct_user_id\n",
       "0           0  23262.696078               101\n",
       "1           4  25417.038835               103\n",
       "2           8  22609.904762               105\n",
       "3          12  24474.144330                97\n",
       "4          16  22391.166667                83\n",
       "...       ...           ...               ...\n",
       "9995     9983  23652.714286                98\n",
       "9996     9987  25471.716981               106\n",
       "9997     9991  24112.888889                99\n",
       "9998     9995  26495.407407               108\n",
       "9999     9999  26898.800000                90\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1119.39 MiB, increment: 155.36 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "# test implementation\n",
    "\n",
    "SIZE = \"100MB\" #[\"100MB\", \"1GB\", \"10GB\"]\n",
    "SAMPLE = 100\n",
    "USE_STREAMING = False\n",
    "\n",
    "listens_table = tables[f'Listens_{SIZE}']\n",
    "\n",
    "groupby_average_distinct = HashGroupbyAverageAndDistinct(\n",
    "    num_partitions=4,\n",
    "    parquet_batch_size=10000000,\n",
    "    use_streaming=USE_STREAMING\n",
    ")\n",
    "\n",
    "results = groupby_average_distinct.groupby_average_distinct(\n",
    "    table=listens_table,\n",
    "    groupby_cols=['song_id'],\n",
    "    average_col='user_id',\n",
    "    average_col_name='avg_user_id',\n",
    "    distinct_col='user_id',\n",
    "    distinct_col_name='distinct_user_id',\n",
    "    select_cols=['song_id']\n",
    ")\n",
    "\n",
    "display(results.retrieve_data(sample=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feYU7Tdlke9Z"
   },
   "source": [
    "# Section 4: Query Planning & Optimization\n",
    "\n",
    "In this section, you'll implement smart query planning using metadata analysis. The key idea is to **avoid loading data unnecessarily** by:\n",
    "1. Analyzing Parquet metadata first (row counts, column names, file sizes)\n",
    "2. Making intelligent decisions about join order and algorithm selection\n",
    "3. Loading only the columns you actually need for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "aQt-sR-zhWkN"
   },
   "outputs": [],
   "source": [
    "def analyze_metadata_before_loading(file_paths):\n",
    "    \n",
    "    metadata = {}\n",
    "    for table_name, coldb in file_paths.items():\n",
    "        # Use provided helpers for counts/sizes\n",
    "        tm = coldb.table_metadata() # num_files, total_rows, total_compressed_bytes\n",
    "        du = coldb.table_disk_usage() # num_files, total_bytes (on disk)\n",
    "\n",
    "        # Get schema from the first parquet file\n",
    "        base_dir = coldb.base_file_name\n",
    "        parquet_files = sorted(glob.glob(f\"{base_dir}/*.parquet\"))\n",
    "        columns = {}\n",
    "        if parquet_files:\n",
    "            pf = pq.ParquetFile(parquet_files[0])\n",
    "            # Use Arrow schema to read field names and types\n",
    "            columns = {field.name: str(field.type) for field in pf.schema_arrow}\n",
    "\n",
    "        metadata[table_name] = {\n",
    "            \"num_files\": tm[\"num_files\"],\n",
    "            \"rows\": int(tm[\"total_rows\"]),\n",
    "            \"columns\": columns,\n",
    "            \"bytes_on_disk\": int(du[\"total_bytes\"]),\n",
    "            \"total_compressed_bytes\": int(tm[\"total_compressed_bytes\"]),\n",
    "            \"can_process_in_12GB\": ColumnarDbFile.can_process_parquet(int(du[\"total_bytes\"]))\n",
    "        }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def plan_query_execution(metadata, parsed_query, memory_bytes=12 * 1024**3, overhead_per_row=24):\n",
    "    \"\"\"\n",
    "    Use parsed SQL + table metadata to:\n",
    "      - pick columns to read (column pruning)\n",
    "      - choose join order (follow SQL joins; build on smaller side)\n",
    "      - choose algorithm per step (HPJ if build fits in memory, else SMJ)\n",
    "    Returns a plan dict.\n",
    "    \"\"\"\n",
    "    # 0) Helpers\n",
    "    def meta_key_for(table_name_lower: str) -> str:\n",
    "        #  metadata keys are capitalized: 'Songs','Users','Listens'\n",
    "        return table_name_lower.capitalize()\n",
    "\n",
    "    def _size_of_type(t: str) -> int:\n",
    "        t = (t or \"\").lower()\n",
    "        if \"int64\" in t or \"float64\" in t or \"double\" in t or \"timestamp\" in t: return 8\n",
    "        if \"int32\" in t or \"float32\" in t: return 4\n",
    "        if \"bool\" in t: return 1\n",
    "        return 16  # fallback for strings\n",
    "\n",
    "    def _estimate_build_bytes(rows: int, key_type: str, payload_bytes=0, overhead=48, load=1.3):\n",
    "        per_row = _size_of_type(key_type) + payload_bytes + overhead\n",
    "        return int(rows * per_row * load)\n",
    "\n",
    "    def _size_of_type(t: str) -> int:\n",
    "        t = (t or \"\").lower()\n",
    "        if \"int64\" in t or \"float64\" in t or \"double\" in t or \"timestamp\" in t: return 8\n",
    "        if \"int32\" in t or \"float32\" in t: return 4\n",
    "        if \"bool\" in t: return 1\n",
    "        return 16  # fallback for strings\n",
    "\n",
    "    def _estimate_build_bytes(rows: int, key_type: str, payload_bytes=0, overhead=48, load=1.3):\n",
    "        per_row = _size_of_type(key_type) + payload_bytes + overhead\n",
    "        return int(rows * per_row * load)\n",
    "\n",
    "    def _io_costs_hpjsmj(bytes_L, bytes_R, mem_bytes, page_size=64*1024*1024, Cr=1, Cw=1):\n",
    "        # Pages\n",
    "        P_L = math.ceil(bytes_L / page_size)\n",
    "        P_R = math.ceil(bytes_R / page_size)\n",
    "        # Buffers (RAM pages)\n",
    "        B = max(2, mem_bytes // page_size)\n",
    "\n",
    "        # HPJ cost (special case Cr=Cw=1): 3*(P(L)+P(R))\n",
    "        hpj_ios = 3 * (P_L + P_R)\n",
    "\n",
    "        # External BigSort cost per table (simplified):\n",
    "        # 2*N * (1 + ceil(log_{B-1}(N))) with B buffers\n",
    "        def bigsort_cost(P):\n",
    "            base = max(2, B - 1)\n",
    "            passes = max(1, math.ceil(math.log(max(1, P), base)))\n",
    "            return 2 * P * (1 + passes)\n",
    "\n",
    "        # SMJ = BigSort(L)+BigSort(R)+Merge(L,R)\n",
    "        smj_ios = bigsort_cost(P_L) + bigsort_cost(P_R) + (P_L + P_R)  # merge read\n",
    "\n",
    "        return hpj_ios, smj_ios\n",
    "\n",
    "    def decide_join_algo(meta, left_tbl, right_tbl, left_key, right_key,\n",
    "                    rows_left=None, rows_right=None,\n",
    "                    mem_budget_bytes=12*1024**3, overhead=48,\n",
    "                    left_already_sorted=False, right_already_sorted=False,\n",
    "                    need_sorted_output=False):\n",
    "        \"\"\"\n",
    "        Choose join algorithm based on:\n",
    "        1. If smaller table fits in memory (5× expansion) --> HPJ\n",
    "        2. Otherwise --> SMJ\n",
    "        \"\"\"\n",
    "        # Sizes and rows\n",
    "        L = left_tbl\n",
    "        R = right_tbl\n",
    "        rows_L = rows_left  if rows_left is not None else int(meta[L][\"rows\"])\n",
    "        rows_R = rows_right if rows_right is not None else int(meta[R][\"rows\"])\n",
    "        tL = meta[L][\"columns\"].get(left_key, \"int64\")\n",
    "        tR = meta[R][\"columns\"].get(right_key, \"int64\")\n",
    "        \n",
    "        bytes_L = int(meta[L][\"bytes_on_disk\"])\n",
    "        bytes_R = int(meta[R][\"bytes_on_disk\"])\n",
    "        \n",
    "        # Determine smaller table\n",
    "        smaller_size = min(bytes_L, bytes_R)\n",
    "        smaller_table = L if bytes_L <= bytes_R else R\n",
    "        larger_table = R if smaller_table == L else L\n",
    "        \n",
    "        # Check if smaller table fits in memory for HPJ (5× expansion factor for pandas)\n",
    "        estimated_ram_needed = smaller_size * 5\n",
    "        fits_in_memory = estimated_ram_needed < mem_budget_bytes\n",
    "    \n",
    "        # If smaller table fits → HPJ is faster\n",
    "        if fits_in_memory:\n",
    "            algo = \"HPJ\"\n",
    "            build_tbl = smaller_table\n",
    "            build_side = \"L\" if smaller_table == L else \"R\"\n",
    "            \n",
    "            # Compute I/O costs for reporting\n",
    "            build_L = _estimate_build_bytes(rows_L, tL, overhead=overhead)\n",
    "            build_R = _estimate_build_bytes(rows_R, tR, overhead=overhead)\n",
    "            \n",
    "            P_L = math.ceil(bytes_L / (64*1024*1024))\n",
    "            P_R = math.ceil(bytes_R / (64*1024*1024))\n",
    "            hpj_ios, smj_ios = _io_costs_hpjsmj(bytes_L, bytes_R, mem_budget_bytes)\n",
    "            smj_ios = None  # not computed since we chose HPJ\n",
    "            \n",
    "            return {\n",
    "                \"algorithm\": algo,\n",
    "                \"build_side\": build_side,\n",
    "                \"build_table\": build_tbl,\n",
    "                \"hpj_ios\": hpj_ios,\n",
    "                \"smj_ios\": smj_ios,\n",
    "                \"hpj_build_bytes\": {\"L\": build_L, \"R\": build_R},\n",
    "                \"estimated_ram_needed\": estimated_ram_needed,\n",
    "                \"fits_in_memory\": True,\n",
    "            }\n",
    "        \n",
    "        # Else: SMJ for limited memory\n",
    "        \n",
    "        else:\n",
    "            algo = \"SMJ\"\n",
    "            \n",
    "            # Compute I/O costs\n",
    "            P_L = math.ceil(bytes_L / (64*1024*1024))\n",
    "            P_R = math.ceil(bytes_R / (64*1024*1024))\n",
    "            B = max(2, mem_budget_bytes // (64*1024*1024))\n",
    "            \n",
    "            # BigSort cost per table (accounts for already-sorted data)\n",
    "            def bigsort_cost(P, already_sorted=False):\n",
    "                if already_sorted:\n",
    "                    return 0  # no sort needed, just read once\n",
    "                base = max(2, B - 1)\n",
    "                passes = max(1, math.ceil(math.log(max(1, P), base)))\n",
    "                return 2 * P * (1 + passes)\n",
    "            \n",
    "            sort_cost_L = bigsort_cost(P_L, left_already_sorted)\n",
    "            sort_cost_R = bigsort_cost(P_R, right_already_sorted)\n",
    "            \n",
    "            # SMJ total: sort both + merge (read both once)\n",
    "            smj_ios = sort_cost_L + sort_cost_R + (P_L + P_R)\n",
    "            \n",
    "            # HPJ cost for comparison: 3*(P(L)+P(R)) when Cr=Cw=1\n",
    "            hpj_ios = 3 * (P_L + P_R)\n",
    "            \n",
    "            # Optionally prefer SMJ if output needs to be sorted for GROUP BY\n",
    "            if need_sorted_output:\n",
    "                smj_ios *= 0.9  # slight bias toward SMJ\n",
    "            \n",
    "            build_L = _estimate_build_bytes(rows_L, tL, overhead=overhead)\n",
    "            build_R = _estimate_build_bytes(rows_R, tR, overhead=overhead)\n",
    "            \n",
    "            return {\n",
    "                \"algorithm\": algo,\n",
    "                \"build_side\": None,  # SMJ has no build side\n",
    "                \"build_table\": None,\n",
    "                \"hpj_ios\": hpj_ios,\n",
    "                \"smj_ios\": smj_ios,\n",
    "                \"hpj_build_bytes\": {\"L\": build_L, \"R\": build_R},\n",
    "                \"estimated_ram_needed\": estimated_ram_needed,\n",
    "                \"fits_in_memory\": False,\n",
    "                \"left_already_sorted\": left_already_sorted,\n",
    "                \"right_already_sorted\": right_already_sorted,\n",
    "            }\n",
    "            \n",
    "    def _ndv_guess(meta, tbl, col, coverage=0.9):\n",
    "        \"\"\"Estimate distinct values for a column in a base table.\"\"\"\n",
    "        if tbl == \"Songs\" and col == \"song_id\": return int(meta[\"Songs\"][\"rows\"])\n",
    "        if tbl == \"Users\" and col == \"user_id\": return int(meta[\"Users\"][\"rows\"])\n",
    "        if tbl == \"Listens\" and col == \"song_id\": return int(min(meta[\"Songs\"][\"rows\"], meta[\"Listens\"][\"rows\"]) * coverage)\n",
    "        if tbl == \"Listens\" and col == \"user_id\": return int(min(meta[\"Users\"][\"rows\"], meta[\"Listens\"][\"rows\"]) * coverage)\n",
    "        return int(meta[tbl][\"rows\"])\n",
    "\n",
    "    def _rows_after_join(left_rows, right_rows, ndvL, ndvR, left_unique=False, right_unique=False):\n",
    "        \"\"\"Estimate output rows for an equi-join.\"\"\"\n",
    "        if left_unique and not right_unique:\n",
    "            return right_rows  # PK-FK: output size = FK side\n",
    "        if right_unique and not left_unique:\n",
    "            return left_rows\n",
    "        # Generic: rows_out ≈ (rows_L × rows_R) / max(ndv_L, ndv_R)\n",
    "        denom = max(1, max(ndvL, ndvR))\n",
    "        est = (left_rows * right_rows) // denom\n",
    "        # Bound between max(rows) and rows_L × rows_R\n",
    "        return max(min(est, left_rows * right_rows), max(left_rows, right_rows))\n",
    "\n",
    "    # 1) Alias: table-name mappings\n",
    "    alias_to_table_lower = parsed_query[\"tables\"]\n",
    "    alias_to_meta = {a: meta_key_for(t) for a, t in alias_to_table_lower.items()}\n",
    "\n",
    "    # 2) Columns needed per alias\n",
    "    cols_needed_by_alias: dict[str, set] = {a: set() for a in alias_to_table_lower.keys()}\n",
    "\n",
    "    # 2a) Joins (include join keys)\n",
    "    for j in parsed_query[\"joins\"][\"Joins\"]:\n",
    "        cols_needed_by_alias[j[\"left_alias\"]].add(j[\"left_column\"])\n",
    "        cols_needed_by_alias[j[\"right_alias\"]].add(j[\"right_column\"])\n",
    "\n",
    "    # 2b) GROUP BY columns\n",
    "    for a, c in parsed_query.get(\"GroupBy\", []):\n",
    "        cols_needed_by_alias[a].add(c)\n",
    "\n",
    "    # 2c) SELECT items and aggregations\n",
    "    for item in parsed_query.get(\"select\", []):\n",
    "        if item[\"kind\"] == \"column\":\n",
    "            a, c = item[\"source\"]\n",
    "            cols_needed_by_alias[a].add(c)\n",
    "\n",
    "    for agg in parsed_query.get(\"aggregations\", {}).values():\n",
    "        a, c = agg[\"source\"]\n",
    "        cols_needed_by_alias[a].add(c)\n",
    "\n",
    "    # 2d) ORDER BY columns (aggregations already covered)\n",
    "    for ob in parsed_query.get(\"orderBy\", []):\n",
    "        if ob[\"kind\"] == \"column\":\n",
    "            a, c = ob[\"source\"]\n",
    "            cols_needed_by_alias[a].add(c)\n",
    "\n",
    "    # 2e) Convert to real table names\n",
    "    columns_to_load = {}\n",
    "    for a, cols in cols_needed_by_alias.items():\n",
    "        tkey = alias_to_meta[a]\n",
    "        # intersect with actual table columns for safety\n",
    "        actual_cols = set(metadata[tkey][\"columns\"].keys())\n",
    "        columns_to_load[tkey] = sorted(list(set(cols) & actual_cols)) if actual_cols else sorted(list(cols))\n",
    "\n",
    "    # 3) Join order and algorithm selection\n",
    "    left_alias, left_table_lower = parsed_query[\"joins\"][\"base_table\"]\n",
    "    left_table_key = meta_key_for(left_table_lower)\n",
    "\n",
    "    # Track running estimate for the \"current\" intermediate result\n",
    "    current_rows = int(metadata[left_table_key][\"rows\"])\n",
    "    current_key_ndv = {}\n",
    "    left_sorted_on = None  # track if current left side is sorted (key name or None)\n",
    "\n",
    "    joins = parsed_query[\"joins\"][\"Joins\"]\n",
    "    join_plan = []\n",
    "\n",
    "    for join in joins:\n",
    "        right_table_key = meta_key_for(join[\"joined_table_name\"])\n",
    "        left_key  = join[\"left_column\"]\n",
    "        right_key = join[\"right_column\"]\n",
    "\n",
    "        # Check if sides are already sorted\n",
    "        left_already_sorted = (left_sorted_on == left_key)\n",
    "        right_already_sorted = False  # base tables assumed unsorted\n",
    "\n",
    "        # --- Algorithm decision ---\n",
    "        choice = decide_join_algo(\n",
    "            metadata,\n",
    "            left_table_key,\n",
    "            right_table_key,\n",
    "            left_key,\n",
    "            right_key,\n",
    "            rows_left=current_rows,\n",
    "            mem_budget_bytes=12*1024**3,\n",
    "            overhead=48,\n",
    "            left_already_sorted=left_already_sorted,\n",
    "            right_already_sorted=right_already_sorted,\n",
    "            need_sorted_output=len(parsed_query.get(\"GroupBy\", [])) > 0,\n",
    "        )\n",
    "        # Add join keys and table names to the choice dict\n",
    "        choice[\"left_table\"] = left_table_key\n",
    "        choice[\"right_table\"] = right_table_key\n",
    "        choice[\"left_key\"] = left_key\n",
    "        choice[\"right_key\"] = right_key\n",
    "\n",
    "\n",
    "        join_plan.append(choice)\n",
    "        print(f\"Join {left_table_key}({current_rows} rows) ⋈ {right_table_key}: {choice['algorithm']}\")\n",
    "\n",
    "        # --- Update running estimate ---\n",
    "        provider_alias = join[\"left_alias\"] if join[\"left_alias\"] != join[\"joined_table_alias\"] else join[\"right_alias\"]\n",
    "        provider_tbl = meta_key_for(parsed_query[\"tables\"][provider_alias])\n",
    "\n",
    "        ndvL = current_key_ndv.get(left_key, _ndv_guess(metadata, provider_tbl, left_key))\n",
    "        ndvR = _ndv_guess(metadata, right_table_key, right_key)\n",
    "\n",
    "        left_unique  = (provider_tbl == \"Songs\" and left_key == \"song_id\") or \\\n",
    "                    (provider_tbl == \"Users\" and left_key == \"user_id\")\n",
    "        right_unique = (right_table_key == \"Songs\" and right_key == \"song_id\") or \\\n",
    "                    (right_table_key == \"Users\" and right_key == \"user_id\")\n",
    "\n",
    "        right_rows = int(metadata[right_table_key][\"rows\"])\n",
    "        next_rows = _rows_after_join(current_rows, right_rows, ndvL, ndvR, left_unique, right_unique)\n",
    "\n",
    "        current_rows = next_rows\n",
    "        current_key_ndv[left_key]  = min(ndvL, current_rows)\n",
    "        current_key_ndv[right_key] = min(ndvR, current_rows)\n",
    "\n",
    "        # Track if output is sorted (SMJ produces sorted output on join key)\n",
    "        if choice[\"algorithm\"] == \"SMJ\":\n",
    "            # SMJ output is sorted on both keys; track the one we'll use next\n",
    "            left_sorted_on = left_key  # or right_key, depending on which side continues\n",
    "        else:\n",
    "            # HPJ output is unsorted\n",
    "            left_sorted_on = None\n",
    "\n",
    "        # Advance left_table_key\n",
    "        if choice[\"algorithm\"] == \"HPJ\":\n",
    "            left_table_key = right_table_key if choice[\"build_table\"] == left_table_key else left_table_key\n",
    "        else:\n",
    "            left_table_key = right_table_key if right_rows >= current_rows else left_table_key\n",
    "\n",
    "\n",
    "    print(f\"\\nFinal estimated rows after all joins: {current_rows}\")\n",
    "\n",
    "    # 4) Assemble final plan dictionary\n",
    "    plan = {\n",
    "        \"columns_to_load\": columns_to_load,        # Dict[table_name, List[column]]\n",
    "        \"join_plan\": join_plan,                    # List of join decisions from the loop\n",
    "        \"estimated_final_rows\": current_rows,      # Estimated rows after all joins\n",
    "        \"group_by\": parsed_query.get(\"GroupBy\", []),\n",
    "        \"select\": parsed_query.get(\"select\", []),\n",
    "        \"aggregations\": parsed_query.get(\"aggregations\", {}),\n",
    "        \"order_by\": parsed_query.get(\"orderBy\", []),\n",
    "        \"memory_budget_bytes\": memory_bytes,\n",
    "    }\n",
    "\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_dict_for_query_planner(tables, size=\"100MB\"):\n",
    "    '''\n",
    "    Returns a new table dictionary to be size specific\n",
    "    1. selects entries from original table dictionary that ends with size\n",
    "    2. drops size prefix from dictionary key\n",
    "    '''\n",
    "    return {name.split(\"_\")[0]: db for name,db in tables.items() if name.endswith(size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "UhwOd7sfhWkN"
   },
   "outputs": [],
   "source": [
    "class QueryPlanner:\n",
    "    def __init__(self, tables, query):\n",
    "        self.tables = tables\n",
    "        self.query = query\n",
    "    \n",
    "    def parse_query(self):\n",
    "        self.parsed_query = parse_sql(self.query)\n",
    "    \n",
    "    def load_metadata(self): \n",
    "        self.metadata = analyze_metadata_before_loading(self.tables)\n",
    "    \n",
    "    def load_plan(self):\n",
    "        self.plan = plan_query_execution(self.metadata, self.parsed_query)\n",
    "\n",
    "        # columns_to_load: Dict[str, List[str]]\n",
    "        # Maps table name to list of column names to read.\n",
    "        # Only includes columns needed for joins, filters, GROUP BY, aggregations, and SELECT.\n",
    "        # Example: {\"Songs\": [\"song_id\", \"title\"], \"Listens\": [\"song_id\", \"user_id\"], \"Users\": [\"user_id\", \"age\"]}\n",
    "        self.columns_to_load = self.plan[\"columns_to_load\"]\n",
    "\n",
    "        # join_plan: List[Dict]\n",
    "        # Ordered list of join steps. Each dict contains:\n",
    "        #   - \"algorithm\": \"HPJ\" or \"SMJ\"\n",
    "        #   - \"build_side\": \"L\" or \"R\" (for HPJ only, tells the smaller side, which side to build hash table on)\n",
    "        #   - \"build_table\": table name like \"Songs\" (for HPJ only, the smaller side)\n",
    "        #   - \"hpj_ios\", \"smj_ios\": estimated I/O costs for comparison\n",
    "        #   - \"hpj_build_bytes\": {\"L\": bytes, \"R\": bytes} - memory needed for each side\n",
    "        #   - \"fits_in_memory\": bool (whether build side fits in RAM)\n",
    "        self.join_plan = self.plan[\"join_plan\"]\n",
    "\n",
    "        # group_by: List[Tuple[str, str]]\n",
    "        # List of (alias, column_name) tuples for GROUP BY clause.\n",
    "        # Example: [('s', 'song_id'), ('s', 'title')] for GROUP BY s.song_id, s.title\n",
    "        self.group_by = self.plan[\"group_by\"]\n",
    "\n",
    "        # select: List[Dict]\n",
    "        # List of SELECT items. Each dict contains:\n",
    "        #   - \"kind\": \"column\" or \"aggregation\"\n",
    "        #   - For \"column\": {\"kind\": \"column\", \"source\": (alias, col_name), \"alias\": output_name or None}\n",
    "        #   - For \"aggregation\": {\"kind\": \"aggregation\", \"agg_key\": int, \"alias\": output_name or None}\n",
    "        # Example: [\n",
    "        #   {\"kind\": \"column\", \"source\": ('s', 'song_id'), \"alias\": None},\n",
    "        #   {\"kind\": \"aggregation\", \"agg_key\": 1, \"alias\": \"avg_age\"}\n",
    "        # ]\n",
    "        self.select = self.plan[\"select\"]\n",
    "\n",
    "        # aggregations: Dict[int, Dict]\n",
    "        # Maps aggregation ID (agg_key) to aggregation spec. Each spec contains:\n",
    "        #   - \"func\": \"avg\", \"count\", \"sum\", etc.\n",
    "        #   - \"source\": (alias, column_name) tuple - which column to aggregate\n",
    "        #   - \"distinct\": bool - whether COUNT(DISTINCT ...) or not\n",
    "        #   - \"output_name\": string or None - the AS alias for output\n",
    "        # Example: {\n",
    "        #   1: {\"func\": \"avg\", \"source\": ('u', 'age'), \"distinct\": False, \"output_name\": \"avg_age\"},\n",
    "        #   2: {\"func\": \"count\", \"source\": ('l', 'user_id'), \"distinct\": True, \"output_name\": None}\n",
    "        # }\n",
    "        self.aggregations = self.plan[\"aggregations\"]\n",
    "\n",
    "        # order_by: List[Dict]\n",
    "        # List of ORDER BY clauses. Each dict contains:\n",
    "        #   - \"kind\": \"column\" or \"aggregation\"\n",
    "        #   - For \"column\": {\"kind\": \"column\", \"source\": (alias, col_name), \"direction\": \"asc\"|\"desc\"}\n",
    "        #   - For \"aggregation\": {\"kind\": \"aggregation\", \"agg_key\": int, \"direction\": \"asc\"|\"desc\"}\n",
    "        self.order_by = self.plan[\"order_by\"]\n",
    "\n",
    "    def plan(self):\n",
    "        self.parse_query()\n",
    "        self.load_metadata()\n",
    "        self.load_plan()\n",
    "\n",
    "\n",
    "class QueryExecutor:\n",
    "    def __init__(self, tables, num_partitions=8, output_dir=\"temp\", \n",
    "                 planner=None, size=\"100MB\", use_streaming=True, \n",
    "                 join_algo_override = None,\n",
    "                 parquet_batch_size=1000000,\n",
    "                 num_pages_per_split=100):\n",
    "        self.tables = tables\n",
    "        self.num_partitions = num_partitions\n",
    "        self.use_streaming = use_streaming\n",
    "        self.parquet_batch_size = parquet_batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.planner = planner or QueryPlanner(get_tables_dict_for_query_planner(tables), query)\n",
    "        self.num_pages_per_split = num_pages_per_split\n",
    "        self.join_algo = join_algo_override\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        self.planner.plan()\n",
    "\n",
    "    def execute_hardcoded_query(self):\n",
    "        \"\"\"\n",
    "        Executes the following SQL query:\n",
    "\n",
    "        SELECT s.song_id, AVG(u.age) AS avg_age,\n",
    "        COUNT(DISTINCT l.user_id)\n",
    "        FROM Songs s\n",
    "        JOIN Listens l ON s.song_id = l.song_id\n",
    "        JOIN Users u ON l.user_id = u.user_id\n",
    "        GROUP BY s.song_id, s.title\n",
    "        ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
    "        \"\"\"\n",
    "        # Hardcoded\n",
    "        columns = self.planner.columns_to_load\n",
    "\n",
    "        \n",
    "        join_order = [\"Listens\"] + [join_step[\"build_table\"] for join_step in self.planner.join_plan]\n",
    "        join_algorithm =  [\n",
    "            (self.join_algo if self.join_algo else join_step[\"algorithm\"])\n",
    "            for join_step in self.planner.join_plan\n",
    "        ]\n",
    "        join_keys = [join_step[\"left_key\"] for join_step in self.planner.join_plan]\n",
    "\n",
    "        # do joins\n",
    "        result = self.tables[f\"{join_order[0]}_{SIZE}\"]\n",
    "        columns_table1 = columns[join_order[0]]\n",
    "        for i in range(1, len(join_order)):\n",
    "            table = self.tables[f\"{join_order[i]}_{SIZE}\"]\n",
    "            if join_algorithm[i-1] == \"HPJ\":\n",
    "                hpj = FastHashPartitionJoin(num_partitions=self.num_partitions, parquet_batch_size=self.parquet_batch_size, use_streaming=self.use_streaming)\n",
    "                result = hpj.join(result, table, join_keys[i-1], join_keys[i-1], columns_table1=columns_table1, columns_table2=columns[join_order[i]])\n",
    "            else:\n",
    "                smj = SortMergeJoin(num_pages_per_split=self.num_pages_per_split)\n",
    "                result = smj.join(result, table, join_keys[i-1], join_keys[i-1], columns_table1=columns_table1, columns_table2=columns[join_order[i]])\n",
    "            columns_table1 = None # after first join, keep all columns from Left table\n",
    "\n",
    "        # do group by \n",
    "        groupby_cols = [\"song_id\", \"title\"]\n",
    "        avg_col = \"age\"\n",
    "        avg_col_name = \"avg_age\"\n",
    "        distinct_col = \"user_id\"\n",
    "        distinct_col_name = \"count_distinct_user_id\"\n",
    "        select_cols = [\"song_id\"]\n",
    "\n",
    "        groupby_average_distinct = HashGroupbyAverageAndDistinct(\n",
    "            num_partitions=self.num_partitions,\n",
    "            parquet_batch_size=self.parquet_batch_size,\n",
    "            use_streaming=self.use_streaming\n",
    "        )\n",
    "\n",
    "        result = groupby_average_distinct.groupby_average_distinct(\n",
    "            result, \n",
    "            groupby_cols=groupby_cols,\n",
    "            average_col=avg_col,\n",
    "            average_col_name=avg_col_name,\n",
    "            distinct_col=distinct_col,\n",
    "            distinct_col_name=distinct_col_name,\n",
    "            select_cols=select_cols\n",
    "        )\n",
    "        \n",
    "        # sort by count distinct\n",
    "        smj_sorter = SortMergeJoin()\n",
    "        sorted_result = smj_sorter._external_sort(\n",
    "            result,\n",
    "            \"count_distinct_user_id\",\n",
    "            self.output_dir,\n",
    "            \"result\",\n",
    "            ascending = False,\n",
    "            tiebreak_key=\"song_id\"\n",
    "        )\n",
    "\n",
    "        return sorted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BunT6g1HklaH"
   },
   "source": [
    "# Section 5: Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "LpRb3IsakmzT"
   },
   "outputs": [],
   "source": [
    "def benchmark_query(executor, dataset_size):\n",
    "    \"\"\"Benchmark the query execution time and memory usage.\"\"\"\n",
    "    print(f\"\\nBenchmarking with {dataset_size} dataset...\")\n",
    "    start_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = executor.execute_hardcoded_query()\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "\n",
    "    print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Memory Usage: {end_mem - start_mem:.2f} MB\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUny8jDziWxk"
   },
   "source": [
    "## 100MB Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "4OBi6vhriYRK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berk/opt/miniforge3/envs/cs145-project2/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 'parse_sql' took 0.0001 seconds.\n",
      "Join Songs(10000 rows) ⋈ Listens: HPJ\n",
      "Join Listens(1000000 rows) ⋈ Users: HPJ\n",
      "\n",
      "Final estimated rows after all joins: 1000000\n",
      "\n",
      "Benchmarking with 100MB dataset...\n",
      "Method '_external_sort' took 0.1552 seconds.\n",
      "Method '_external_sort' took 0.0714 seconds.\n",
      "Method '_streaming_inner_join' took 9.1655 seconds.\n",
      "Method 'join' took 9.3925 seconds.\n",
      "Method '_external_sort' took 0.8786 seconds.\n",
      "Method '_external_sort' took 0.0721 seconds.\n",
      "Method '_streaming_inner_join' took 48.1490 seconds.\n",
      "Method 'join' took 49.1001 seconds.\n",
      "Method 'groupby_average_distinct' took 3.8671 seconds.\n",
      "Method '_external_sort' took 0.0611 seconds.\n",
      "Execution Time: 62.42 seconds\n",
      "Memory Usage: 87.05 MB\n",
      "peak memory: 965.44 MiB, increment: 149.02 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "SIZE = \"100MB\"\n",
    "executor_smj = QueryExecutor(tables, size=SIZE, join_algo_override=\"SMJ\", \n",
    "                             output_dir=\"temp_smj_100mb\")\n",
    "output_smj_100mb = benchmark_query(executor_smj, SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berk/opt/miniforge3/envs/cs145-project2/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 'parse_sql' took 0.0002 seconds.\n",
      "Join Songs(10000 rows) ⋈ Listens: HPJ\n",
      "Join Listens(1000000 rows) ⋈ Users: HPJ\n",
      "\n",
      "Final estimated rows after all joins: 1000000\n",
      "\n",
      "Benchmarking with 100MB dataset...\n",
      "Method '_hash_partition' took 0.3273 seconds.\n",
      "Method '_hash_partition' took 0.0120 seconds.\n",
      "Method '_build_hash_map' took 0.0135 seconds.\n",
      "Method '_build_hash_map' took 0.0133 seconds.\n",
      "Method '_build_hash_map' took 0.0131 seconds.\n",
      "Method '_build_hash_map' took 0.0128 seconds.\n",
      "Method '_build_hash_map' took 0.0130 seconds.\n",
      "Method '_build_hash_map' took 0.0140 seconds.\n",
      "Method '_build_hash_map' took 0.0133 seconds.\n",
      "Method '_build_hash_map' took 0.0129 seconds.\n",
      "Method 'join' took 2.0401 seconds.\n",
      "Method '_hash_partition' took 0.3189 seconds.\n",
      "Method '_hash_partition' took 0.0220 seconds.\n",
      "Method '_build_hash_map' took 0.0596 seconds.\n",
      "Method '_build_hash_map' took 0.0604 seconds.\n",
      "Method '_build_hash_map' took 0.0600 seconds.\n",
      "Method '_build_hash_map' took 0.0605 seconds.\n",
      "Method '_build_hash_map' took 0.0591 seconds.\n",
      "Method '_build_hash_map' took 0.0588 seconds.\n",
      "Method '_build_hash_map' took 0.0590 seconds.\n",
      "Method '_build_hash_map' took 0.0598 seconds.\n",
      "Method 'join' took 2.3985 seconds.\n",
      "Method 'groupby_average_distinct' took 3.2129 seconds.\n",
      "Method '_external_sort' took 0.0628 seconds.\n",
      "Execution Time: 7.71 seconds\n",
      "Memory Usage: 95.69 MB\n",
      "peak memory: 891.81 MiB, increment: 100.70 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "executor_hpj = QueryExecutor(tables, size=SIZE, join_algo_override=\"HPJ\", \n",
    "                             output_dir=\"temp_hpj_100mb\")\n",
    "output_hpj_100mb = benchmark_query(executor_hpj, SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure results are equal, utilizes custom implemented equals method on ColumnarDbFile\n",
    "output_hpj_100mb == output_smj_100mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HalUj5s-ifAu"
   },
   "source": [
    "## 1GB Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "CwmefvmzigRO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 'parse_sql' took 0.0001 seconds.\n",
      "Join Songs(10000 rows) ⋈ Listens: HPJ\n",
      "Join Listens(1000000 rows) ⋈ Users: HPJ\n",
      "\n",
      "Final estimated rows after all joins: 1000000\n",
      "\n",
      "Benchmarking with 1GB dataset...\n",
      "Method '_external_sort' took 1.4152 seconds.\n",
      "Method '_external_sort' took 0.0861 seconds.\n",
      "Method '_streaming_inner_join' took 89.3497 seconds.\n",
      "Method 'join' took 90.8516 seconds.\n",
      "Method '_external_sort' took 7.7476 seconds.\n",
      "Method '_external_sort' took 0.0875 seconds.\n",
      "Method '_streaming_inner_join' took 510.1438 seconds.\n",
      "Method 'join' took 517.9800 seconds.\n",
      "Method 'groupby_average_distinct' took 43.7831 seconds.\n",
      "Method '_external_sort' took 0.0842 seconds.\n",
      "Execution Time: 652.70 seconds\n",
      "Memory Usage: 1454.02 MB\n",
      "peak memory: 2856.08 MiB, increment: 2659.83 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "SIZE = \"1GB\"\n",
    "executor_smj = QueryExecutor(tables, size=SIZE, join_algo_override=\"SMJ\", \n",
    "                             output_dir=\"temp_smj_1GB\")\n",
    "output_smj_1gb = benchmark_query(executor_smj, SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 'parse_sql' took 0.0002 seconds.\n",
      "Join Songs(10000 rows) ⋈ Listens: HPJ\n",
      "Join Listens(1000000 rows) ⋈ Users: HPJ\n",
      "\n",
      "Final estimated rows after all joins: 1000000\n",
      "\n",
      "Benchmarking with 1GB dataset...\n",
      "Method '_hash_partition' took 2.8575 seconds.\n",
      "Method '_hash_partition' took 0.0751 seconds.\n",
      "Method '_build_hash_map' took 0.1243 seconds.\n",
      "Method '_build_hash_map' took 0.1247 seconds.\n",
      "Method '_build_hash_map' took 0.1243 seconds.\n",
      "Method '_build_hash_map' took 0.1278 seconds.\n",
      "Method '_build_hash_map' took 0.1259 seconds.\n",
      "Method '_build_hash_map' took 0.1261 seconds.\n",
      "Method '_build_hash_map' took 0.1256 seconds.\n",
      "Method '_build_hash_map' took 0.1253 seconds.\n",
      "Method 'join' took 12.3049 seconds.\n",
      "Method '_hash_partition' took 3.4995 seconds.\n",
      "Method '_hash_partition' took 0.1758 seconds.\n",
      "Method '_build_hash_map' took 0.6315 seconds.\n",
      "Method '_build_hash_map' took 0.6109 seconds.\n",
      "Method '_build_hash_map' took 0.6154 seconds.\n",
      "Method '_build_hash_map' took 0.6145 seconds.\n",
      "Method '_build_hash_map' took 0.6144 seconds.\n",
      "Method '_build_hash_map' took 0.6149 seconds.\n",
      "Method '_build_hash_map' took 0.6106 seconds.\n",
      "Method '_build_hash_map' took 0.6127 seconds.\n",
      "Method 'join' took 17.4998 seconds.\n",
      "Method 'groupby_average_distinct' took 36.9421 seconds.\n",
      "Method '_external_sort' took 0.0824 seconds.\n",
      "Execution Time: 66.83 seconds\n",
      "Memory Usage: 152.84 MB\n",
      "peak memory: 1668.80 MiB, increment: 430.66 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "executor_hpj = QueryExecutor(tables, size=SIZE, join_algo_override=\"HPJ\", \n",
    "                             output_dir=\"temp_hpj_1gb\")\n",
    "output_hpj_1gb = benchmark_query(executor_hpj, SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 GB Challenge\n",
    "Design considerations that allowed this our discussed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 'parse_sql' took 0.0002 seconds.\n",
      "Join Songs(10000 rows) ⋈ Listens: HPJ\n",
      "Join Listens(1000000 rows) ⋈ Users: HPJ\n",
      "\n",
      "Final estimated rows after all joins: 1000000\n",
      "\n",
      "Benchmarking with 10GB dataset...\n",
      "Method '_hash_partition' took 29.2806 seconds.\n",
      "Method '_hash_partition' took 0.5365 seconds.\n",
      "Method '_build_hash_map' took 1.2689 seconds.\n",
      "Method '_build_hash_map' took 1.2185 seconds.\n",
      "Method '_build_hash_map' took 1.2179 seconds.\n",
      "Method '_build_hash_map' took 1.2235 seconds.\n",
      "Method '_build_hash_map' took 1.2203 seconds.\n",
      "Method '_build_hash_map' took 1.2251 seconds.\n",
      "Method '_build_hash_map' took 1.2227 seconds.\n",
      "Method '_build_hash_map' took 1.2263 seconds.\n",
      "Method 'join' took 126.1223 seconds.\n",
      "Method '_hash_partition' took 43.6144 seconds.\n",
      "Method '_hash_partition' took 1.4022 seconds.\n",
      "Method '_build_hash_map' took 6.2268 seconds.\n",
      "Method '_build_hash_map' took 6.1643 seconds.\n",
      "Method '_build_hash_map' took 6.2018 seconds.\n",
      "Method '_build_hash_map' took 6.1267 seconds.\n",
      "Method '_build_hash_map' took 6.1332 seconds.\n",
      "Method '_build_hash_map' took 6.1904 seconds.\n",
      "Method '_build_hash_map' took 6.1754 seconds.\n",
      "Method '_build_hash_map' took 6.1360 seconds.\n",
      "Method 'join' took 209.7325 seconds.\n",
      "Method 'groupby_average_distinct' took 582.6983 seconds.\n",
      "Method '_external_sort' took 0.2542 seconds.\n",
      "Execution Time: 918.81 seconds\n",
      "Memory Usage: 869.44 MB\n",
      "peak memory: 2224.78 MiB, increment: 1399.91 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "SIZE = \"10GB\"\n",
    "executor_hpj = QueryExecutor(tables, size=SIZE, join_algo_override=\"HPJ\", \n",
    "                             output_dir=\"temp_hpj_10GB\")\n",
    "output_hpj = benchmark_query(executor_hpj, SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8h2f0TAijZT"
   },
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| DATASET SIZE | ALGORITHM | TOTAL TIME (S) | JOIN TIME (First + Second Join) (S) | PEAK MEMORY (GB) |\n",
    "| ------------ | --------- | -------------- | ------------- | ---------------- |\n",
    "| 100MB        | HPJ       | 7.71   | 2.04 + 2.39 = 4.43   |  0.93 |\n",
    "| 100MB        | SMJ       | 62.42   | 9.39 + 49.1 = 58.49  | 1.01  |\n",
    "| 1GB          | HPJ       | **66.83**  | 12.30 + 17.50 = 29.80  | 1.75   |\n",
    "| 1GB          | SMJ       | 625.41   | 91.94 + 491.85 = 583.79    | 3.00    |\n",
    "| **10GB**          | HPJ       | 918.81   | 29.28 + 126.12 = 155.4    | 2.33   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our HPJ algorithm wildly outperformed the benchmarks in terms of speed and displayed great memory efficiency. We implemented the following optimizations to achieve speed and memory performance:\n",
    "\n",
    "- Partitioned Processing: Divides data into smaller partitions using hash-based distribution, enabling processing of datasets larger than available RAM by only loading one partition pair at a time into memory.\n",
    "- Asymmetric Join Strategy: Identifies the smaller side of each partition to build the in-memory hash table, then streams the larger side in configurable batches (default 50k rows), reducing peak memory usage from O(2×partition_size) to O(smaller_partition + batch_size).\n",
    "- Vectorized Operations for join: Replaces row-by-row Python loops with bulk NumPy/Pandas operations - using groupby for hash map construction (5-10x faster), array-based index collection, and single bulk .iloc[array] calls instead of thousands of individual .iloc[i] accesses.\n",
    "- Streaming Output Mode: Optionally writes results to a single Parquet file using ParquetWriter instead of creating multiple files, reducing I/O overhead and file fragmentation for large result sets. See ColumnarDbFile.start_stream()\n",
    "    - this provided minimal speedup (~1-2 seconds) but provides better disk usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within our SMJ algorithm, we identified several bottlenecks. The primary bottleneck in Sort-Merge Join (SMJ) is the external sorting phase, which incurs O(n log n) CPU cost and 2–3× I/O amplification per sort pass when data exceeds memory. For large tables sorting dominates runtime, especially with small memory budgets that force many merge passes. Key optimizations include: \n",
    "1) increasing buffer size: to reduce the number of external merge passes—each doubling of B can halve the pass count via B-way merging\n",
    "2) parallel sorting: by partitioning input data across threads and sorting runs concurrently before a final merge\n",
    "3) Hybrid approaches: For workloads with skewed join keys, we could have implemented that hash-partition only the heavy-hitter groups after sorting can prevent memory blowups during the merge without re-sorting the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Design Choices to Allow for 10GB dataset processing**: The most important design consideration was to never load a full table into RAM. Instead we paginate the reading. We relied on loading datasets in batches (see ColumnarDBFile.iter_pages) or by calling iter_batches on intermediate parquet files. Keeping a batch size that is large but still within RAM constraints was crucial for performance. \n",
    "\n",
    "These strategies were implemented in external_sort to avoid fully sorting in memory. \n",
    "\n",
    "In our HPJ join implementation, we rely on keep a large number of partitions (~8) and only loading the smaller partition file into memory to build the hash table. The larger partition file is loaded in batches as discussed above. \n",
    "\n",
    "Finally, in our groupby implementation, it would be infeasible to call pd.groupby on a whole dataset (would require loading into memory). Thus we hash partition on the groupby key and call groupby on each resulting partition file, gaurenteeing that the same groupby key is in the same file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Potential Optimizations:\n",
    "- the GroupBy operation showed performance slower (see outputs above) than HPJ joins. this could be improved by implementing COUNT(DISTINCT) with HyperLogLog approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs145-project2",
   "language": "python",
   "name": "cs145-project2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
