{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/berkyalcinkaya/cs145-project2-systems/blob/main/cs145_project2_systems_template_fa2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab (Main)\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/berkyalcinkaya/cs145-project2-systems/blob/berk/cs145_project2_systems_template_fa2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab (berk)\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM8u06dhhieu"
   },
   "source": [
    "## Collaborators\n",
    "\n",
    "1.   Berk Yalcinkaya\n",
    "2.   Nick Allen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-T7CuUFejs1R"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kSLvQ6xjjqQh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import time\n",
    "import psutil\n",
    "import heapq\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable, Dict, Union, Any, Tuple\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "from IPython.display import display\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_parquet_files():\n",
    "    for file in glob.glob(\"*.parquet\"):\n",
    "        os.remove(file)\n",
    "    return\n",
    "\n",
    "clear_parquet_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwVW6zMghmMq"
   },
   "source": [
    "# Section 0: Generate Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y0yOxXShWkL"
   },
   "source": [
    "This section has already been implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Djt3vqewhA76"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def generate_songs_chunk(start, size, string_length=100):\n",
    "    data = {\n",
    "        \"song_id\": range(start, start + size),\n",
    "        \"title\": [f\"Song_{i}\" for i in range(start, start + size)],\n",
    "    }\n",
    "    base_strings = generate_base_strings(size, string_length)\n",
    "    for i in range(1, 11):\n",
    "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def generate_users_chunk(start, size, string_length=100):\n",
    "    data = {\n",
    "        \"user_id\": range(start, start + size),\n",
    "        \"age\": [18 + ((start + i) % 60) for i in range(size)],\n",
    "    }\n",
    "    base_strings = generate_base_strings(size, string_length)\n",
    "    for i in range(1, 11):\n",
    "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def generate_listens_chunk(start, size, num_users, num_songs, string_length=16):\n",
    "    data = {\n",
    "        \"listen_id\": range(start, start + size),\n",
    "        \"user_id\": np.random.randint(0, num_users, size=size),\n",
    "        \"song_id\": np.random.randint(0, num_songs, size=size),\n",
    "    }\n",
    "    base_strings = generate_base_strings(size, string_length)\n",
    "    for i in range(1, 11):\n",
    "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def generate_base_strings(num_records, string_length):\n",
    "    chars = np.array(list(\"ab\"))\n",
    "    random_indices = np.random.randint(0, len(chars), size=(num_records, string_length))\n",
    "    char_array = chars[random_indices]\n",
    "    return np.array(list(map(\"\".join, char_array)))\n",
    "\n",
    "\n",
    "def _write_parquet_streamed(\n",
    "    filename,\n",
    "    total_rows,\n",
    "    make_chunk_fn,\n",
    "    chunk_size=250_000,\n",
    "    compression=\"snappy\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream DataFrame chunks to a single Parquet file with one ParquetWriter.\n",
    "    - schema_df: optional small DataFrame to lock schema; if None we'll infer from the first chunk.\n",
    "    \"\"\"\n",
    "    written = 0\n",
    "\n",
    "    first_chunk = make_chunk_fn(0, min(chunk_size, total_rows))\n",
    "    first_table = pa.Table.from_pandas(first_chunk, preserve_index=False)\n",
    "    writer = pq.ParquetWriter(filename, first_table.schema, compression=compression)\n",
    "    writer.write_table(first_table)\n",
    "\n",
    "    written += len(first_chunk)\n",
    "    del first_chunk\n",
    "    gc.collect()\n",
    "\n",
    "    while written < total_rows:\n",
    "        take = min(chunk_size, total_rows - written)\n",
    "        chunk_df = make_chunk_fn(written, take)\n",
    "        writer.write_table(pa.Table.from_pandas(chunk_df, preserve_index=False))\n",
    "        written += take\n",
    "        del chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def generate_test_data(target_size=\"100MB\"):\n",
    "    \"\"\"\n",
    "    Generate datasets with proper foreign key relationships.\n",
    "\n",
    "    Target COMPRESSED Parquet file sizes on disk:\n",
    "    100MB total compressed:\n",
    "        - Songs: 10K rows → ~5MB (5% of total)\n",
    "        - Users: 50K rows → ~20MB (20% of total)\n",
    "        - Listens: 1M rows → ~75MB (75% of total)\n",
    "    1GB total compressed:\n",
    "        - Songs: 100K rows → ~50MB (5% of total)\n",
    "        - Users: 500K rows → ~200MB (20% of total)\n",
    "        - Listens: 10M rows → ~750MB (75% of total)\n",
    "\n",
    "    Each table needs:\n",
    "        - Primary key column(s)\n",
    "        - 10 additional string columns of k characters each\n",
    "        - For Users: add 'age' column (random 18-80)\n",
    "\n",
    "    CRITICAL: Listens table must have valid foreign keys!\n",
    "    Every song_id must exist in Songs\n",
    "    Every user_id must exist in Users\n",
    "    \"\"\"\n",
    "\n",
    "    assert target_size in [\"100MB\", \"1GB\", \"10GB\"]\n",
    "    if target_size == \"100MB\":\n",
    "        num_songs = 10_000\n",
    "        num_users = 50_000\n",
    "        num_listens = 1_000_000\n",
    "\n",
    "        songs_chunk = 10_000\n",
    "        users_chunk = 50_000\n",
    "        listens_chunk = 1_000_000\n",
    "    elif target_size == \"1GB\":\n",
    "        num_songs = 100_000\n",
    "        num_users = 500_000\n",
    "        num_listens = 10_000_000\n",
    "\n",
    "        songs_chunk = 10_000\n",
    "        users_chunk = 50_000\n",
    "        listens_chunk = 1_000_000\n",
    "    else: \n",
    "        num_songs = 1_000_000\n",
    "        num_users = 5_000_000\n",
    "        num_listens = 100_000_000\n",
    "\n",
    "        songs_chunk = 10_000\n",
    "        users_chunk = 50_000\n",
    "        listens_chunk = 1_000_000\n",
    "\n",
    "    print(\"Writing Songs\")\n",
    "    _write_parquet_streamed(\n",
    "        filename=f\"songs_{target_size}.parquet\",\n",
    "        total_rows=num_songs,\n",
    "        make_chunk_fn=lambda start, size: generate_songs_chunk(start, size),\n",
    "        chunk_size=songs_chunk,\n",
    "    )\n",
    "\n",
    "    print(\"Writing Users\")\n",
    "    _write_parquet_streamed(\n",
    "        filename=f\"users_{target_size}.parquet\",\n",
    "        total_rows=num_users,\n",
    "        make_chunk_fn=lambda start, size: generate_users_chunk(start, size),\n",
    "        chunk_size=users_chunk,\n",
    "    )\n",
    "\n",
    "    print(\"Writing Listens\")\n",
    "    _write_parquet_streamed(\n",
    "        filename=f\"listens_{target_size}.parquet\",\n",
    "        total_rows=num_listens,\n",
    "        make_chunk_fn=lambda start, size: generate_listens_chunk(\n",
    "            start, size, num_users, num_songs\n",
    "        ),\n",
    "        chunk_size=listens_chunk,\n",
    "    )\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0b: Define Memory and Performance Benchmarking Functions\n",
    "- Memory will be monitored using the memory_profiler function: %%memit above a cell monitors memory usage of entire cell, %memit monitors the memory usage of a single line\n",
    "- CPU performance will be measured with a custom decorator defined below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure and print the execution time of a function.\n",
    "\n",
    "    Usage:\n",
    "        @timer\n",
    "        def my_function(...):\n",
    "            ...\n",
    "\n",
    "    When the decorated function is called, it will print the elapsed time in seconds with a descriptive message.\n",
    "\n",
    "    Returns:\n",
    "        The result of the wrapped function, after printing its runtime.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        print(f\"Method '{func.__name__}' took {end_time - start_time:.4f} seconds.\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5qnpQghhWkL",
    "outputId": "40816b75-44cb-4338-8610-e9838e59970a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100MB data already generated\n",
      "1GB data already generated\n",
      "Writing Songs\n",
      "Writing Users\n",
      "Writing Listens\n",
      "Done!\n",
      "peak memory: 2553.41 MiB, increment: 718.86 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "random.seed(0)\n",
    "if not os.path.exists(\"listens_100MB.parquet\"):\n",
    "    generate_test_data(\"100MB\")\n",
    "else:\n",
    "    print(\"100MB data already generated\")\n",
    "if not os.path.exists(\"listens_1GB.parquet\"):\n",
    "    generate_test_data('1GB')\n",
    "else:\n",
    "    print(\"1GB data already generated\")\n",
    "if not os.path.exists(\"listens_10GB.parquet\"):\n",
    "    generate_test_data('10GB')\n",
    "else:\n",
    "    print(\"10GB data already generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEiGGznFhtxo"
   },
   "source": [
    "# Section 1: Parquet-based Columnar Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BGrkP5PhWkM"
   },
   "source": [
    "Implement Parquet-based storage for the tables\n",
    "- For simplicity, store all data for a table in a single Parquet file and use a single DataFrame object as a buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b0o8zkpGhWkM"
   },
   "outputs": [],
   "source": [
    "# see ed: https://edstem.org/us/courses/87394/discussion/7251811 for advice on writing to a parquet without loading existing into RAM\n",
    "# a ColumnarDbFile is actually a directory with an arbitrary number of parquet files inside\n",
    "# Append writes a new file with the next postfix\n",
    "# Retrieve reads all parquet files and concatenates them together, done natively by pandas\n",
    "class ColumnarDbFile:\n",
    "    def __init__(self, table_name, file_dir='data', file_pfx=''):\n",
    "        self.file_pfx = file_pfx\n",
    "        self.table_name = table_name\n",
    "        self.file_dir = file_dir\n",
    "        #os.makedirs(self.file_dir, exist_ok=True)\n",
    "        self.base_file_name = f\"{self.file_dir}/{self.file_pfx}_{self.table_name}\"\n",
    "        os.makedirs(self.base_file_name, exist_ok=True)\n",
    "        \n",
    "        # Streaming state\n",
    "        self._streaming = False\n",
    "        self._stream_writer = None\n",
    "        self._stream_file_path = None\n",
    "\n",
    "    def build_table(self, data):\n",
    "        \"\"\"Build and save table data to Parquet.\"\"\"\n",
    "        assert self._get_num_parquets() == 0\n",
    "        target_path = f\"{self.base_file_name}/{self.table_name}-0.parquet\"\n",
    "        # If data is a string and is a valid file path, copy it\n",
    "        if isinstance(data, str) and os.path.isfile(data):\n",
    "            shutil.copy(data, target_path)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            data.to_parquet(target_path)\n",
    "        else:\n",
    "            raise ValueError(\"data must be a pandas DataFrame or a valid file path string\")\n",
    "        return\n",
    "\n",
    "    def retrieve_data(self, columns=None, sample=None):\n",
    "        \"\"\"Create pd.DataFrame by reading from Parquet\"\"\"\n",
    "        if sample is not None:\n",
    "            return next(self.iter_pages(sample, columns=columns, as_pandas=True))\n",
    "        else:\n",
    "            return pd.read_parquet(self.base_file_name, columns=columns)\n",
    "\n",
    "    def append_data(self, data):\n",
    "        \"\"\"Append new data to Parquet\n",
    "        \n",
    "        Behavior depends on streaming mode:\n",
    "        - If streaming (start_stream() called): writes to a single parquet file via ParquetWriter\n",
    "        - Otherwise: creates a new parquet file for each call\n",
    "        \"\"\"\n",
    "        if self._streaming:\n",
    "            # Convert DataFrame to PyArrow Table\n",
    "            table = pa.Table.from_pandas(data, preserve_index=False)\n",
    "            \n",
    "            # Lazy writer creation: create on first append with schema\n",
    "            if self._stream_writer is None:\n",
    "                self._stream_writer = pq.ParquetWriter(self._stream_file_path, table.schema)\n",
    "            \n",
    "            # Write to stream\n",
    "            self._stream_writer.write_table(table)\n",
    "        else:\n",
    "            # Original behavior: create new file\n",
    "            data.to_parquet(self.get_new_parquet_file())\n",
    "        return\n",
    "\n",
    "    def get_new_parquet_file(self):\n",
    "        '''return a path to a new file with name uniqueness'''\n",
    "        return f\"{self.base_file_name}/{self.table_name}-{self._get_num_parquets()}.parquet\"\n",
    "\n",
    "    def _get_num_parquets(self):\n",
    "        return len(self.get_all_parquet_paths())\n",
    "\n",
    "    def get_all_parquet_paths(self):\n",
    "        return glob.glob(f\"{self.base_file_name}/*.parquet\")\n",
    "    \n",
    "    def start_stream(self):\n",
    "        \"\"\"Start streaming mode for efficient batch writes.\n",
    "        \n",
    "        After calling this, append_data() will write to a single parquet file\n",
    "        using ParquetWriter (streaming) instead of creating separate files.\n",
    "        Must call stop_stream() when done to properly close the writer.\n",
    "        \n",
    "        If called multiple times, closes any existing writer and starts a new stream.\n",
    "        \n",
    "        Can also be used as a context manager:\n",
    "            with output_db:\n",
    "                output_db.append_data(df1)\n",
    "                output_db.append_data(df2)\n",
    "            # Automatically stops streaming\n",
    "        \"\"\"\n",
    "        # Close existing writer if streaming was already active\n",
    "        if self._streaming and self._stream_writer is not None:\n",
    "            self._stream_writer.close()\n",
    "        \n",
    "        # Initialize streaming state\n",
    "        self._streaming = True\n",
    "        self._stream_file_path = self.get_new_parquet_file()\n",
    "        self._stream_writer = None  # Will be created lazily on first append_data()\n",
    "    \n",
    "    def stop_stream(self):\n",
    "        \"\"\"Stop streaming mode and close the ParquetWriter.\n",
    "        \n",
    "        Safe to call multiple times or if streaming was never started.\n",
    "        \"\"\"\n",
    "        if self._stream_writer is not None:\n",
    "            self._stream_writer.close()\n",
    "            self._stream_writer = None\n",
    "        \n",
    "        self._streaming = False\n",
    "        self._stream_file_path = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry: start streaming mode.\"\"\"\n",
    "        self.start_stream()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit: stop streaming mode.\"\"\"\n",
    "        self.stop_stream()\n",
    "        return False  # Don't suppress exceptions\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Destructor: ensure stream is closed if not explicitly stopped.\"\"\"\n",
    "        # Safety net: close writer if streaming was left open\n",
    "        if self._streaming and self._stream_writer is not None:\n",
    "            try:\n",
    "                self._stream_writer.close()\n",
    "            except:\n",
    "                pass  # Ignore errors during cleanup\n",
    "\n",
    "    def table_metadata(self):\n",
    "        \"\"\"Return total rows and total byte size of the table without loading data.\"\"\"\n",
    "        parquet_files = glob.glob(f\"{self.base_file_name}/*.parquet\")\n",
    "\n",
    "        total_rows = 0\n",
    "        total_bytes = 0\n",
    "\n",
    "        for file in parquet_files:\n",
    "            pf = pq.ParquetFile(file)\n",
    "            meta = pf.metadata\n",
    "\n",
    "            total_rows += meta.num_rows\n",
    "            total_bytes += meta.serialized_size  # includes footer + metadata\n",
    "\n",
    "        return {\n",
    "            \"num_files\": len(parquet_files),\n",
    "            \"total_rows\": total_rows,\n",
    "            \"total_compressed_bytes\": total_bytes,\n",
    "        }\n",
    "\n",
    "    def table_disk_usage(self):\n",
    "        parquet_files = glob.glob(f\"{self.base_file_name}/*.parquet\")\n",
    "\n",
    "        total_bytes = sum(os.path.getsize(f) for f in parquet_files)\n",
    "\n",
    "        return {\n",
    "            \"num_files\": len(parquet_files),\n",
    "            \"total_bytes\": total_bytes\n",
    "        }\n",
    "\n",
    "    def iter_pages(self, rows_per_batch: int = 100_000, columns=None, as_pandas=True):\n",
    "        for path in self.get_all_parquet_paths():        \n",
    "            pf = pq.ParquetFile(path)\n",
    "            for batch in pf.iter_batches(batch_size=rows_per_batch, columns=columns):\n",
    "                yield batch.to_pandas() if as_pandas else batch\n",
    "\n",
    "    @staticmethod\n",
    "    def fits_in_12GB(bytes_needed: int) -> bool:\n",
    "        TWELVE_GB = 12 * 1024**3\n",
    "        return bytes_needed <= TWELVE_GB\n",
    "\n",
    "    @staticmethod\n",
    "    def can_process_parquet(bytes_on_disk: int, compression_factor: int = 5) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if a Parquet dataset of `bytes_on_disk` can be processed\n",
    "        within 12 GB of RAM, after accounting for decompression expansion.\n",
    "        \"\"\"\n",
    "        estimated_ram = bytes_on_disk * compression_factor\n",
    "        TWELVE_GB = 12 * 1024**3\n",
    "        return estimated_ram <= TWELVE_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tables...\n",
      "Removing existing data directory\n",
      "Tables built successfully.\n",
      "peak memory: 1770.94 MiB, increment: 0.14 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "print(\"Building tables...\")\n",
    "if os.path.exists('data'):\n",
    "    print(\"Removing existing data directory\")\n",
    "    shutil.rmtree('data')\n",
    "\n",
    "sizes = [\"100MB\", \"1GB\", \"10GB\"]\n",
    "tables = {}\n",
    "for size in sizes:\n",
    "    for table_name in [\"Songs\", \"Users\", \"Listens\"]:\n",
    "        key = f\"{table_name}_{size}\"\n",
    "        tables[key] = ColumnarDbFile(f\"{table_name}_{size}\", file_dir='data')\n",
    "        parquet_path = f\"{table_name.lower()}_{size}.parquet\"\n",
    "        assert os.path.exists(parquet_path)\n",
    "        tables[key].build_table(parquet_path)\n",
    "\n",
    "print(\"Tables built successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "LfutqrA2hWkM",
    "outputId": "3fdee8d3-19d1-404a-cbb8-312fc4ff8485"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Song_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Song_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Song_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Song_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Song_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>Song_9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>Song_9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>Song_9997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>Song_9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>Song_9999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      song_id      title\n",
       "0           0     Song_0\n",
       "1           1     Song_1\n",
       "2           2     Song_2\n",
       "3           3     Song_3\n",
       "4           4     Song_4\n",
       "...       ...        ...\n",
       "9995     9995  Song_9995\n",
       "9996     9996  Song_9996\n",
       "9997     9997  Song_9997\n",
       "9998     9998  Song_9998\n",
       "9999     9999  Song_9999\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve data\n",
    "tables['Songs_100MB'].retrieve_data(columns = ['song_id', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ZiPY1Hs9hWkM",
    "outputId": "d1bad27a-9419-41f8-94b9-1aa6abe5d6bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listen_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>song_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19936</td>\n",
       "      <td>7687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37756</td>\n",
       "      <td>9045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>35676</td>\n",
       "      <td>3593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18861</td>\n",
       "      <td>2977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9826</td>\n",
       "      <td>4653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>999995</td>\n",
       "      <td>15502</td>\n",
       "      <td>4168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>999996</td>\n",
       "      <td>1562</td>\n",
       "      <td>1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>999997</td>\n",
       "      <td>5838</td>\n",
       "      <td>2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>999998</td>\n",
       "      <td>35276</td>\n",
       "      <td>1541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>999999</td>\n",
       "      <td>24936</td>\n",
       "      <td>6419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        listen_id  user_id  song_id\n",
       "0               0    19936     7687\n",
       "1               1    37756     9045\n",
       "2               2    35676     3593\n",
       "3               3    18861     2977\n",
       "4               4     9826     4653\n",
       "...           ...      ...      ...\n",
       "999995     999995    15502     4168\n",
       "999996     999996     1562     1217\n",
       "999997     999997     5838     2871\n",
       "999998     999998    35276     1541\n",
       "999999     999999    24936     6419\n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables['Listens_100MB'].retrieve_data(columns = ['listen_id', 'user_id', 'song_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtLVO3cChWkM"
   },
   "source": [
    "Analyze and report on:\n",
    "- Space efficiency compared to row storage\n",
    "  - e.g. Compare file sizes on disk: How much disk space does Parquet use vs. a row storage format like CSV?\n",
    "- Compression ratios achieved with Parquet\n",
    "  - e.g. Compare Parquet’s uncompressed encoded size (reported in its metadata) to its compressed on-disk size to compute compression ratios.\n",
    "  - You could also report the memory expansion factor: how much larger the dataset becomes when loaded into a `pd.DataFrame` compared to the compressed file size.\n",
    "- Read/write performance characteristics\n",
    "  - e.g. Read performance: How long does it take to read all columns from Parquet vs. CSV?\n",
    "  - e.g. Columnar advantage: How long does it take to read selective columns from Parquet vs. reading all columns?\n",
    "  - e.g. Write performance: How long does it take to write data to Parquet vs. CSV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7kfNisQFhWkM"
   },
   "outputs": [],
   "source": [
    "def analyze(size=\"100MB\"):\n",
    "    \"\"\"Analyze storage efficiency, compression, and read/write performance.\"\"\"\n",
    "\n",
    "    table_files = {\n",
    "        \"Songs\": f\"songs_{size}.parquet\",\n",
    "        \"Users\": f\"users_{size}.parquet\",\n",
    "        \"Listens\": f\"listens_{size}.parquet\",\n",
    "    }\n",
    "\n",
    "    report_rows = []\n",
    "\n",
    "    for table_name, parquet_file in table_files.items():\n",
    "        parquet_path = Path(parquet_file)\n",
    "\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        mem_usage_bytes = df.memory_usage(deep=True).sum() # memory usage of the dataframe\n",
    "        parquet_size_bytes = parquet_path.stat().st_size # size of the parquet file on disk\n",
    "\n",
    "        parquet_file_obj = pq.ParquetFile(parquet_path)\n",
    "        metadata = parquet_file_obj.metadata\n",
    "        uncompressed_bytes = 0\n",
    "\n",
    "        # iterate over all row groups and columns to get the total uncompressed size of the parquet file\n",
    "        for rg_idx in range(metadata.num_row_groups):\n",
    "            row_group = metadata.row_group(rg_idx)\n",
    "            for col_idx in range(row_group.num_columns):\n",
    "                column_meta = row_group.column(col_idx)\n",
    "                if column_meta.total_uncompressed_size is not None:\n",
    "                    uncompressed_bytes += column_meta.total_uncompressed_size\n",
    "\n",
    "        # calculate compression ratio and memory expansion\n",
    "        compression_ratio = (\n",
    "            uncompressed_bytes / parquet_size_bytes\n",
    "        )\n",
    "        memory_expansion = (\n",
    "            mem_usage_bytes / parquet_size_bytes\n",
    "        )\n",
    "\n",
    "        # test reading speed of parquet file vs csv, for all columns and selective columns\n",
    "        # pick 1 less than the total number of columns to test reading selective columns\n",
    "        subset_columns = list(df.columns)[0:len(df.columns)-1]\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmpdir_path = Path(tmpdir)\n",
    "\n",
    "            csv_path = tmpdir_path / f\"{parquet_path.stem}.csv\"\n",
    "            start = time.perf_counter()\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            write_csv_time = time.perf_counter() - start\n",
    "            csv_size_bytes = csv_path.stat().st_size\n",
    "\n",
    "            parquet_tmp_path = tmpdir_path / f\"{parquet_path.stem}.parquet\"\n",
    "            start = time.perf_counter()\n",
    "            df.to_parquet(parquet_tmp_path, index=False)\n",
    "            write_parquet_time = time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            _ = pd.read_parquet(parquet_path)\n",
    "            read_parquet_all = time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            _ = pd.read_csv(csv_path)\n",
    "            read_csv_all = time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            _ = pd.read_parquet(parquet_path, columns=subset_columns)\n",
    "            read_parquet_subset = time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            _ = pd.read_csv(csv_path, usecols=subset_columns)\n",
    "            read_csv_subset = time.perf_counter() - start\n",
    "\n",
    "        size_saving_pct = (\n",
    "            100.0 * (1 - parquet_size_bytes / csv_size_bytes)\n",
    "        )\n",
    "\n",
    "        # append the results to the report\n",
    "        report_rows.append(\n",
    "            {\n",
    "                \"table\": table_name,\n",
    "                \"parquet_size_mb\": parquet_size_bytes / (1024 ** 2),\n",
    "                \"csv_size_mb\": csv_size_bytes / (1024 ** 2),\n",
    "                \"size_saving_pct\": size_saving_pct,\n",
    "                \"compression_ratio\": compression_ratio,\n",
    "                \"memory_expansion\": memory_expansion,\n",
    "                \"read_parquet_all_s\": read_parquet_all,\n",
    "                \"read_csv_all_s\": read_csv_all,\n",
    "                \"read_parquet_subset_s\": read_parquet_subset,\n",
    "                \"read_csv_subset_s\": read_csv_subset,\n",
    "                \"write_parquet_s\": write_parquet_time,\n",
    "                \"write_csv_s\": write_csv_time,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "    summary = pd.DataFrame(report_rows)\n",
    "    print(\"Analysis Summary for Tables of Size \" + size + \" (sizes in MB, times in seconds):\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "Xkw4z1GMhXsH",
    "outputId": "39eebc07-7084-40c3-b28d-25ac2ecb8dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Summary for Tables of Size 100MB (sizes in MB, times in seconds):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>parquet_size_mb</th>\n",
       "      <th>csv_size_mb</th>\n",
       "      <th>size_saving_pct</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>memory_expansion</th>\n",
       "      <th>read_parquet_all_s</th>\n",
       "      <th>read_csv_all_s</th>\n",
       "      <th>read_parquet_subset_s</th>\n",
       "      <th>read_csv_subset_s</th>\n",
       "      <th>write_parquet_s</th>\n",
       "      <th>write_csv_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Songs</td>\n",
       "      <td>4.271927</td>\n",
       "      <td>9.773173</td>\n",
       "      <td>56.289255</td>\n",
       "      <td>2.415910</td>\n",
       "      <td>3.473430</td>\n",
       "      <td>0.009230</td>\n",
       "      <td>0.070341</td>\n",
       "      <td>0.007404</td>\n",
       "      <td>0.064247</td>\n",
       "      <td>0.026480</td>\n",
       "      <td>0.099192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Users</td>\n",
       "      <td>20.347857</td>\n",
       "      <td>48.579238</td>\n",
       "      <td>58.114089</td>\n",
       "      <td>2.471382</td>\n",
       "      <td>3.529207</td>\n",
       "      <td>0.040647</td>\n",
       "      <td>0.349069</td>\n",
       "      <td>0.036233</td>\n",
       "      <td>0.315945</td>\n",
       "      <td>0.098274</td>\n",
       "      <td>0.457715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Listens</td>\n",
       "      <td>79.926873</td>\n",
       "      <td>178.866784</td>\n",
       "      <td>55.314860</td>\n",
       "      <td>2.432530</td>\n",
       "      <td>8.042059</td>\n",
       "      <td>0.259581</td>\n",
       "      <td>1.787849</td>\n",
       "      <td>0.283681</td>\n",
       "      <td>1.761461</td>\n",
       "      <td>0.606222</td>\n",
       "      <td>2.841885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     table  parquet_size_mb  csv_size_mb  size_saving_pct  compression_ratio  \\\n",
       "0    Songs         4.271927     9.773173        56.289255           2.415910   \n",
       "1    Users        20.347857    48.579238        58.114089           2.471382   \n",
       "2  Listens        79.926873   178.866784        55.314860           2.432530   \n",
       "\n",
       "   memory_expansion  read_parquet_all_s  read_csv_all_s  \\\n",
       "0          3.473430            0.009230        0.070341   \n",
       "1          3.529207            0.040647        0.349069   \n",
       "2          8.042059            0.259581        1.787849   \n",
       "\n",
       "   read_parquet_subset_s  read_csv_subset_s  write_parquet_s  write_csv_s  \n",
       "0               0.007404           0.064247         0.026480     0.099192  \n",
       "1               0.036233           0.315945         0.098274     0.457715  \n",
       "2               0.283681           1.761461         0.606222     2.841885  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1890.39 MiB, increment: 507.52 MiB\n"
     ]
    }
   ],
   "source": [
    "display(analyze(size=\"100MB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIDZx-dvhXsH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Summary for Tables of Size 1GB (sizes in MB, times in seconds):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>parquet_size_mb</th>\n",
       "      <th>csv_size_mb</th>\n",
       "      <th>size_saving_pct</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>memory_expansion</th>\n",
       "      <th>read_parquet_all_s</th>\n",
       "      <th>read_csv_all_s</th>\n",
       "      <th>read_parquet_subset_s</th>\n",
       "      <th>read_csv_subset_s</th>\n",
       "      <th>write_parquet_s</th>\n",
       "      <th>write_csv_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Songs</td>\n",
       "      <td>42.661434</td>\n",
       "      <td>97.921290</td>\n",
       "      <td>56.432933</td>\n",
       "      <td>2.421422</td>\n",
       "      <td>3.480348</td>\n",
       "      <td>0.107253</td>\n",
       "      <td>0.775493</td>\n",
       "      <td>0.101759</td>\n",
       "      <td>0.723374</td>\n",
       "      <td>0.193498</td>\n",
       "      <td>0.898970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Users</td>\n",
       "      <td>203.403031</td>\n",
       "      <td>486.268065</td>\n",
       "      <td>58.170596</td>\n",
       "      <td>2.472300</td>\n",
       "      <td>3.530512</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>3.862735</td>\n",
       "      <td>0.607247</td>\n",
       "      <td>3.443182</td>\n",
       "      <td>0.965171</td>\n",
       "      <td>4.605921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Listens</td>\n",
       "      <td>834.570071</td>\n",
       "      <td>1817.281654</td>\n",
       "      <td>54.075910</td>\n",
       "      <td>2.409382</td>\n",
       "      <td>7.701888</td>\n",
       "      <td>1.894318</td>\n",
       "      <td>21.883511</td>\n",
       "      <td>3.630786</td>\n",
       "      <td>20.319147</td>\n",
       "      <td>6.950246</td>\n",
       "      <td>29.203660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     table  parquet_size_mb  csv_size_mb  size_saving_pct  compression_ratio  \\\n",
       "0    Songs        42.661434    97.921290        56.432933           2.421422   \n",
       "1    Users       203.403031   486.268065        58.170596           2.472300   \n",
       "2  Listens       834.570071  1817.281654        54.075910           2.409382   \n",
       "\n",
       "   memory_expansion  read_parquet_all_s  read_csv_all_s  \\\n",
       "0          3.480348            0.107253        0.775493   \n",
       "1          3.530512            0.703518        3.862735   \n",
       "2          7.701888            1.894318       21.883511   \n",
       "\n",
       "   read_parquet_subset_s  read_csv_subset_s  write_parquet_s  write_csv_s  \n",
       "0               0.101759           0.723374         0.193498     0.898970  \n",
       "1               0.607247           3.443182         0.965171     4.605921  \n",
       "2               3.630786          20.319147         6.950246    29.203660  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5707.55 MiB, increment: 4143.92 MiB\n"
     ]
    }
   ],
   "source": [
    "display(analyze(size=\"1GB\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8U3edewiDBa"
   },
   "source": [
    "# Section 2: Parse SQL Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_KoWHLohWkM"
   },
   "source": [
    "In this section, you should implement logic to parse the following SQL query:\n",
    "```sql\n",
    "    SELECT s.song_id, AVG(u.age) AS avg_age,\n",
    "       COUNT(DISTINCT l.user_id) AS count_distinct_users,\n",
    "    FROM Songs s\n",
    "    JOIN Listens l ON s.song_id = l.song_id\n",
    "    JOIN Users u ON l.user_id = u.user_id\n",
    "    GROUP BY s.song_id, s.title\n",
    "    ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
    "```\n",
    "\n",
    "You should manually extract the components from the provided query (i.e. you don't need to implement a general SQL parser, just handle this specific query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "TT3jWKFYhWkN"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT s.song_id, AVG(u.age) AS avg_age,\n",
    "COUNT(DISTINCT l.user_id)\n",
    "FROM Songs s\n",
    "JOIN Listens l ON s.song_id = l.song_id\n",
    "JOIN Users u ON l.user_id = u.user_id\n",
    "GROUP BY s.song_id, s.title\n",
    "ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "J1PmMhCRhv0r"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import re\n",
    "\n",
    "def parse_tables(query):\n",
    "\n",
    "    # pattern matches: \"from songs s\" or \"join listens l\"\n",
    "    pattern = r\"(from|join)\\s+([a-z_]+)\\s+([a-z])\"\n",
    "\n",
    "    matches = re.findall(pattern, query)\n",
    "\n",
    "    tables = {}\n",
    "    for _, table_name, alias in matches:\n",
    "        tables[alias] = table_name\n",
    "\n",
    "    return tables\n",
    "\n",
    "def parse_joins(query):\n",
    "\n",
    "    # 1) Get the base table from the FROM clause\n",
    "    base_match = re.search(r\"from\\s+([a-z_]+)\\s+([a-z])\", query)\n",
    "    if not base_match:\n",
    "        raise ValueError(\"Could not find FROM clause\")\n",
    "\n",
    "    base_table_name = base_match.group(1)\n",
    "    base_alias = base_match.group(2)\n",
    "    base_table = (base_alias, base_table_name)\n",
    "\n",
    "    # 2) Get each JOIN clause, in order\n",
    "    # pattern matches:\n",
    "    #   join listens l on s.song_id = l.song_id\n",
    "    join_pattern = (\n",
    "        r\"join\\s+([a-z_]+)\\s+([a-z])\\s+on\\s+\"\n",
    "        r\"([a-z])\\.([a-z_]+)\\s*=\\s*([a-z])\\.([a-z_]+)\"\n",
    "    )\n",
    "\n",
    "    joins = []\n",
    "    for m in re.finditer(join_pattern, query):\n",
    "        joined_table_name = m.group(1)\n",
    "        joined_alias = m.group(2)\n",
    "        left_alias = m.group(3)\n",
    "        left_col = m.group(4)\n",
    "        right_alias = m.group(5)\n",
    "        right_col = m.group(6)\n",
    "\n",
    "        joins.append(\n",
    "            {\n",
    "                \"joined_table_alias\": joined_alias,\n",
    "                \"joined_table_name\": joined_table_name,\n",
    "                \"left_alias\": left_alias,\n",
    "                \"left_column\": left_col,\n",
    "                \"right_alias\": right_alias,\n",
    "                \"right_column\": right_col,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\"base_table\" : base_table, \"Joins\" : joins}\n",
    "\n",
    "\n",
    "def parse_group_by(query):\n",
    "    \"\"\"\n",
    "    Return GROUP BY columns as a list of (alias, column) tuples.\n",
    "    Example: [('s', 'song_id'), ('s', 'title')]\n",
    "    \"\"\"\n",
    "    q = query.lower()\n",
    "\n",
    "    # Capture whatever is between GROUP BY and ORDER BY/semicolon/end\n",
    "    match = re.search(r\"group\\s+by\\s+(.+?)(order\\s+by|;|$)\", q, re.DOTALL)\n",
    "    if not match:\n",
    "        return []\n",
    "\n",
    "    groupby_text = match.group(1).strip()\n",
    "\n",
    "    columns = []\n",
    "    for col in groupby_text.split(\",\"):\n",
    "        col = col.strip()\n",
    "\n",
    "        # Expect pattern: alias.column\n",
    "        alias, column = col.split(\".\")\n",
    "        columns.append((alias, column))\n",
    "\n",
    "    return columns\n",
    "\n",
    "def parse_select_and_aggregations(query):\n",
    "    \"\"\"\n",
    "    Build:\n",
    "      aggregations: {agg_key: {...}}\n",
    "      select: list of items that may refer to agg_key\n",
    "    \"\"\"\n",
    "    q = query.lower()\n",
    "\n",
    "    m = re.search(r\"select\\s+(.+?)\\s+from\", q, re.DOTALL)\n",
    "    if not m:\n",
    "        return [], {}\n",
    "\n",
    "    select_text = m.group(1).strip()\n",
    "    raw_items = [item.strip() for item in select_text.split(\",\") if item.strip()]\n",
    "\n",
    "    select_list = []\n",
    "    aggregations = {}\n",
    "    agg_id = 1\n",
    "\n",
    "    for idx, item in enumerate(raw_items, start=1):\n",
    "        # AVG(...)\n",
    "        if item.startswith(\"avg(\"):\n",
    "            m_avg = re.match(\n",
    "                r\"avg\\(\\s*([a-z])\\.([a-z_]+)\\s*\\)(\\s+as\\s+([a-z_]+))?\",\n",
    "                item\n",
    "            )\n",
    "            if not m_avg:\n",
    "                raise ValueError(f\"Could not parse AVG aggregation: {item}\")\n",
    "            alias_letter = m_avg.group(1)\n",
    "            col_name = m_avg.group(2)\n",
    "            out_alias = m_avg.group(4) if m_avg.group(4) else None\n",
    "\n",
    "            aggregations[agg_id] = {\n",
    "                \"func\": \"avg\",\n",
    "                \"source\": (alias_letter, col_name),\n",
    "                \"distinct\": False,\n",
    "                \"output_name\": out_alias,\n",
    "            }\n",
    "\n",
    "            select_list.append(\n",
    "                {\n",
    "                    \"kind\": \"aggregation\",\n",
    "                    \"agg_key\": agg_id,\n",
    "                    \"alias\": out_alias,\n",
    "\n",
    "                }\n",
    "            )\n",
    "            agg_id += 1\n",
    "\n",
    "        # COUNT(DISTINCT ...)\n",
    "        elif item.startswith(\"count(\"):\n",
    "            m_cnt = re.match(\n",
    "                r\"count\\(\\s*distinct\\s+([a-z])\\.([a-z_]+)\\s*\\)(\\s+as\\s+([a-z_]+))?\",\n",
    "                item\n",
    "            )\n",
    "            if not m_cnt:\n",
    "                raise ValueError(f\"Could not parse COUNT aggregation: {item}\")\n",
    "            alias_letter = m_cnt.group(1)\n",
    "            col_name = m_cnt.group(2)\n",
    "            out_alias = m_cnt.group(4) if m_cnt.group(4) else None\n",
    "\n",
    "            aggregations[agg_id] = {\n",
    "                \"func\": \"count\",\n",
    "                \"source\": (alias_letter, col_name),\n",
    "                \"distinct\": True,\n",
    "                \"output_name\": out_alias,\n",
    "            }\n",
    "\n",
    "            select_list.append(\n",
    "                {\n",
    "                    \"kind\": \"aggregation\",\n",
    "                    \"agg_key\": agg_id,\n",
    "                    \"alias\": out_alias,\n",
    "                }\n",
    "            )\n",
    "            agg_id += 1\n",
    "\n",
    "        # Plain column: alias.column\n",
    "        else:\n",
    "            alias_letter, col_name = item.split(\".\")\n",
    "            select_list.append(\n",
    "                {\n",
    "                    \"kind\": \"column\",\n",
    "                    \"source\": (alias_letter, col_name),\n",
    "                    \"alias\": None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return select_list, aggregations\n",
    "\n",
    "\n",
    "def parse_order_by(query, aggregations):\n",
    "    \"\"\"\n",
    "    Build order_by list where entries can refer to aggregations via agg_key.\n",
    "    \"\"\"\n",
    "    q = query.lower()\n",
    "\n",
    "    m = re.search(r\"order\\s+by\\s+(.+?)(;|$)\", q, re.DOTALL)\n",
    "    if not m:\n",
    "        return []\n",
    "\n",
    "    order_text = m.group(1).strip()\n",
    "    raw_items = [item.strip() for item in order_text.split(\",\") if item.strip()]\n",
    "\n",
    "    order_by = []\n",
    "\n",
    "    for item in raw_items:\n",
    "        direction = \"asc\"\n",
    "        expr = item\n",
    "\n",
    "        if expr.endswith(\" desc\"):\n",
    "            direction = \"desc\"\n",
    "            expr = expr[:-5].strip()\n",
    "        elif expr.endswith(\" asc\"):\n",
    "            direction = \"asc\"\n",
    "            expr = expr[:-4].strip()\n",
    "\n",
    "        # COUNT(DISTINCT ...) → match an aggregation\n",
    "        if expr.startswith(\"count(\"):\n",
    "            m_cnt = re.match(\n",
    "                r\"count\\(\\s*distinct\\s+([a-z])\\.([a-z_]+)\\s*\\)\",\n",
    "                expr\n",
    "            )\n",
    "            if not m_cnt:\n",
    "                raise ValueError(f\"Could not parse ORDER BY aggregation: {expr}\")\n",
    "            src = (m_cnt.group(1), m_cnt.group(2))\n",
    "\n",
    "            agg_key = None\n",
    "            for k, agg in aggregations.items():\n",
    "                if (\n",
    "                    agg[\"func\"] == \"count\"\n",
    "                    and agg[\"distinct\"]\n",
    "                    and agg[\"source\"] == src\n",
    "                ):\n",
    "                    agg_key = k\n",
    "                    break\n",
    "\n",
    "            if agg_key is None:\n",
    "                raise ValueError(f\"No matching aggregation found for ORDER BY expr: {expr}\")\n",
    "\n",
    "            order_by.append(\n",
    "                {\n",
    "                    \"kind\": \"aggregation\",\n",
    "                    \"agg_key\": agg_key,\n",
    "                    \"direction\": direction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # assume plain column: alias.column\n",
    "            alias_letter, col_name = expr.split(\".\")\n",
    "            order_by.append(\n",
    "                {\n",
    "                    \"kind\": \"column\",\n",
    "                    \"source\": (alias_letter, col_name),\n",
    "                    \"direction\": direction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return order_by\n",
    "\n",
    "def parse_sql(query):\n",
    "    \"\"\"\n",
    "    YOUR TASK: Extract tables, joins, and aggregations\n",
    "    \"\"\"\n",
    "    # Parse SQL string to identify:\n",
    "    # - Tables involved\n",
    "    # - Join conditions\n",
    "    # - GROUP BY columns\n",
    "    # - Aggregation functions\n",
    "    # Your implementation here\n",
    "    query = query.lower()\n",
    "    output = {}\n",
    "\n",
    "    output[\"tables\"] = parse_tables(query)\n",
    "    output[\"joins\"] = parse_joins(query)\n",
    "    output[\"GroupBy\"] = parse_group_by(query)\n",
    "    output[\"select\"], output[\"aggregations\"] = parse_select_and_aggregations(query)\n",
    "    output[\"orderBy\"] = parse_order_by(query, output[\"aggregations\"])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8mb80aeTAxM",
    "outputId": "02681db1-41c7-4339-907a-7c377b821792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tables: {'s': 'songs', 'l': 'listens', 'u': 'users'}\n",
      "joins: {'base_table': ('s', 'songs'), 'Joins': [{'joined_table_alias': 'l', 'joined_table_name': 'listens', 'left_alias': 's', 'left_column': 'song_id', 'right_alias': 'l', 'right_column': 'song_id'}, {'joined_table_alias': 'u', 'joined_table_name': 'users', 'left_alias': 'l', 'left_column': 'user_id', 'right_alias': 'u', 'right_column': 'user_id'}]}\n",
      "GroupBy: [('s', 'song_id'), ('s', 'title')]\n",
      "select: [{'kind': 'column', 'source': ('s', 'song_id'), 'alias': None}, {'kind': 'aggregation', 'agg_key': 1, 'alias': 'avg_age'}, {'kind': 'aggregation', 'agg_key': 2, 'alias': None}]\n",
      "aggregations: {1: {'func': 'avg', 'source': ('u', 'age'), 'distinct': False, 'output_name': 'avg_age'}, 2: {'func': 'count', 'source': ('l', 'user_id'), 'distinct': True, 'output_name': None}}\n",
      "orderBy: [{'kind': 'aggregation', 'agg_key': 2, 'direction': 'desc'}, {'kind': 'column', 'source': ('s', 'song_id'), 'direction': 'asc'}]\n"
     ]
    }
   ],
   "source": [
    "output = parse_sql(query)\n",
    "for key, value in output.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "071jzSZqkGyC"
   },
   "source": [
    "# Section 3: Implement Join Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14LszEqZhWkN"
   },
   "source": [
    "In this section, you will implement the execution operators (*how* to join) and aggregation after joins.\n",
    "\n",
    "**Reminder:** If you use temporary files or folders, you should clean them up either as part of your join logic, or after each run. Otherwise you might run into correctness issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1AVO_NRnkHq1"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def HASHVALUE(value, B):\n",
    "    if isinstance(value, int):\n",
    "        return hash(value) % B\n",
    "    sha256 = hashlib.sha256()\n",
    "    sha256.update(str(value).encode(\"utf-8\"))\n",
    "    return int(sha256.hexdigest(), 16) % B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_partition(\n",
    "    table: ColumnarDbFile,\n",
    "    hash_keys: List[str],\n",
    "    num_partitions: int,\n",
    "    parquet_batch_size: int,\n",
    "    hash_value_fn: Callable[[object, int], int],\n",
    "    make_partition_path_fn: Callable[[int], str],\n",
    "    columns: Optional[List[str]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hash-partition `table` into `num_partitions` Parquet files.\n",
    "\n",
    "    - `hash_keys` is a list of column names (one or more).\n",
    "    - If len(hash_keys) > 1, we build a temporary concatenated column `_hash_key`\n",
    "      and hash on that.\n",
    "    - `hash_value_fn(key, num_partitions)` returns an int in [0, num_partitions).\n",
    "    - `columns` are the columns to write into each partition.\n",
    "      All `hash_keys` are automatically included in `columns`.\n",
    "    \"\"\"\n",
    "    is_multi_col = len(hash_keys) > 1\n",
    "    hash_col_name = \"_hash_key\" if is_multi_col else hash_keys[0]\n",
    "\n",
    "    # Normalize and ensure hash_keys are included in the columns we read & write\n",
    "    if columns:\n",
    "        for col in hash_keys:\n",
    "            if col not in columns:\n",
    "                columns.append(col)\n",
    "\n",
    "    writers: Dict[int, pq.ParquetWriter] = {}\n",
    "    for batch_df in table.iter_pages(columns=columns, rows_per_batch=parquet_batch_size):\n",
    "        # If multiple hash columns, build a temporary concatenated key column\n",
    "        if is_multi_col:\n",
    "            batch_df[hash_col_name] = (\n",
    "                batch_df[hash_keys]\n",
    "                .astype(str)\n",
    "                .agg(\"|\".join, axis=1)\n",
    "            )\n",
    "\n",
    "        # Compute partition id\n",
    "        batch_df[\"_part\"] = batch_df[hash_col_name].apply(\n",
    "            lambda x: hash_value_fn(x, num_partitions)\n",
    "        )\n",
    "\n",
    "        if columns:\n",
    "            batch_df = batch_df[columns + [\"_part\"]]\n",
    "        if is_multi_col:\n",
    "            batch_df = batch_df.drop(columns=hash_col_name)\n",
    "\n",
    "        # Group rows by partition and write each group\n",
    "        for part_id, part_df in batch_df.groupby(\"_part\"):\n",
    "            part_df = part_df.drop(columns=[\"_part\"])\n",
    "\n",
    "            part_table = pa.Table.from_pandas(part_df, preserve_index=False)\n",
    "\n",
    "            writer = writers.get(part_id)\n",
    "            if writer is None:\n",
    "                part_path = make_partition_path_fn(part_id)\n",
    "                writer = pq.ParquetWriter(part_path, part_table.schema)\n",
    "                writers[part_id] = writer\n",
    "\n",
    "            writer.write_table(part_table)\n",
    "\n",
    "    for w in writers.values():\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q46rgYihWkN"
   },
   "source": [
    "Implement `HashPartitionJoin`:\n",
    "1. Hash partition both tables\n",
    "2. Build hash table from smaller partition\n",
    "3. Probe with larger partition\n",
    "4. Return joined results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastHashPartitionJoin:\n",
    "    def __init__(self, num_partitions=4, parquet_batch_size=100_000, use_streaming=False, time_it=True):\n",
    "        self.num_partitions = num_partitions\n",
    "        self.parquet_batch_size = parquet_batch_size\n",
    "        self.use_streaming = use_streaming\n",
    "        self.time_it = time_it\n",
    "    def join(self, table1: ColumnarDbFile, table2: ColumnarDbFile, join_key1, join_key2,\n",
    "             temp_dir='temp', columns_table1=None, columns_table2=None):\n",
    "        \"\"\"\n",
    "        Perform an optimized hash partition join between two ColumnarDbFile instances.\n",
    "\n",
    "        Speed ups:\n",
    "        - load smaller table into memory and build hash map with pandas groupby, larger table is probed in batches\n",
    "        - vectorized join using numpy operations: see _vectorized_join method for more details\n",
    "        \"\"\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        # Partition both tables\n",
    "        self._hash_partition(table1, join_key1, temp_dir, 'left', columns_table1)\n",
    "        self._hash_partition(table2, join_key2, temp_dir, 'right', columns_table2)\n",
    "\n",
    "        output = ColumnarDbFile(f\"hpj_{table1.table_name}_{table2.table_name}\")\n",
    "        \n",
    "        # Clean up any existing files in the output directory\n",
    "        if os.path.exists(output.base_file_name):\n",
    "            for file_path in output.get_all_parquet_paths():\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        if self.use_streaming:\n",
    "            output.start_stream()\n",
    "\n",
    "        for part_id in range(self.num_partitions):\n",
    "            left_path = self._make_partition_path(temp_dir, \"left\", part_id)\n",
    "            right_path = self._make_partition_path(temp_dir, \"right\", part_id)\n",
    "\n",
    "            if not (os.path.exists(left_path) and os.path.exists(right_path)):\n",
    "                continue\n",
    "            \n",
    "            # Process this partition with batched reading\n",
    "            self._process_partition(\n",
    "                left_path, right_path, join_key1, join_key2, output\n",
    "            )\n",
    "\n",
    "        if self.use_streaming:\n",
    "            output.stop_stream()\n",
    "        \n",
    "        shutil.rmtree(temp_dir)\n",
    "        return output\n",
    "\n",
    "    def _process_partition(self, left_path, right_path, join_key1, join_key2, output):\n",
    "        \"\"\"\n",
    "        Process a partition an individual partition from left and right\n",
    "        Determine which side is smaller and build hash map from that side\n",
    "        Probe with larger side\n",
    "        \"\"\"\n",
    "        # Get metadata to determine which side is smaller\n",
    "        left_size = pq.ParquetFile(left_path).metadata.num_rows\n",
    "        right_size = pq.ParquetFile(right_path).metadata.num_rows\n",
    "        \n",
    "        if left_size <= right_size:\n",
    "            # Build hash map from left, probe with right\n",
    "            self._build_and_probe(left_path, right_path, join_key1, join_key2, \n",
    "                                  output, left_is_build=True)\n",
    "        else:\n",
    "            # Build hash map from right, probe with left\n",
    "            self._build_and_probe(right_path, left_path, join_key2, join_key1, \n",
    "                                  output, left_is_build=False)\n",
    "\n",
    "    def _build_and_probe(self, build_path, probe_path, build_key, probe_key, \n",
    "                         output, left_is_build):\n",
    "        \"\"\"\n",
    "        Build hash map from build side and probe with probe side using batched reading.\n",
    "        \"\"\"\n",
    "        # Build hash map from the smaller side (build side)\n",
    "        hash_map = self._build_hash_map(build_path, build_key)\n",
    "        \n",
    "        # Probe with the larger side in batches\n",
    "        probe_file = pq.ParquetFile(probe_path)\n",
    "        build_df = pq.read_table(build_path).to_pandas()\n",
    "        \n",
    "        for probe_batch in probe_file.iter_batches(batch_size=self.parquet_batch_size):\n",
    "            probe_df = probe_batch.to_pandas()\n",
    "            \n",
    "            # Vectorized join using numpy operations\n",
    "            joined_df = self._vectorized_join(\n",
    "                build_df, probe_df, hash_map, build_key, probe_key, left_is_build\n",
    "            )\n",
    "            \n",
    "            if not joined_df.empty:\n",
    "                output.append_data(joined_df)\n",
    "            \n",
    "            # Explicit memory cleanup\n",
    "            del probe_df\n",
    "            del joined_df\n",
    "            gc.collect()\n",
    "        \n",
    "        del build_df\n",
    "        del hash_map\n",
    "        gc.collect()\n",
    "\n",
    "    def _build_hash_map(self, file_path, key_column):\n",
    "        \"\"\"\n",
    "        Build an optimized hash map using numpy arrays for better performance.\n",
    "        Returns a dictionary mapping keys to numpy arrays of indices.\n",
    "        \"\"\"\n",
    "        df = pq.read_table(file_path).to_pandas()\n",
    "        \n",
    "        # Group indices by key using pandas groupby (much faster than manual loop)\n",
    "        grouped = df.reset_index().groupby(key_column)['index'].apply(np.array).to_dict()\n",
    "        \n",
    "        return grouped\n",
    "\n",
    "    def _vectorized_join(self, build_df, probe_df, hash_map, probe_key, left_is_build):\n",
    "        \"\"\"\n",
    "        Primary optimization using a vectorized join with vectorized join:\n",
    "        1. Get probe keys and find indeces of matches in hash map  \n",
    "        2. Establish a parrallel index for build and probe tables\n",
    "        3. Build result from parallel indices using advanced pandas indexing\n",
    "        \"\"\"\n",
    "        probe_keys = probe_df[probe_key].values\n",
    "        \n",
    "        build_indices = []\n",
    "        probe_indices = []\n",
    "        \n",
    "        # Build index for build and probe tables\n",
    "        for probe_idx, key in enumerate(probe_keys):\n",
    "            if key in hash_map:\n",
    "                build_idxs = hash_map[key]\n",
    "                build_indices.extend(build_idxs)\n",
    "                probe_indices.extend([probe_idx] * len(build_idxs))\n",
    "        \n",
    "        if not build_indices:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        build_indices = np.array(build_indices)\n",
    "        probe_indices = np.array(probe_indices)\n",
    "        \n",
    "        # Build result using advanced indexing\n",
    "        # advanced works as follows here:\n",
    "        # build_df.iloc[build_indices] -> get rows from build_df where index is in build_indices\n",
    "        # probe_df.iloc[probe_indices] -> get rows from probe_df where index is in probe_indices\n",
    "        # these lists are parallel, meaning that the row in position i in build_indices is joined \n",
    "        # with the row in position i in probe_indices\n",
    "        if left_is_build:\n",
    "            left_result = build_df.iloc[build_indices].reset_index(drop=True)\n",
    "            right_result = probe_df.iloc[probe_indices].reset_index(drop=True)\n",
    "        else:\n",
    "            left_result = probe_df.iloc[probe_indices].reset_index(drop=True)\n",
    "            right_result = build_df.iloc[build_indices].reset_index(drop=True)\n",
    "        \n",
    "        # Drop duplicate columns from right side (keeping left)\n",
    "        common_columns = set(left_result.columns) & set(right_result.columns)\n",
    "        if common_columns:\n",
    "            right_result = right_result.drop(columns=list(common_columns))\n",
    "\n",
    "        result = pd.concat([left_result, right_result], axis=1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _make_partition_path(self, output_dir, side, part_id):\n",
    "        return f\"{output_dir}/{side}_part{part_id}.parquet\"\n",
    "\n",
    "    def _hash_partition(self, table: ColumnarDbFile, join_key, output_dir, side, columns=None):\n",
    "        make_partition_path_fn = partial(self._make_partition_path, output_dir, side)\n",
    "        hash_partition(table, [join_key], self.num_partitions, self.parquet_batch_size,\n",
    "                       HASHVALUE, make_partition_path_fn, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see ed: https://edstem.org/us/courses/87394/discussion/7151010 for discussion on this implementation\n",
    "class SlowHashPartitionJoin:\n",
    "    def __init__(self, num_partitions=4, parquet_batch_size=100_000, use_streaming=False):\n",
    "        self.num_partitions = num_partitions\n",
    "        self.parquet_batch_size = parquet_batch_size\n",
    "        self.use_streaming = use_streaming\n",
    "\n",
    "    def join(self, table1: ColumnarDbFile, table2: ColumnarDbFile, join_key1, join_key2,\n",
    "             temp_dir='temp', columns_table1=None, columns_table2=None):\n",
    "        \"\"\"\n",
    "        Perform a hash partition join between two ColumnarDbFile instances.\n",
    "\n",
    "        Parameters:\n",
    "        - table1: Left table (ColumnarDbFile)\n",
    "        - table2: Right table (ColumnarDbFile)\n",
    "        - join_key1: Join key from table1\n",
    "        - join_key2: Join key from table2\n",
    "        - temp_dir: Directory to store temporary files\n",
    "        - columns_table1: List of columns to select from table1\n",
    "        - columns_table2: List of columns to select from table2\n",
    "\n",
    "        Returns:\n",
    "        - join_result_table: ColumnarDbFile instance containing the join results\n",
    "        \"\"\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        # Partition both tables\n",
    "        self._hash_partition(table1, join_key1, temp_dir, 'left', columns_table1)\n",
    "        self._hash_partition(table2, join_key2, temp_dir, 'right', columns_table2)\n",
    "\n",
    "        output = ColumnarDbFile(f\"hpj_{table1.table_name}_{table2.table_name}\")\n",
    "        \n",
    "        # Clean up any existing files in the output directory to avoid reading old files\n",
    "        if os.path.exists(output.base_file_name):\n",
    "            for file_path in output.get_all_parquet_paths():\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        if self.use_streaming:\n",
    "            output.start_stream()\n",
    "\n",
    "        for part_id in range(self.num_partitions):\n",
    "            left_path = self._make_partition_path(temp_dir, \"left\", part_id)\n",
    "            right_path = self._make_partition_path(temp_dir, \"right\", part_id)\n",
    "\n",
    "            if not (os.path.exists(left_path) and os.path.exists(right_path)):\n",
    "                continue\n",
    "            \n",
    "            # BOTTLENECK: read_table is slow, so we should paginate reads here, but as a work around we can increase num_partitions\n",
    "            left_df = pq.read_table(left_path).to_pandas()\n",
    "            right_df = pq.read_table(right_path).to_pandas()\n",
    "\n",
    "            # Decide which side is smaller for this partition\n",
    "            if len(left_df) <= len(right_df):\n",
    "                small_df, big_df = left_df, right_df\n",
    "                small_is_left = True\n",
    "            else:\n",
    "                small_df, big_df = right_df, left_df\n",
    "                small_is_left = False\n",
    "\n",
    "            # Build hash map from the smaller side, storing **indices** only\n",
    "            hash_map = {}\n",
    "            if small_is_left:\n",
    "                # small_df is left: hash on join_key1\n",
    "                for i in range(len(small_df)):\n",
    "                    key = small_df.iloc[i][join_key1]\n",
    "                    if key not in hash_map:\n",
    "                        hash_map[key] = []\n",
    "                    hash_map[key].append(i)\n",
    "            else:\n",
    "                # small_df is right: hash on join_key2\n",
    "                for i in range(len(small_df)):\n",
    "                    key = small_df.iloc[i][join_key2]\n",
    "                    if key not in hash_map:\n",
    "                        hash_map[key] = []\n",
    "                    hash_map[key].append(i)\n",
    "\n",
    "            # Nested-loop join probing with the larger side\n",
    "            joined_rows = []\n",
    "            if small_is_left:\n",
    "                # small = left, big = right\n",
    "                for r_i in range(len(big_df)):\n",
    "                    r_row = big_df.iloc[r_i]\n",
    "                    key = r_row[join_key2]\n",
    "                    if key not in hash_map:\n",
    "                        continue\n",
    "                    for l_idx in hash_map[key]:\n",
    "                        l_row = small_df.iloc[l_idx]\n",
    "                        combined = {}\n",
    "                        # copy all left columns\n",
    "                        for col in left_df.columns:\n",
    "                            combined[col] = l_row[col]\n",
    "                        # copy all right columns\n",
    "                        for col in right_df.columns:\n",
    "                            combined[col] = r_row[col]\n",
    "                        joined_rows.append(combined)\n",
    "            else:\n",
    "                # small = right, big = left\n",
    "                for l_i in range(len(big_df)):\n",
    "                    l_row = big_df.iloc[l_i]\n",
    "                    key = l_row[join_key1]\n",
    "                    if key not in hash_map:\n",
    "                        continue\n",
    "                    for r_idx in hash_map[key]:\n",
    "                        r_row = small_df.iloc[r_idx]\n",
    "                        combined = {}\n",
    "                        # copy all left columns\n",
    "                        for col in left_df.columns:\n",
    "                            combined[col] = l_row[col]\n",
    "                        # copy all right columns\n",
    "                        for col in right_df.columns:\n",
    "                            combined[col] = r_row[col]\n",
    "                        joined_rows.append(combined)\n",
    "\n",
    "            if not joined_rows:\n",
    "                continue\n",
    "            joined_df = pd.DataFrame(joined_rows)\n",
    "            joined_table = pa.Table.from_pandas(joined_df, preserve_index=False)\n",
    "\n",
    "            output.append_data(joined_df)\n",
    "\n",
    "        if self.use_streaming:\n",
    "            output.stop_stream()\n",
    "        \n",
    "        shutil.rmtree(temp_dir)\n",
    "        return output\n",
    "\n",
    "    def _make_partition_path(self, output_dir, side, part_id):\n",
    "        return f\"{output_dir}/{side}_part{part_id}.parquet\"\n",
    "\n",
    "    def _hash_partition(self, table: ColumnarDbFile, join_key, output_dir, side, columns=None):\n",
    "        make_partition_path_fn = partial(self._make_partition_path, output_dir, side)\n",
    "        hash_partition(table, [join_key], self.num_partitions, self.parquet_batch_size,\n",
    "                       HASHVALUE, make_partition_path_fn, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2734.20 MiB, increment: 119.58 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "SIZE = \"1GB\" #[\"100MB\", \"1GB\", \"10GB\"]\n",
    "SAMPLE = 100\n",
    "USE_STREAMING = True\n",
    "\n",
    "songs_table = tables[f'Songs_{SIZE}']\n",
    "listens_table = tables[f'Listens_{SIZE}']\n",
    "\n",
    "# Select specific columns from each table\n",
    "songs_cols = ['song_id', 'title']\n",
    "listens_cols = ['listen_id', 'song_id', 'user_id']\n",
    "\n",
    "# Create HashPartitionJoin instance\n",
    "hpj1 = FastHashPartitionJoin(\n",
    "    num_partitions=4, \n",
    "    parquet_batch_size=1000000,\n",
    "    use_streaming=USE_STREAMING  \n",
    ")\n",
    "\n",
    "# Perform the join\n",
    "result_songs_listens = hpj1.join(\n",
    "    table1=songs_table,           \n",
    "    table2=listens_table,         \n",
    "    join_key1='song_id',          \n",
    "    join_key2='song_id',          \n",
    "    temp_dir='temp_songs_listens',\n",
    "    columns_table1=songs_cols,    \n",
    "    columns_table2=listens_cols   \n",
    ")\n",
    "\n",
    "result_df = result_songs_listens.retrieve_data(sample=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Comprehensive Hash Partition Join Test\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Test: Songs JOIN Listens \n",
      "======================================================================\n",
      "\n",
      "HPJ result shape: (1000000, 4)\n",
      "pd.merge result shape: (1000000, 4)\n",
      "Row counts match!\n",
      "Columns match!\n",
      "Unique song_ids match!\n",
      "\n",
      "Performing full data value comparison...\n",
      "All data values match exactly!\n",
      "Verified 1000000 rows × 4 columns = 4000000 values\n",
      "Duplicate row counts match (0 duplicates)\n",
      "\n",
      " Test PASSED\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE TEST SUMMARY\n",
      "======================================================================\n",
      "✓ ALL TESTS PASSED: Hash Partition Join is CORRECT!\n",
      "  - Row counts match\n",
      "  - Column structure matches\n",
      "  - Unique keys match\n",
      "  - ALL DATA VALUES match exactly\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_STREAMING = True\n",
    "SIZE = \"100MB\"\n",
    "# Optional: Verify your implementation against pd.merge\n",
    "def test_hash_partition_join_comprehensive():\n",
    "    \"\"\"\n",
    "    Comprehensive test that validates both structure AND actual data values.\n",
    "    This ensures the HPJ implementation is truly correct.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"Comprehensive Hash Partition Join Test\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_tests_passed = True\n",
    "    \n",
    "    # Test: Songs JOIN Listens - FULL DATA VALIDATION\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Test: Songs JOIN Listens \")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    songs_table = tables[f'Songs_{SIZE}']\n",
    "    listens_table = tables[f'Listens_{SIZE}']\n",
    "    \n",
    "    songs_cols = ['song_id', 'title']\n",
    "    listens_cols = ['listen_id', 'song_id', 'user_id']\n",
    "    \n",
    "    # Perform joins\n",
    "    hpj1 = FastHashPartitionJoin(num_partitions=4, parquet_batch_size=100_000, use_streaming=USE_STREAMING)\n",
    "    result_table1 = hpj1.join(\n",
    "        songs_table, listens_table,\n",
    "        join_key1='song_id', join_key2='song_id',\n",
    "        temp_dir='temp_test_songs_listens_comp',\n",
    "        columns_table1=songs_cols,\n",
    "        columns_table2=listens_cols\n",
    "    )\n",
    "    \n",
    "    hpj_result1 = result_table1.retrieve_data()\n",
    "    \n",
    "    # Get pd.merge result\n",
    "    songs_df = songs_table.retrieve_data(columns=songs_cols)\n",
    "    listens_df = listens_table.retrieve_data(columns=listens_cols)\n",
    "    pd_result1 = pd.merge(songs_df, listens_df, on='song_id', how='inner')\n",
    "    \n",
    "    print(f\"\\nHPJ result shape: {hpj_result1.shape}\")\n",
    "    print(f\"pd.merge result shape: {pd_result1.shape}\")\n",
    "    \n",
    "    test1_passed = True\n",
    "    \n",
    "    # 1. Row count check\n",
    "    if len(hpj_result1) != len(pd_result1):\n",
    "        print(f\"Row count mismatch -- HPJ: {len(hpj_result1)}, pd.merge: {len(pd_result1)}\")\n",
    "        test1_passed = False\n",
    "        all_tests_passed = False\n",
    "    else:\n",
    "        print(\"Row counts match!\")\n",
    "    \n",
    "    # 2. Column check\n",
    "    hpj_cols = set(hpj_result1.columns)\n",
    "    pd_cols = set(pd_result1.columns)\n",
    "    if hpj_cols != pd_cols:\n",
    "        print(f\"Column mismatch -- HPJ: {hpj_cols}, pd.merge: {pd_cols}\")\n",
    "        test1_passed = False\n",
    "        all_tests_passed = False\n",
    "    else:\n",
    "        print(\"Columns match!\")\n",
    "    \n",
    "    if test1_passed:\n",
    "        # 3. Sort both results for comparison\n",
    "        sort_cols = ['song_id', 'listen_id'] if 'listen_id' in hpj_result1.columns else ['song_id']\n",
    "        hpj_sorted = hpj_result1.sort_values(sort_cols).reset_index(drop=True)\n",
    "        pd_sorted = pd_result1.sort_values(sort_cols).reset_index(drop=True)\n",
    "        \n",
    "        # 4. Check unique keys\n",
    "        hpj_song_ids = set(hpj_result1['song_id'].unique())\n",
    "        pd_song_ids = set(pd_result1['song_id'].unique())\n",
    "        if hpj_song_ids != pd_song_ids:\n",
    "            print(f\"Unique song_ids differ!\")\n",
    "            test1_passed = False\n",
    "            all_tests_passed = False\n",
    "        else:\n",
    "            print(\"Unique song_ids match!\")\n",
    "        \n",
    "        # 5. FULL DATA VALUE COMPARISON - This is the critical check!\n",
    "        print(\"\\nPerforming full data value comparison...\")\n",
    "        data_matches = True\n",
    "        \n",
    "        # Compare each column\n",
    "        for col in sorted(hpj_cols):\n",
    "            hpj_col_data = hpj_sorted[col].values\n",
    "            pd_col_data = pd_sorted[col].values\n",
    "            \n",
    "            # Use np.array_equal for exact comparison\n",
    "            if not np.array_equal(hpj_col_data, pd_col_data):\n",
    "                print(f\"Column '{col}' data mismatch\")\n",
    "                \n",
    "                # Find first mismatch\n",
    "                mismatch_idx = np.where(hpj_col_data != pd_col_data)[0]\n",
    "                if len(mismatch_idx) > 0:\n",
    "                    idx = mismatch_idx[0]\n",
    "                    print(f\"  First mismatch at row {idx}:\")\n",
    "                    print(f\"    HPJ: {hpj_col_data[idx]}\")\n",
    "                    print(f\"    pd.merge: {pd_col_data[idx]}\")\n",
    "                    print(f\"  Total mismatches: {len(mismatch_idx)}\")\n",
    "                \n",
    "                data_matches = False\n",
    "                break\n",
    "        \n",
    "        if data_matches:\n",
    "            print(\"All data values match exactly!\")\n",
    "            print(f\"Verified {len(hpj_sorted)} rows × {len(hpj_cols)} columns = {len(hpj_sorted) * len(hpj_cols)} values\")\n",
    "        else:\n",
    "            print(\"✗ Data values do NOT match!\")\n",
    "            test1_passed = False\n",
    "            all_tests_passed = False\n",
    "        \n",
    "        # 6. Check for duplicate rows (should be same in both)\n",
    "        hpj_duplicates = hpj_sorted.duplicated().sum()\n",
    "        pd_duplicates = pd_sorted.duplicated().sum()\n",
    "        if hpj_duplicates != pd_duplicates:\n",
    "            print(f\"Duplicate row counts differ (HPJ: {hpj_duplicates}, pd.merge: {pd_duplicates})\")\n",
    "        else:\n",
    "            print(f\"Duplicate row counts match ({hpj_duplicates} duplicates)\")\n",
    "    \n",
    "    if test1_passed:\n",
    "        print(\"\\n Test PASSED\")\n",
    "    else:\n",
    "        print(\"\\n Test FAILED!\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE TEST SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    if all_tests_passed:\n",
    "        print(\"✓ ALL TESTS PASSED: Hash Partition Join is CORRECT!\")\n",
    "        print(\"  - Row counts match\")\n",
    "        print(\"  - Column structure matches\")\n",
    "        print(\"  - Unique keys match\")\n",
    "        print(\"  - ALL DATA VALUES match exactly\")\n",
    "    else:\n",
    "        print(\"✗ TESTS FAILED: Implementation has issues\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return all_tests_passed\n",
    "\n",
    "test_hash_partition_join_comprehensive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hzzs-5K8hWkN"
   },
   "source": [
    "Implement `SortMergeJoin`:\n",
    "1. Sort both tables by join key\n",
    "2. Merge sorted sequences\n",
    "3. Handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odvuVpv2hWkN"
   },
   "outputs": [],
   "source": [
    "BWAY_MERGE_FACTOR = 10\n",
    "\n",
    "class SortMergeJoin:\n",
    "    def __init__(\n",
    "        self, bway_merge_factor: int = BWAY_MERGE_FACTOR, num_pages_per_split=1000\n",
    "    ):\n",
    "        self.bway_merge_factor = bway_merge_factor\n",
    "        self.num_pages_per_split = num_pages_per_split\n",
    "\n",
    "    def _flush_run(\n",
    "        self,\n",
    "        dfs: List[pd.DataFrame],\n",
    "        join_key: str,\n",
    "        output_dir: str,\n",
    "        side: str,\n",
    "        run_idx: int,\n",
    "    ) -> str:\n",
    "\n",
    "        df_run = pd.concat(dfs, ignore_index=True)\n",
    "        df_run_sorted = df_run.sort_values(by=join_key)\n",
    "\n",
    "        run_file = os.path.join(output_dir, f\"{side}_run_{run_idx}.parquet\")\n",
    "        df_run_sorted.to_parquet(run_file)\n",
    "\n",
    "        dfs.clear()\n",
    "        del df_run, df_run_sorted\n",
    "        gc.collect()\n",
    "\n",
    "        return run_file\n",
    "\n",
    "\n",
    "    def _external_sort(\n",
    "        self,\n",
    "        table: ColumnarDbFile,\n",
    "        join_key: str,\n",
    "        output_dir: str,\n",
    "        side: str,\n",
    "        columns: Optional[List[str]] = None,\n",
    "    ) -> ColumnarDbFile:\n",
    "        \"\"\"\n",
    "        Perform an external sort on a table based on the join key and return a sorted ColumnarDbFile.\n",
    "        Use _bway_merge to merge sorted files\n",
    "        \"\"\"\n",
    "\n",
    "        # Get table size (on disk)\n",
    "        disk_usage = table.table_disk_usage()\n",
    "        total_bytes = disk_usage[\"total_bytes\"]\n",
    "\n",
    "        # Check if we can safely process in 12 GB RAM\n",
    "        if table.can_process_parquet(total_bytes):\n",
    "\n",
    "            # read data in and sort all in RAM\n",
    "            df = table.retrieve_data(columns=columns)\n",
    "            df_sorted = df.sort_values(by=join_key).reset_index(drop=True)\n",
    "\n",
    "            # create paraquet in output dir for the table\n",
    "            sorted_name = f\"{side}_{table.table_name}_sorted\"\n",
    "            sorted_table = ColumnarDbFile(sorted_name, file_dir=output_dir)\n",
    "            sorted_table.build_table(df_sorted)\n",
    "\n",
    "            # clean unnecessary overhead and return table\n",
    "            del df, df_sorted\n",
    "            gc.collect()\n",
    "            return sorted_table\n",
    "\n",
    "        else:\n",
    "            print(\"sorting table \", table.table_name, \"with \", total_bytes, \"bytes using external sort\")\n",
    "            print(\"GBs : \", total_bytes / (1024 * 1024 * 1024))\n",
    "            # Get list of parquet files in the table directory\n",
    "            parquet_files = glob.glob(f\"{table.base_file_name}/*.parquet\")\n",
    "\n",
    "            runs_path: List[str] = []\n",
    "            run_idx = 0\n",
    "            current_dfs: List[pd.DataFrame] = []\n",
    "            current_row_groups = 0\n",
    "\n",
    "            # loop through all the parquet files\n",
    "            print(f\"Sorting {len(parquet_files)} files\")\n",
    "            for file in parquet_files:\n",
    "                pf = pq.ParquetFile(file)\n",
    "\n",
    "                # safe bounded unit of work for sorting\n",
    "                num_row_groups = pf.metadata.num_row_groups\n",
    "\n",
    "                for rg in range(num_row_groups):\n",
    "\n",
    "                    # read a row group as a chunk\n",
    "                    batch = pf.read_row_group(rg, columns=columns)\n",
    "                    df_chunk = batch.to_pandas()\n",
    "                    current_dfs.append(df_chunk)\n",
    "                    current_row_groups += 1\n",
    "\n",
    "                    # treating a row group as a page\n",
    "                    if current_row_groups > self.num_pages_per_split:\n",
    "\n",
    "                        print(\"flushing run \", run_idx)\n",
    "                        run_file = self._flush_run(\n",
    "                        current_dfs, join_key, output_dir, side, run_idx\n",
    "                        )\n",
    "                        runs_path.append(run_file)\n",
    "                        run_idx += 1\n",
    "                        current_row_groups = 0\n",
    "                        print(f\"Flushed run {run_idx} at {run_file}\")\n",
    "\n",
    "            # flush remaining partial run\n",
    "            if current_dfs:\n",
    "              run_file = self._flush_run(\n",
    "                  current_dfs, join_key, output_dir, side, run_idx\n",
    "              )\n",
    "              runs_path.append(run_file)\n",
    "\n",
    "            # Create the wrapper first so we write where it will read\n",
    "            sorted_table = ColumnarDbFile(\n",
    "                table_name=f\"{side}_{table.table_name}_sorted\",\n",
    "                file_dir=output_dir,\n",
    "            )\n",
    "\n",
    "            # Write the final merged file inside that directory, matching ColumnarDbFile\n",
    "            final_sorted_path = os.path.join(\n",
    "                sorted_table.base_file_name, f\"{sorted_table.table_name}-0.parquet\"\n",
    "            )\n",
    "            print(\"merging all runs into \", final_sorted_path)\n",
    "            self._merge_all_runs(runs_path, final_sorted_path, join_key)\n",
    "\n",
    "            return sorted_table\n",
    "\n",
    "    def _merge_all_runs(self, sorted_files: List[str], output_file: str, join_key: str):\n",
    "        \"\"\"\n",
    "        Merge multiple sorted Parquet files into a single sorted Parquet file.\n",
    "        \"\"\"\n",
    "        B = self.bway_merge_factor\n",
    "\n",
    "        # copy that we will mutate\n",
    "        runs = list(sorted_files)\n",
    "        pass_idx = 0\n",
    "\n",
    "        while len(runs) > 1:\n",
    "          print(\"merging pass \", pass_idx)\n",
    "          next_runs = []\n",
    "\n",
    "          # B - 1 input buffers +1 output buffer\n",
    "          for i in range(0, len(runs), B - 1):\n",
    "                batch = runs[i : i + (B - 1)]   # B-1 input buffers\n",
    "\n",
    "                # choose an output path for this merged batch\n",
    "                # on the final pass, we want the result at `output_file`\n",
    "                if len(runs) <= B - 1 and len(next_runs) == 0:\n",
    "                    # last pass, first (and only) merged run -> final output\n",
    "                    merged_path = output_file\n",
    "                else:\n",
    "                    # intermediate pass: write to a temp run file\n",
    "                    base_dir = os.path.dirname(output_file)\n",
    "                    merged_path = os.path.join(\n",
    "                        base_dir,\n",
    "                        f\"bway_pass{pass_idx}_run{len(next_runs)}.parquet\",\n",
    "                    )\n",
    "\n",
    "                # K-way merge this batch into merged_path\n",
    "                self._bway_merge(batch, merged_path, join_key)\n",
    "\n",
    "                next_runs.append(merged_path)\n",
    "\n",
    "          runs = next_runs\n",
    "          pass_idx += 1\n",
    "\n",
    "        # At this point, runs has exactly one file.\n",
    "        final_run = runs[0]\n",
    "        if final_run != output_file:\n",
    "            # In case we didn't land exactly on output_file path\n",
    "            if os.path.exists(output_file):\n",
    "                os.remove(output_file)\n",
    "            shutil.move(final_run, output_file)\n",
    "\n",
    "        return output_file\n",
    "\n",
    "\n",
    "    def _bway_merge(self, sorted_files: List[str], output_file: str, join_key: str):\n",
    "        \"\"\"\n",
    "        Merge a batch of sorted files into a single sorted file by join_key.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for path in sorted_files:\n",
    "            df = pd.read_parquet(path)\n",
    "            dfs.append(df)\n",
    "\n",
    "        merged = pd.concat(dfs, ignore_index=True)\n",
    "        merged_sorted = merged.sort_values(by=join_key)\n",
    "        merged_sorted.to_parquet(output_file, index=False)\n",
    "        print(sorted_files)\n",
    "\n",
    "    def join(\n",
    "        self,\n",
    "        table1: ColumnarDbFile,\n",
    "        table2: ColumnarDbFile,\n",
    "        join_key1: str,\n",
    "        join_key2: str,\n",
    "        temp_dir: str = \"temp\",\n",
    "        columns_table1: Optional[List[str]] = None,\n",
    "        columns_table2: Optional[List[str]] = None,\n",
    "    ) -> Optional[ColumnarDbFile]:\n",
    "        \"\"\"\n",
    "        Perform a sort-merge join between two ColumnarDbFile instances and return a sorted ColumnarDbFile.\n",
    "        \"\"\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        # Sort both tables externally\n",
    "        sorted_table1 = self._external_sort(\n",
    "            table1, join_key1, temp_dir, \"left\", columns_table1\n",
    "        )\n",
    "        sorted_table2 = self._external_sort(\n",
    "            table2, join_key2, temp_dir, \"right\", columns_table2\n",
    "        )\n",
    "\n",
    "        # 2) load sorted dataframes from the ColumnarDbFiles\n",
    "        sorted_table1 = sorted_table1.retrieve_data(columns=columns_table1)\n",
    "        sorted_table2 = sorted_table2.retrieve_data(columns=columns_table2)\n",
    "\n",
    "        joined_df = pd.merge(\n",
    "            sorted_table1,\n",
    "            sorted_table2,\n",
    "            left_on=join_key1,\n",
    "            right_on=join_key2,\n",
    "            how=\"inner\",\n",
    "        )\n",
    "\n",
    "        result_table = ColumnarDbFile(\"join_result\", file_dir=temp_dir)\n",
    "        result_table.build_table(joined_df)\n",
    "\n",
    "        return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yhxUqHS5ShN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ColumnarDbFile at 0x7b08d42781a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songs_table = tables['Songs']\n",
    "users_table = tables['Users']\n",
    "listens_table = tables['Listens']\n",
    "\n",
    "smj = SortMergeJoin()\n",
    "\n",
    "# Example: join Songs with Listens on song_id\n",
    "sorted_join_result = smj.join(\n",
    "    songs_table,\n",
    "    listens_table,\n",
    "    join_key1=\"song_id\",\n",
    "    join_key2=\"song_id\",\n",
    "    temp_dir=\"temp_songs_listens\",\n",
    "    columns_table1= [\"song_id\", \"title\"],\n",
    "    columns_table2= [\"song_id\", \"user_id\"]\n",
    ")\n",
    "\n",
    "display(sorted_join_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af8XhdLWhWkN"
   },
   "source": [
    "Implement GROUP BY after joins:\n",
    "- Here you could use `pd.groupby` or do manual aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "4iwQ65LBhWkN"
   },
   "outputs": [],
   "source": [
    "# GROUP BY s.song_id, s.title\n",
    "class HashGroupbyAverageAndDistinct():\n",
    "    def __init__(self, num_partitions, parquet_batch_size, use_streaming=False):\n",
    "        self.num_partitions = num_partitions\n",
    "        self.parquet_batch_size = parquet_batch_size\n",
    "        self.use_streaming = use_streaming\n",
    "    \n",
    "    def _make_partition_path(self, temp_dir, part_id):\n",
    "        return os.path.join(temp_dir, f\"group_part{part_id}.parquet\")\n",
    "    \n",
    "    def groupby_average_distinct(self,\n",
    "                        table: ColumnarDbFile, \n",
    "                        groupby_cols: List[str],\n",
    "                        average_col: str, \n",
    "                        average_col_name: str, \n",
    "                        distinct_col: str,\n",
    "                        distinct_col_name: str,\n",
    "                        select_cols: List[str], \n",
    "                        temp_dir='groupby_temp') -> ColumnarDbFile:\n",
    "        \"\"\"\n",
    "        Perform:\n",
    "            SELECT select_cols..., AVG(average_col) AS average_col_name, COUNT(DISTINCT distinct_col)\n",
    "            FROM table\n",
    "            GROUP BY groupby_cols...\n",
    "        \n",
    "        Hash partitioning on (concatenation of) groupby_cols, and then in-memory aggregation per partition\n",
    "\n",
    "        Assumptions:\n",
    "        - groupby_col is non-empty\n",
    "        - select_cols is a subset of groupby_col\n",
    "        - Per-partition hash table fits in memory\n",
    "        \n",
    "        Uses self.use_streaming to determine whether to use ParquetWriter streaming\n",
    "        for efficient batch writes.\n",
    "        \"\"\"\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        \n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        # hash on groupby columns (safe: same group always shares this)\n",
    "\n",
    "        hash_partition(\n",
    "            table=table,\n",
    "            hash_keys=groupby_cols,  # hash on groupby columns (safe: same group always shares this)\n",
    "            num_partitions=self.num_partitions,\n",
    "            parquet_batch_size=self.parquet_batch_size,\n",
    "            hash_value_fn=HASHVALUE,\n",
    "            make_partition_path_fn=partial(self._make_partition_path, temp_dir),\n",
    "            columns= list(set(groupby_cols + [average_col, distinct_col])),\n",
    "        )\n",
    "\n",
    "        output_db = ColumnarDbFile(f\"{table.table_name}_groupby_avg\")\n",
    "        \n",
    "        # Start streaming if enabled\n",
    "        if self.use_streaming:\n",
    "            output_db.start_stream()\n",
    "        for part_id in range(self.num_partitions):\n",
    "            part_path = self._make_partition_path(temp_dir, part_id)\n",
    "            if not os.path.exists(part_path):\n",
    "                continue\n",
    "\n",
    "            # In-memory hash table for this partition:\n",
    "            # key: tuple of groupby_col values\n",
    "            # value: Tuple of (sum of average_col, count of average_col, set of distinct distinct_col values)\n",
    "            SUM_IDX = 0\n",
    "            COUNT_IDX = 1\n",
    "            DISTINCT_SET_IDX = 2\n",
    "            agg_map: Dict[Any, Tuple[float, int, set]] = {}\n",
    "\n",
    "            pf = pq.ParquetFile(part_path)\n",
    "            for batch in pf.iter_batches(batch_size=self.parquet_batch_size):\n",
    "                df = batch.to_pandas()\n",
    "\n",
    "                grouped = (\n",
    "                    df.groupby(groupby_cols)\n",
    "                    .agg(\n",
    "                        sum_avg=(average_col, \"sum\"),\n",
    "                        cnt_avg=(average_col, \"count\"),  # SQL AVG ignores NULLs\n",
    "                        distinct_set=(distinct_col, lambda s: set(s.dropna()))  # SQL ignores NULL in COUNT DISTINCT\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for key_tuple, row in grouped.iterrows():\n",
    "                    if not isinstance(key_tuple, tuple):\n",
    "                        key_tuple = (key_tuple,)\n",
    "\n",
    "                    state = agg_map.setdefault(key_tuple, [0.0, 0, set()])\n",
    "                    state[SUM_IDX] += row[\"sum_avg\"]\n",
    "                    state[COUNT_IDX] += row[\"cnt_avg\"]\n",
    "                    state[DISTINCT_SET_IDX] |= row[\"distinct_set\"]\n",
    "\n",
    "\n",
    "            # Turn the per-partition hash table into a DataFrame and append\n",
    "            if agg_map:\n",
    "                # Pre-compute column index mapping to avoid repeated index() calls\n",
    "                col_idx_map = {col: groupby_cols.index(col) for col in select_cols}\n",
    "                \n",
    "                # Build output efficiently using list comprehensions\n",
    "                out_rows = []\n",
    "                for key_tuple, state in agg_map.items():\n",
    "                    # Ensure key_tuple is a tuple (handles single vs multi-column)\n",
    "                    if not isinstance(key_tuple, tuple):\n",
    "                        key_tuple = (key_tuple,)\n",
    "                    \n",
    "                    row_dict = {col: key_tuple[col_idx_map[col]] for col in select_cols}\n",
    "                    row_dict[average_col_name] = state[SUM_IDX] / state[COUNT_IDX]\n",
    "                    row_dict[distinct_col_name] = len(state[DISTINCT_SET_IDX])\n",
    "                    out_rows.append(row_dict)\n",
    "\n",
    "                if out_rows:\n",
    "                    out_df = pd.DataFrame(out_rows)\n",
    "                    output_db.append_data(out_df)\n",
    "\n",
    "        # Stop streaming if it was enabled\n",
    "        if self.use_streaming:\n",
    "            output_db.stop_stream()\n",
    "\n",
    "        return output_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2980.83 MiB, increment: 722.27 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "# test implementation\n",
    "\n",
    "SIZE = \"1GB\" #[\"100MB\", \"1GB\", \"10GB\"]\n",
    "SAMPLE = 100\n",
    "USE_STREAMING = False\n",
    "\n",
    "listens_table = tables[f'Listens_{SIZE}']\n",
    "\n",
    "groupby_average_distinct = HashGroupbyAverageAndDistinct(\n",
    "    num_partitions=4,\n",
    "    parquet_batch_size=10000000,\n",
    "    use_streaming=USE_STREAMING\n",
    ")\n",
    "\n",
    "results = groupby_average_distinct.groupby_average_distinct(\n",
    "    table=listens_table,\n",
    "    groupby_cols=['song_id'],\n",
    "    average_col='user_id',\n",
    "    average_col_name='avg_user_id',\n",
    "    distinct_col='user_id',\n",
    "    distinct_col_name='distinct_user_id',\n",
    "    select_cols=['song_id']\n",
    ")\n",
    "result_df = result_songs_listens.retrieve_data(sample=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feYU7Tdlke9Z"
   },
   "source": [
    "# Section 4: Query Planning & Optimization\n",
    "\n",
    "In this section, you'll implement smart query planning using metadata analysis. The key idea is to **avoid loading data unnecessarily** by:\n",
    "1. Analyzing Parquet metadata first (row counts, column names, file sizes)\n",
    "2. Making intelligent decisions about join order and algorithm selection\n",
    "3. Loading only the columns you actually need for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQt-sR-zhWkN"
   },
   "outputs": [],
   "source": [
    "def analyze_metadata_before_loading(file_paths):\n",
    "    \"\"\"YOUR TASK: Get table statistics WITHOUT loading data\n",
    "\n",
    "    Hints:\n",
    "    - Use pq.ParquetFile() to access metadata\n",
    "    - Extract: num_rows, column names, file sizes\n",
    "    - DON'T use pd.read_parquet() here - that loads data!\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "\n",
    "    # TODO: For each table ('songs', 'users', 'listens'):\n",
    "    #   - Open the Parquet file (but don't load data)\n",
    "    #   - Extract metadata like row count, columns, sizes\n",
    "    #   - Store in a dictionary\n",
    "    pass  # Your implementation here\n",
    "\n",
    "\n",
    "def plan_query_execution(metadata, parsed_query):\n",
    "    \"\"\"YOUR TASK: Use metadata to make smart decisions\n",
    "\n",
    "    Questions to answer:\n",
    "    - Which table is smallest? Largest?\n",
    "    - Will a hash table fit in memory?\n",
    "    - Which columns does the query actually need?\n",
    "    - What's the optimal join order?\n",
    "    \"\"\"\n",
    "    # TODO: Based on metadata, decide:\n",
    "    #   1. Join order (smallest first? or different strategy?)\n",
    "    #   2. Algorithm choice (HPJ if fits in memory, else SMJ)\n",
    "    #   3. Which columns to load for each table\n",
    "    pass  # Your implementation here\n",
    "\n",
    "\n",
    "# After planning, load ONLY what you need:\n",
    "# Example (you implement the actual logic):\n",
    "# columns_needed = ['song_id', 'artist']  # From your planning\n",
    "# df = pd.read_parquet('songs.parquet', columns=columns_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhwOd7sfhWkN"
   },
   "outputs": [],
   "source": [
    "class QueryPlanner:\n",
    "    pass # Your implementation here\n",
    "\n",
    "\n",
    "class QueryExecutor:\n",
    "    def __init__(self, tables, num_partitions=8, output_dir=\"temp\", planner=None, size=\"100MB\"):\n",
    "        self.tables = tables\n",
    "        self.num_partitions = num_partitions\n",
    "        self.output_dir = output_dir\n",
    "        self.planner = planner or QueryPlanner()\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def execute_hardcoded_query(self):\n",
    "        \"\"\"\n",
    "        Executes the following SQL query:\n",
    "\n",
    "        SELECT s.song_id, AVG(u.age) AS avg_age,\n",
    "        COUNT(DISTINCT l.user_id)\n",
    "        FROM Songs s\n",
    "        JOIN Listens l ON s.song_id = l.song_id\n",
    "        JOIN Users u ON l.user_id = u.user_id\n",
    "        GROUP BY s.song_id, s.title\n",
    "        ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
    "        \"\"\"\n",
    "        # Hardcoded\n",
    "        columns = {\"Songs\": [\"song_id\", \"title\"], \"Listens\": [\"listen_id\", \"song_id\", \"user_id\"], \"Users\": [\"user_id\", \"age\"]}\n",
    "\n",
    "        # TODO: this should be specified by the query planner\n",
    "        join_order = [\"Listens\", \"Users\", \"Songs\"]\n",
    "        join_algorithm = [\"HPJ\", \"HPJ\"]\n",
    "\n",
    "        # do joins\n",
    "        result = self.tables[f\"{join_order[0]}_{SIZE}\"]\n",
    "        for i in range(1, len(join_order)):\n",
    "            table = self.tables[f\"{join_order[i]}_{SIZE}\"]\n",
    "            if join_algorithm[i-1] == \"HPJ\":\n",
    "                # use HPJ\n",
    "            else:\n",
    "                # use SMJ\n",
    "\n",
    "        # do group by \n",
    "        avg_col = \"age\"\n",
    "        avg_col_name = \"avg_age\"\n",
    "        distinct_col = \"user_id\"\n",
    "        distinct_col_name = \"distinct_user_id\"\n",
    "        select_cols = [\"song_id\"]\n",
    "\n",
    "        # do group by\n",
    "        \n",
    "\n",
    "        # sort by count distinct\n",
    "\n",
    "        # Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BunT6g1HklaH"
   },
   "source": [
    "# Section 5: Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpRb3IsakmzT"
   },
   "outputs": [],
   "source": [
    "def benchmark_query(executor, dataset_size):\n",
    "    \"\"\"Benchmark the query execution time and memory usage.\"\"\"\n",
    "    print(f\"\\nBenchmarking with {dataset_size} dataset...\")\n",
    "    start_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = executor.execute_hardcoded_query()\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "\n",
    "    print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Memory Usage: {end_mem - start_mem:.2f} MB\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUny8jDziWxk"
   },
   "source": [
    "## 100MB Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OBi6vhriYRK"
   },
   "outputs": [],
   "source": [
    "# Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HalUj5s-ifAu"
   },
   "source": [
    "## 1GB Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwmefvmzigRO"
   },
   "outputs": [],
   "source": [
    "# Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8h2f0TAijZT"
   },
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AQL4sxdinZn"
   },
   "outputs": [],
   "source": [
    "# baselines: https://edstem.org/us/courses/87394/discussion/7276409\n",
    "# Your implementation here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs145-project2",
   "language": "python",
   "name": "cs145-project2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
