{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM8u06dhhieu"
      },
      "source": [
        "## Collaborators\n",
        "\n",
        "1.   Berk Yalcinkaya\n",
        "2.   Nick Allen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T7CuUFejs1R"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kSLvQ6xjjqQh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import uuid\n",
        "import argparse\n",
        "import time\n",
        "import psutil\n",
        "import heapq\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwVW6zMghmMq"
      },
      "source": [
        "# Section 0: Generate Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y0yOxXShWkL"
      },
      "source": [
        "This section has already been implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Djt3vqewhA76"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "\n",
        "def generate_songs_chunk(start, size, string_length=100):\n",
        "    data = {\n",
        "        \"song_id\": range(start, start + size),\n",
        "        \"title\": [f\"Song_{i}\" for i in range(start, start + size)],\n",
        "    }\n",
        "    base_strings = generate_base_strings(size, string_length)\n",
        "    for i in range(1, 11):\n",
        "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_users_chunk(start, size, string_length=100):\n",
        "    data = {\n",
        "        \"user_id\": range(start, start + size),\n",
        "        \"age\": [18 + ((start + i) % 60) for i in range(size)],\n",
        "    }\n",
        "    base_strings = generate_base_strings(size, string_length)\n",
        "    for i in range(1, 11):\n",
        "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_listens_chunk(start, size, num_users, num_songs, string_length=16):\n",
        "    data = {\n",
        "        \"listen_id\": range(start, start + size),\n",
        "        \"user_id\": np.random.randint(0, num_users, size=size),\n",
        "        \"song_id\": np.random.randint(0, num_songs, size=size),\n",
        "    }\n",
        "    base_strings = generate_base_strings(size, string_length)\n",
        "    for i in range(1, 11):\n",
        "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_base_strings(num_records, string_length):\n",
        "    chars = np.array(list(\"ab\"))\n",
        "    random_indices = np.random.randint(0, len(chars), size=(num_records, string_length))\n",
        "    char_array = chars[random_indices]\n",
        "    return np.array(list(map(\"\".join, char_array)))\n",
        "\n",
        "\n",
        "def _write_parquet_streamed(\n",
        "    filename,\n",
        "    total_rows,\n",
        "    make_chunk_fn,\n",
        "    chunk_size=250_000,\n",
        "    compression=\"snappy\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Stream DataFrame chunks to a single Parquet file with one ParquetWriter.\n",
        "    - schema_df: optional small DataFrame to lock schema; if None we'll infer from the first chunk.\n",
        "    \"\"\"\n",
        "    written = 0\n",
        "\n",
        "    first_chunk = make_chunk_fn(0, min(chunk_size, total_rows))\n",
        "    first_table = pa.Table.from_pandas(first_chunk, preserve_index=False)\n",
        "    writer = pq.ParquetWriter(filename, first_table.schema, compression=compression)\n",
        "    writer.write_table(first_table)\n",
        "\n",
        "    written += len(first_chunk)\n",
        "    del first_chunk\n",
        "    gc.collect()\n",
        "\n",
        "    while written < total_rows:\n",
        "        take = min(chunk_size, total_rows - written)\n",
        "        chunk_df = make_chunk_fn(written, take)\n",
        "        writer.write_table(pa.Table.from_pandas(chunk_df, preserve_index=False))\n",
        "        written += take\n",
        "        del chunk_df\n",
        "        gc.collect()\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def generate_test_data(target_size=\"100MB\"):\n",
        "    \"\"\"\n",
        "    Generate datasets with proper foreign key relationships.\n",
        "\n",
        "    Target COMPRESSED Parquet file sizes on disk:\n",
        "    100MB total compressed:\n",
        "        - Songs: 10K rows → ~5MB (5% of total)\n",
        "        - Users: 50K rows → ~20MB (20% of total)\n",
        "        - Listens: 1M rows → ~75MB (75% of total)\n",
        "    1GB total compressed:\n",
        "        - Songs: 100K rows → ~50MB (5% of total)\n",
        "        - Users: 500K rows → ~200MB (20% of total)\n",
        "        - Listens: 10M rows → ~750MB (75% of total)\n",
        "\n",
        "    Each table needs:\n",
        "        - Primary key column(s)\n",
        "        - 10 additional string columns of k characters each\n",
        "        - For Users: add 'age' column (random 18-80)\n",
        "\n",
        "    CRITICAL: Listens table must have valid foreign keys!\n",
        "    Every song_id must exist in Songs\n",
        "    Every user_id must exist in Users\n",
        "    \"\"\"\n",
        "\n",
        "    assert target_size in [\"100MB\", \"1GB\"]\n",
        "    if target_size == \"100MB\":\n",
        "        num_songs = 10_000\n",
        "        num_users = 50_000\n",
        "        num_listens = 1_000_000\n",
        "\n",
        "        songs_chunk = 10_000\n",
        "        users_chunk = 50_000\n",
        "        listens_chunk = 1_000_000\n",
        "    else:\n",
        "        num_songs = 100_000\n",
        "        num_users = 500_000\n",
        "        num_listens = 10_000_000\n",
        "\n",
        "        songs_chunk = 10_000\n",
        "        users_chunk = 50_000\n",
        "        listens_chunk = 1_000_000\n",
        "\n",
        "    print(\"Writing Songs\")\n",
        "    _write_parquet_streamed(\n",
        "        filename=f\"songs_{target_size}.parquet\",\n",
        "        total_rows=num_songs,\n",
        "        make_chunk_fn=lambda start, size: generate_songs_chunk(start, size),\n",
        "        chunk_size=songs_chunk,\n",
        "    )\n",
        "\n",
        "    print(\"Writing Users\")\n",
        "    _write_parquet_streamed(\n",
        "        filename=f\"users_{target_size}.parquet\",\n",
        "        total_rows=num_users,\n",
        "        make_chunk_fn=lambda start, size: generate_users_chunk(start, size),\n",
        "        chunk_size=users_chunk,\n",
        "    )\n",
        "\n",
        "    print(\"Writing Listens\")\n",
        "    _write_parquet_streamed(\n",
        "        filename=f\"listens_{target_size}.parquet\",\n",
        "        total_rows=num_listens,\n",
        "        make_chunk_fn=lambda start, size: generate_listens_chunk(\n",
        "            start, size, num_users, num_songs\n",
        "        ),\n",
        "        chunk_size=listens_chunk,\n",
        "    )\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r5qnpQghhWkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Songs\n",
            "Writing Users\n",
            "Writing Listens\n",
            "Done!\n",
            "Writing Songs\n",
            "Writing Users\n",
            "Writing Listens\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "random.seed(0)\n",
        "\n",
        "generate_test_data('100MB')\n",
        "generate_test_data('1GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEiGGznFhtxo"
      },
      "source": [
        "# Section 1: Parquet-based Columnar Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BGrkP5PhWkM"
      },
      "source": [
        "Implement Parquet-based storage for the tables\n",
        "- For simplicity, store all data for a table in a single Parquet file and use a single DataFrame object as a buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0o8zkpGhWkM"
      },
      "outputs": [],
      "source": [
        "# see ed: https://edstem.org/us/courses/87394/discussion/7251811 for advice on writing to a parquet without loading existing into RAM\n",
        "class ColumnarDbFile:\n",
        "    def __init__(self, table_name, file_dir='data', file_pfx=''):\n",
        "        self.file_pfx = file_pfx\n",
        "        self.table_name = table_name\n",
        "        self.file_dir = file_dir\n",
        "        os.makedirs(self.file_dir, exist_ok=True)\n",
        "        self.base_file_name = f\"{self.file_dir}/{self.file_pfx}_{self.table_name}\"\n",
        "\n",
        "    def build_table(self, data):\n",
        "        \"\"\"Build and save table data to Parquet.\"\"\"\n",
        "        data.to_parquet(self.base_file_name)\n",
        "        return\n",
        "\n",
        "    def retrieve_data(self, columns=None):\n",
        "        \"\"\"Create pd.DataFrame by reading from Parquet\"\"\"\n",
        "        return pd.read_parquet(self.base_file_name, columns=columns)\n",
        "\n",
        "    def append_data(self, data):\n",
        "        \"\"\"Append new data to Parquet\"\"\"\n",
        "        # Your implementation here\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gPpXtxghWkM"
      },
      "outputs": [],
      "source": [
        "print(\"Building tables...\")\n",
        "if os.path.exists('data'):\n",
        "    shutil.rmtree('data')\n",
        "tables = {\n",
        "    'Songs': ColumnarDbFile(\"Songs\", file_dir='data'),\n",
        "    'Users': ColumnarDbFile(\"Users\", file_dir='data'),\n",
        "    'Listens': ColumnarDbFile(\"Listens\", file_dir='data')\n",
        "}\n",
        "\n",
        "size = \"100MB\"\n",
        "songs_data = pd.read_parquet(f'songs_{size}.parquet')\n",
        "users_data = pd.read_parquet(f'users_{size}.parquet')\n",
        "listens_data = pd.read_parquet(f'listens_{size}.parquet')\n",
        "\n",
        "tables['Songs'].build_table(songs_data)\n",
        "tables['Users'].build_table(users_data)\n",
        "tables['Listens'].build_table(listens_data)\n",
        "print(\"Tables built successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfutqrA2hWkM"
      },
      "outputs": [],
      "source": [
        "# retrieve data\n",
        "tables['Songs'].retrieve_data(columns = ['song_id', 'title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiPY1Hs9hWkM"
      },
      "outputs": [],
      "source": [
        "tables['Listens'].retrieve_data(columns = ['listen_id', 'user_id', 'song_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtLVO3cChWkM"
      },
      "source": [
        "Analyze and report on:\n",
        "- Space efficiency compared to row storage\n",
        "  - e.g. Compare file sizes on disk: How much disk space does Parquet use vs. a row storage format like CSV?\n",
        "- Compression ratios achieved with Parquet\n",
        "  - e.g. Compare Parquet’s uncompressed encoded size (reported in its metadata) to its compressed on-disk size to compute compression ratios.\n",
        "  - You could also report the memory expansion factor: how much larger the dataset becomes when loaded into a `pd.DataFrame` compared to the compressed file size.\n",
        "- Read/write performance characteristics\n",
        "  - e.g. Read performance: How long does it take to read all columns from Parquet vs. CSV?\n",
        "  - e.g. Columnar advantage: How long does it take to read selective columns from Parquet vs. reading all columns?\n",
        "  - e.g. Write performance: How long does it take to write data to Parquet vs. CSV?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kfNisQFhWkM"
      },
      "outputs": [],
      "source": [
        "def analyze():\n",
        "    pass # Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8U3edewiDBa"
      },
      "source": [
        "# Section 2: Parse SQL Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_KoWHLohWkM"
      },
      "source": [
        "In this section, you should implement logic to parse the following SQL query:\n",
        "```sql\n",
        "    SELECT s.song_id, AVG(u.age) AS avg_age,\n",
        "       COUNT(DISTINCT l.user_id) AS count_distinct_users,\n",
        "    FROM Songs s\n",
        "    JOIN Listens l ON s.song_id = l.song_id\n",
        "    JOIN Users u ON l.user_id = u.user_id\n",
        "    GROUP BY s.song_id, s.title\n",
        "    ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
        "```\n",
        "\n",
        "You should manually extract the components from the provided query (i.e. you don't need to implement a general SQL parser, just handle this specific query)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT3jWKFYhWkN"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT s.song_id, AVG(u.age) AS avg_age,\n",
        "COUNT(DISTINCT l.user_id)\n",
        "FROM Songs s\n",
        "JOIN Listens l ON s.song_id = l.song_id\n",
        "JOIN Users u ON l.user_id = u.user_id\n",
        "GROUP BY s.song_id, s.title\n",
        "ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1PmMhCRhv0r"
      },
      "outputs": [],
      "source": [
        "def parse_sql(query):\n",
        "    \"\"\"\n",
        "    YOUR TASK: Extract tables, joins, and aggregations\n",
        "    \"\"\"\n",
        "    # Parse SQL string to identify:\n",
        "    # - Tables involved\n",
        "    # - Join conditions\n",
        "    # - GROUP BY columns\n",
        "    # - Aggregation functions\n",
        "    pass  # Your implementation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8mb80aeTAxM"
      },
      "outputs": [],
      "source": [
        "parse_sql(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071jzSZqkGyC"
      },
      "source": [
        "# Section 3: Implement Join Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14LszEqZhWkN"
      },
      "source": [
        "In this section, you will implement the execution operators (*how* to join) and aggregation after joins.\n",
        "\n",
        "**Reminder:** If you use temporary files or folders, you should clean them up either as part of your join logic, or after each run. Otherwise you might run into correctness issues!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AVO_NRnkHq1"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "\n",
        "def HASHVALUE(value, B):\n",
        "    if isinstance(value, int):\n",
        "        return hash(value) % B\n",
        "    sha256 = hashlib.sha256()\n",
        "    sha256.update(str(value).encode(\"utf-8\"))\n",
        "    return int(sha256.hexdigest(), 16) % B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q46rgYihWkN"
      },
      "source": [
        "Implement `HashPartitionJoin`:\n",
        "1. Hash partition both tables\n",
        "2. Build hash table from smaller partition\n",
        "3. Probe with larger partition\n",
        "4. Return joined results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1pw1bPrhWkN"
      },
      "outputs": [],
      "source": [
        "class HashPartitionJoin:\n",
        "    def __init__(self, num_partitions=4):\n",
        "        self.num_partitions = num_partitions\n",
        "\n",
        "    def join(self, table1: ColumnarDbFile, table2: ColumnarDbFile, join_key1, join_key2,\n",
        "             temp_dir='temp', columns_table1=None, columns_table2=None):\n",
        "        \"\"\"\n",
        "        Perform a hash partition join between two ColumnarDbFile instances.\n",
        "\n",
        "        Parameters:\n",
        "        - table1: Left table (ColumnarDbFile)\n",
        "        - table2: Right table (ColumnarDbFile)\n",
        "        - join_key1: Join key from table1\n",
        "        - join_key2: Join key from table2\n",
        "        - temp_dir: Directory to store temporary files\n",
        "        - columns_table1: List of columns to select from table1\n",
        "        - columns_table2: List of columns to select from table2\n",
        "\n",
        "        Returns:\n",
        "        - join_result_table: ColumnarDbFile instance containing the join results\n",
        "        \"\"\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "        # Partition both tables\n",
        "        partitions1 = self._hash_partition(table1, join_key1, temp_dir, 'left', columns_table1)\n",
        "        partitions2 = self._hash_partition(table2, join_key2, temp_dir, 'right', columns_table2)\n",
        "\n",
        "        # Your implementation here\n",
        "\n",
        "    def _hash_partition(self, table: ColumnarDbFile, join_key, output_dir, side, columns=None):\n",
        "        # Your implementation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzYK2oWS5O47"
      },
      "outputs": [],
      "source": [
        "# Optional: Verify your implementation against pd.merge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzzs-5K8hWkN"
      },
      "source": [
        "Implement `SortMergeJoin`:\n",
        "1. Sort both tables by join key\n",
        "2. Merge sorted sequences\n",
        "3. Handle duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odvuVpv2hWkN"
      },
      "outputs": [],
      "source": [
        "BWAY_MERGE_FACTOR = 10\n",
        "\n",
        "class SortMergeJoin:\n",
        "    def __init__(\n",
        "        self, bway_merge_factor: int = BWAY_MERGE_FACTOR, num_pages_per_split=1000\n",
        "    ):\n",
        "        self.bway_merge_factor = bway_merge_factor\n",
        "        self.num_pages_per_split = num_pages_per_split\n",
        "\n",
        "    def _external_sort(\n",
        "        self,\n",
        "        table: ColumnarDbFile,\n",
        "        join_key: str,\n",
        "        output_dir: str,\n",
        "        side: str,\n",
        "        columns: Optional[List[str]] = None,\n",
        "    ) -> ColumnarDbFile:\n",
        "        \"\"\"\n",
        "        Perform an external sort on a table based on the join key and return a sorted ColumnarDbFile.\n",
        "        Use _bway_merge to merge sorted files\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "\n",
        "    def _bway_merge(self, sorted_files: List[str], output_file: str, join_key: str):\n",
        "        \"\"\"\n",
        "        Merge multiple sorted Parquet files into a single sorted Parquet file using B-way merge.\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "\n",
        "    def join(\n",
        "        self,\n",
        "        table1: ColumnarDbFile,\n",
        "        table2: ColumnarDbFile,\n",
        "        join_key1: str,\n",
        "        join_key2: str,\n",
        "        temp_dir: str = \"temp\",\n",
        "        columns_table1: Optional[List[str]] = None,\n",
        "        columns_table2: Optional[List[str]] = None,\n",
        "    ) -> Optional[ColumnarDbFile]:\n",
        "        \"\"\"\n",
        "        Perform a sort-merge join between two ColumnarDbFile instances and return a sorted ColumnarDbFile.\n",
        "        \"\"\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Sort both tables externally\n",
        "        sorted_table1 = self._external_sort(\n",
        "            table1, join_key1, temp_dir, \"left\", columns_table1\n",
        "        )\n",
        "        sorted_table2 = self._external_sort(\n",
        "            table2, join_key2, temp_dir, \"right\", columns_table2\n",
        "        )\n",
        "\n",
        "        # Your implementation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yhxUqHS5ShN"
      },
      "outputs": [],
      "source": [
        "# Optional: Verify your implementation against pd.merge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af8XhdLWhWkN"
      },
      "source": [
        "Implement GROUP BY after joins:\n",
        "- Here you could use `pd.groupby` or do manual aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iwQ65LBhWkN"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feYU7Tdlke9Z"
      },
      "source": [
        "# Section 4: Query Planning & Optimization\n",
        "\n",
        "In this section, you'll implement smart query planning using metadata analysis. The key idea is to **avoid loading data unnecessarily** by:\n",
        "1. Analyzing Parquet metadata first (row counts, column names, file sizes)\n",
        "2. Making intelligent decisions about join order and algorithm selection\n",
        "3. Loading only the columns you actually need for the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQt-sR-zhWkN"
      },
      "outputs": [],
      "source": [
        "def analyze_metadata_before_loading(file_paths):\n",
        "    \"\"\"YOUR TASK: Get table statistics WITHOUT loading data\n",
        "\n",
        "    Hints:\n",
        "    - Use pq.ParquetFile() to access metadata\n",
        "    - Extract: num_rows, column names, file sizes\n",
        "    - DON'T use pd.read_parquet() here - that loads data!\n",
        "    \"\"\"\n",
        "    metadata = {}\n",
        "\n",
        "    # TODO: For each table ('songs', 'users', 'listens'):\n",
        "    #   - Open the Parquet file (but don't load data)\n",
        "    #   - Extract metadata like row count, columns, sizes\n",
        "    #   - Store in a dictionary\n",
        "    pass  # Your implementation here\n",
        "\n",
        "\n",
        "def plan_query_execution(metadata, parsed_query):\n",
        "    \"\"\"YOUR TASK: Use metadata to make smart decisions\n",
        "\n",
        "    Questions to answer:\n",
        "    - Which table is smallest? Largest?\n",
        "    - Will a hash table fit in memory?\n",
        "    - Which columns does the query actually need?\n",
        "    - What's the optimal join order?\n",
        "    \"\"\"\n",
        "    # TODO: Based on metadata, decide:\n",
        "    #   1. Join order (smallest first? or different strategy?)\n",
        "    #   2. Algorithm choice (HPJ if fits in memory, else SMJ)\n",
        "    #   3. Which columns to load for each table\n",
        "    pass  # Your implementation here\n",
        "\n",
        "\n",
        "# After planning, load ONLY what you need:\n",
        "# Example (you implement the actual logic):\n",
        "# columns_needed = ['song_id', 'artist']  # From your planning\n",
        "# df = pd.read_parquet('songs.parquet', columns=columns_needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhwOd7sfhWkN"
      },
      "outputs": [],
      "source": [
        "class QueryPlanner:\n",
        "    pass # Your implementation here\n",
        "\n",
        "\n",
        "class QueryExecutor:\n",
        "    def __init__(self, tables, num_partitions=8, output_dir=\"temp\", planner=None):\n",
        "        self.tables = tables\n",
        "        self.num_partitions = num_partitions\n",
        "        self.output_dir = output_dir\n",
        "        self.planner = planner or QueryPlanner()\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def execute_hardcoded_query(self):\n",
        "        \"\"\"\n",
        "        Executes the following SQL query:\n",
        "\n",
        "        SELECT s.song_id, AVG(u.age) AS avg_age,\n",
        "        COUNT(DISTINCT l.user_id)\n",
        "        FROM Songs s\n",
        "        JOIN Listens l ON s.song_id = l.song_id\n",
        "        JOIN Users u ON l.user_id = u.user_id\n",
        "        GROUP BY s.song_id, s.title\n",
        "        ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
        "        \"\"\"\n",
        "\n",
        "        # Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BunT6g1HklaH"
      },
      "source": [
        "# Section 5: Performance Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpRb3IsakmzT"
      },
      "outputs": [],
      "source": [
        "def benchmark_query(executor, dataset_size):\n",
        "    \"\"\"Benchmark the query execution time and memory usage.\"\"\"\n",
        "    print(f\"\\nBenchmarking with {dataset_size} dataset...\")\n",
        "    start_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "    start_time = time.time()\n",
        "\n",
        "    result = executor.execute_hardcoded_query()\n",
        "\n",
        "    end_time = time.time()\n",
        "    end_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
        "    print(f\"Memory Usage: {end_mem - start_mem:.2f} MB\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUny8jDziWxk"
      },
      "source": [
        "## 100MB Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OBi6vhriYRK"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HalUj5s-ifAu"
      },
      "source": [
        "## 1GB Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwmefvmzigRO"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8h2f0TAijZT"
      },
      "source": [
        "## Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AQL4sxdinZn"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs145-project2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
