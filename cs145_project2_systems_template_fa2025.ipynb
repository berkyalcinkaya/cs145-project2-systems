{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berkyalcinkaya/cs145-project2-systems/blob/main/cs145_project2_systems_template_fa2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM8u06dhhieu"
      },
      "source": [
        "## Collaborators\n",
        "\n",
        "1.   Berk Yalcinkaya\n",
        "2.   Nick Allen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T7CuUFejs1R"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kSLvQ6xjjqQh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import uuid\n",
        "import argparse\n",
        "import time\n",
        "import psutil\n",
        "import heapq\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import shutil\n",
        "import glob\n",
        "import gc\n",
        "from IPython.display import display\n",
        "import tempfile\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwVW6zMghmMq"
      },
      "source": [
        "# Section 0: Generate Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y0yOxXShWkL"
      },
      "source": [
        "This section has already been implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Djt3vqewhA76"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "\n",
        "def generate_songs_chunk(start, size, string_length=100):\n",
        "    data = {\n",
        "        \"song_id\": range(start, start + size),\n",
        "        \"title\": [f\"Song_{i}\" for i in range(start, start + size)],\n",
        "    }\n",
        "    base_strings = generate_base_strings(size, string_length)\n",
        "    for i in range(1, 11):\n",
        "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_users_chunk(start, size, string_length=100):\n",
        "    data = {\n",
        "        \"user_id\": range(start, start + size),\n",
        "        \"age\": [18 + ((start + i) % 60) for i in range(size)],\n",
        "    }\n",
        "    base_strings = generate_base_strings(size, string_length)\n",
        "    for i in range(1, 11):\n",
        "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_listens_chunk(start, size, num_users, num_songs, string_length=16):\n",
        "    data = {\n",
        "        \"listen_id\": range(start, start + size),\n",
        "        \"user_id\": np.random.randint(0, num_users, size=size),\n",
        "        \"song_id\": np.random.randint(0, num_songs, size=size),\n",
        "    }\n",
        "    base_strings = generate_base_strings(size, string_length)\n",
        "    for i in range(1, 11):\n",
        "        data[f\"extra_col_{i}\"] = np.roll(base_strings, shift=i)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_base_strings(num_records, string_length):\n",
        "    chars = np.array(list(\"ab\"))\n",
        "    random_indices = np.random.randint(0, len(chars), size=(num_records, string_length))\n",
        "    char_array = chars[random_indices]\n",
        "    return np.array(list(map(\"\".join, char_array)))\n",
        "\n",
        "\n",
        "def _write_parquet_streamed(\n",
        "    filename,\n",
        "    total_rows,\n",
        "    make_chunk_fn,\n",
        "    chunk_size=250_000,\n",
        "    compression=\"snappy\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Stream DataFrame chunks to a single Parquet file with one ParquetWriter.\n",
        "    - schema_df: optional small DataFrame to lock schema; if None we'll infer from the first chunk.\n",
        "    \"\"\"\n",
        "    written = 0\n",
        "\n",
        "    first_chunk = make_chunk_fn(0, min(chunk_size, total_rows))\n",
        "    first_table = pa.Table.from_pandas(first_chunk, preserve_index=False)\n",
        "    writer = pq.ParquetWriter(filename, first_table.schema, compression=compression)\n",
        "    writer.write_table(first_table)\n",
        "\n",
        "    written += len(first_chunk)\n",
        "    del first_chunk\n",
        "    gc.collect()\n",
        "\n",
        "    while written < total_rows:\n",
        "        take = min(chunk_size, total_rows - written)\n",
        "        chunk_df = make_chunk_fn(written, take)\n",
        "        writer.write_table(pa.Table.from_pandas(chunk_df, preserve_index=False))\n",
        "        written += take\n",
        "        del chunk_df\n",
        "        gc.collect()\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def generate_test_data(target_size=\"100MB\"):\n",
        "    \"\"\"\n",
        "    Generate datasets with proper foreign key relationships.\n",
        "\n",
        "    Target COMPRESSED Parquet file sizes on disk:\n",
        "    100MB total compressed:\n",
        "        - Songs: 10K rows → ~5MB (5% of total)\n",
        "        - Users: 50K rows → ~20MB (20% of total)\n",
        "        - Listens: 1M rows → ~75MB (75% of total)\n",
        "    1GB total compressed:\n",
        "        - Songs: 100K rows → ~50MB (5% of total)\n",
        "        - Users: 500K rows → ~200MB (20% of total)\n",
        "        - Listens: 10M rows → ~750MB (75% of total)\n",
        "\n",
        "    Each table needs:\n",
        "        - Primary key column(s)\n",
        "        - 10 additional string columns of k characters each\n",
        "        - For Users: add 'age' column (random 18-80)\n",
        "\n",
        "    CRITICAL: Listens table must have valid foreign keys!\n",
        "    Every song_id must exist in Songs\n",
        "    Every user_id must exist in Users\n",
        "    \"\"\"\n",
        "\n",
        "    assert target_size in [\"100MB\", \"1GB\"]\n",
        "    if target_size == \"100MB\":\n",
        "        num_songs = 10_000\n",
        "        num_users = 50_000\n",
        "        num_listens = 1_000_000\n",
        "\n",
        "        songs_chunk = 10_000\n",
        "        users_chunk = 50_000\n",
        "        listens_chunk = 1_000_000\n",
        "    else:\n",
        "        num_songs = 100_000\n",
        "        num_users = 500_000\n",
        "        num_listens = 10_000_000\n",
        "\n",
        "        songs_chunk = 10_000\n",
        "        users_chunk = 50_000\n",
        "        listens_chunk = 1_000_000\n",
        "\n",
        "    print(\"Writing Songs\")\n",
        "    _write_parquet_streamed(\n",
        "        filename=f\"songs_{target_size}.parquet\",\n",
        "        total_rows=num_songs,\n",
        "        make_chunk_fn=lambda start, size: generate_songs_chunk(start, size),\n",
        "        chunk_size=songs_chunk,\n",
        "    )\n",
        "\n",
        "    print(\"Writing Users\")\n",
        "    _write_parquet_streamed(\n",
        "        filename=f\"users_{target_size}.parquet\",\n",
        "        total_rows=num_users,\n",
        "        make_chunk_fn=lambda start, size: generate_users_chunk(start, size),\n",
        "        chunk_size=users_chunk,\n",
        "    )\n",
        "\n",
        "    print(\"Writing Listens\")\n",
        "    _write_parquet_streamed(\n",
        "        filename=f\"listens_{target_size}.parquet\",\n",
        "        total_rows=num_listens,\n",
        "        make_chunk_fn=lambda start, size: generate_listens_chunk(\n",
        "            start, size, num_users, num_songs\n",
        "        ),\n",
        "        chunk_size=listens_chunk,\n",
        "    )\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5qnpQghhWkL",
        "outputId": "40816b75-44cb-4338-8610-e9838e59970a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Songs\n",
            "Writing Users\n",
            "Writing Listens\n",
            "Done!\n",
            "Writing Songs\n",
            "Writing Users\n",
            "Writing Listens\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "random.seed(0)\n",
        "\n",
        "generate_test_data('100MB')\n",
        "generate_test_data('1GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEiGGznFhtxo"
      },
      "source": [
        "# Section 1: Parquet-based Columnar Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BGrkP5PhWkM"
      },
      "source": [
        "Implement Parquet-based storage for the tables\n",
        "- For simplicity, store all data for a table in a single Parquet file and use a single DataFrame object as a buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0o8zkpGhWkM"
      },
      "outputs": [],
      "source": [
        "# see ed: https://edstem.org/us/courses/87394/discussion/7251811 for advice on writing to a parquet without loading existing into RAM\n",
        "# a ColumnarDbFile is actually a directory with an arbitrary number of parquet files inside\n",
        "# Append writes a new file with the next postfix\n",
        "# Retrieve reads all parquet files and concatenates them together, done natively by pandas\n",
        "class ColumnarDbFile:\n",
        "    def __init__(self, table_name, file_dir='data', file_pfx=''):\n",
        "        self.file_pfx = file_pfx\n",
        "        self.table_name = table_name\n",
        "        self.file_dir = file_dir\n",
        "        #os.makedirs(self.file_dir, exist_ok=True)\n",
        "        self.base_file_name = f\"{self.file_dir}/{self.file_pfx}_{self.table_name}\"\n",
        "        os.makedirs(self.base_file_name, exist_ok=True)\n",
        "\n",
        "    def build_table(self, data):\n",
        "        \"\"\"Build and save table data to Parquet.\"\"\"\n",
        "        data.to_parquet(f\"{self.base_file_name}/{self.table_name}-0.parquet\")\n",
        "        return\n",
        "    \n",
        "    def get_parquet_file():\n",
        "        '''return a path to a new file'''\n",
        "\n",
        "\n",
        "    def retrieve_data(self, columns=None):\n",
        "        \"\"\"Create pd.DataFrame by reading from Parquet\"\"\"\n",
        "        return pd.read_parquet(self.base_file_name, columns=columns)\n",
        "\n",
        "    def append_data(self, data):\n",
        "        \"\"\"Append new data to Parquet\"\"\"\n",
        "        # Use glob to count the number of parquet files in the directory\n",
        "        data.to_parquet(f\"{self.base_file_name}/{self.table_name}-{self._get_num_parquets}.parquet\")\n",
        "        return\n",
        "\n",
        "    def _get_num_parquets(self):\n",
        "        return len(glob.glob(f\"{self.base_file_name}/*.parquet\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gPpXtxghWkM",
        "outputId": "43c2aa9b-e2a3-4059-b2db-3d1955dcd07d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building tables...\n",
            "Tables built successfully.\n"
          ]
        }
      ],
      "source": [
        "print(\"Building tables...\")\n",
        "if os.path.exists('data'):\n",
        "    shutil.rmtree('data')\n",
        "tables = {\n",
        "    'Songs': ColumnarDbFile(\"Songs\", file_dir='data'),\n",
        "    'Users': ColumnarDbFile(\"Users\", file_dir='data'),\n",
        "    'Listens': ColumnarDbFile(\"Listens\", file_dir='data')\n",
        "}\n",
        "\n",
        "size = \"100MB\"\n",
        "songs_data = pd.read_parquet(f'songs_{size}.parquet')\n",
        "users_data = pd.read_parquet(f'users_{size}.parquet')\n",
        "listens_data = pd.read_parquet(f'listens_{size}.parquet')\n",
        "\n",
        "tables['Songs'].build_table(songs_data)\n",
        "tables['Users'].build_table(users_data)\n",
        "tables['Listens'].build_table(listens_data)\n",
        "print(\"Tables built successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "LfutqrA2hWkM",
        "outputId": "3fdee8d3-19d1-404a-cbb8-312fc4ff8485"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"tables['Songs']\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"song_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2886,\n        \"min\": 0,\n        \"max\": 9999,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          6252,\n          4684,\n          1731\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"Song_6252\",\n          \"Song_4684\",\n          \"Song_1731\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-db8432b5-f94d-4bf7-8617-898348ace850\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>song_id</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Song_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Song_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Song_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Song_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Song_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>Song_9995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>Song_9996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>Song_9997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>Song_9998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>Song_9999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db8432b5-f94d-4bf7-8617-898348ace850')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-db8432b5-f94d-4bf7-8617-898348ace850 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-db8432b5-f94d-4bf7-8617-898348ace850');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5f5f480b-65a8-4561-a061-6192b2bea004\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f5f480b-65a8-4561-a061-6192b2bea004')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5f5f480b-65a8-4561-a061-6192b2bea004 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      song_id      title\n",
              "0           0     Song_0\n",
              "1           1     Song_1\n",
              "2           2     Song_2\n",
              "3           3     Song_3\n",
              "4           4     Song_4\n",
              "...       ...        ...\n",
              "9995     9995  Song_9995\n",
              "9996     9996  Song_9996\n",
              "9997     9997  Song_9997\n",
              "9998     9998  Song_9998\n",
              "9999     9999  Song_9999\n",
              "\n",
              "[10000 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# retrieve data\n",
        "tables['Songs'].retrieve_data(columns = ['song_id', 'title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ZiPY1Hs9hWkM",
        "outputId": "d1bad27a-9419-41f8-94b9-1aa6abe5d6bb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-896e2a54-ec4c-42ff-8ccf-e6f9c00c2290\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>listen_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>song_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>33109</td>\n",
              "      <td>7536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2971</td>\n",
              "      <td>6538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>12240</td>\n",
              "      <td>1787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>13548</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>13304</td>\n",
              "      <td>1375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>999995</td>\n",
              "      <td>21848</td>\n",
              "      <td>773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>999996</td>\n",
              "      <td>9875</td>\n",
              "      <td>7612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>999997</td>\n",
              "      <td>24926</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>999998</td>\n",
              "      <td>46770</td>\n",
              "      <td>4747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>999999</td>\n",
              "      <td>27668</td>\n",
              "      <td>9028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000000 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-896e2a54-ec4c-42ff-8ccf-e6f9c00c2290')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-896e2a54-ec4c-42ff-8ccf-e6f9c00c2290 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-896e2a54-ec4c-42ff-8ccf-e6f9c00c2290');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-17038cc2-262f-4a68-8287-84c19a0a8a8b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-17038cc2-262f-4a68-8287-84c19a0a8a8b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-17038cc2-262f-4a68-8287-84c19a0a8a8b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        listen_id  user_id  song_id\n",
              "0               0    33109     7536\n",
              "1               1     2971     6538\n",
              "2               2    12240     1787\n",
              "3               3    13548     2160\n",
              "4               4    13304     1375\n",
              "...           ...      ...      ...\n",
              "999995     999995    21848      773\n",
              "999996     999996     9875     7612\n",
              "999997     999997    24926      180\n",
              "999998     999998    46770     4747\n",
              "999999     999999    27668     9028\n",
              "\n",
              "[1000000 rows x 3 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tables['Listens'].retrieve_data(columns = ['listen_id', 'user_id', 'song_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtLVO3cChWkM"
      },
      "source": [
        "Analyze and report on:\n",
        "- Space efficiency compared to row storage\n",
        "  - e.g. Compare file sizes on disk: How much disk space does Parquet use vs. a row storage format like CSV?\n",
        "- Compression ratios achieved with Parquet\n",
        "  - e.g. Compare Parquet’s uncompressed encoded size (reported in its metadata) to its compressed on-disk size to compute compression ratios.\n",
        "  - You could also report the memory expansion factor: how much larger the dataset becomes when loaded into a `pd.DataFrame` compared to the compressed file size.\n",
        "- Read/write performance characteristics\n",
        "  - e.g. Read performance: How long does it take to read all columns from Parquet vs. CSV?\n",
        "  - e.g. Columnar advantage: How long does it take to read selective columns from Parquet vs. reading all columns?\n",
        "  - e.g. Write performance: How long does it take to write data to Parquet vs. CSV?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7kfNisQFhWkM"
      },
      "outputs": [],
      "source": [
        "def analyze(size=\"100MB\"):\n",
        "    \"\"\"Analyze storage efficiency, compression, and read/write performance.\"\"\"\n",
        "\n",
        "    table_files = {\n",
        "        \"Songs\": f\"songs_{size}.parquet\",\n",
        "        \"Users\": f\"users_{size}.parquet\",\n",
        "        \"Listens\": f\"listens_{size}.parquet\",\n",
        "    }\n",
        "\n",
        "    report_rows = []\n",
        "\n",
        "    for table_name, parquet_file in table_files.items():\n",
        "        parquet_path = Path(parquet_file)\n",
        "\n",
        "        df = pd.read_parquet(parquet_path)\n",
        "        mem_usage_bytes = df.memory_usage(deep=True).sum() # memory usage of the dataframe\n",
        "        parquet_size_bytes = parquet_path.stat().st_size # size of the parquet file on disk\n",
        "\n",
        "        parquet_file_obj = pq.ParquetFile(parquet_path)\n",
        "        metadata = parquet_file_obj.metadata\n",
        "        uncompressed_bytes = 0\n",
        "\n",
        "        # iterate over all row groups and columns to get the total uncompressed size of the parquet file\n",
        "        for rg_idx in range(metadata.num_row_groups):\n",
        "            row_group = metadata.row_group(rg_idx)\n",
        "            for col_idx in range(row_group.num_columns):\n",
        "                column_meta = row_group.column(col_idx)\n",
        "                if column_meta.total_uncompressed_size is not None:\n",
        "                    uncompressed_bytes += column_meta.total_uncompressed_size\n",
        "\n",
        "        # calculate compression ratio and memory expansion\n",
        "        compression_ratio = (\n",
        "            uncompressed_bytes / parquet_size_bytes\n",
        "        )\n",
        "        memory_expansion = (\n",
        "            mem_usage_bytes / parquet_size_bytes\n",
        "        )\n",
        "\n",
        "        # test reading speed of parquet file vs csv, for all columns and selective columns\n",
        "        # pick 1 less than the total number of columns to test reading selective columns\n",
        "        subset_columns = list(df.columns)[0:len(df.columns)-1]\n",
        "\n",
        "        with tempfile.TemporaryDirectory() as tmpdir:\n",
        "            tmpdir_path = Path(tmpdir)\n",
        "\n",
        "            csv_path = tmpdir_path / f\"{parquet_path.stem}.csv\"\n",
        "            start = time.perf_counter()\n",
        "            df.to_csv(csv_path, index=False)\n",
        "            write_csv_time = time.perf_counter() - start\n",
        "            csv_size_bytes = csv_path.stat().st_size\n",
        "\n",
        "            parquet_tmp_path = tmpdir_path / f\"{parquet_path.stem}.parquet\"\n",
        "            start = time.perf_counter()\n",
        "            df.to_parquet(parquet_tmp_path, index=False)\n",
        "            write_parquet_time = time.perf_counter() - start\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            _ = pd.read_parquet(parquet_path)\n",
        "            read_parquet_all = time.perf_counter() - start\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            _ = pd.read_csv(csv_path)\n",
        "            read_csv_all = time.perf_counter() - start\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            _ = pd.read_parquet(parquet_path, columns=subset_columns)\n",
        "            read_parquet_subset = time.perf_counter() - start\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            _ = pd.read_csv(csv_path, usecols=subset_columns)\n",
        "            read_csv_subset = time.perf_counter() - start\n",
        "\n",
        "        size_saving_pct = (\n",
        "            100.0 * (1 - parquet_size_bytes / csv_size_bytes)\n",
        "        )\n",
        "\n",
        "        # append the results to the report\n",
        "        report_rows.append(\n",
        "            {\n",
        "                \"table\": table_name,\n",
        "                \"parquet_size_mb\": parquet_size_bytes / (1024 ** 2),\n",
        "                \"csv_size_mb\": csv_size_bytes / (1024 ** 2),\n",
        "                \"size_saving_pct\": size_saving_pct,\n",
        "                \"compression_ratio\": compression_ratio,\n",
        "                \"memory_expansion\": memory_expansion,\n",
        "                \"read_parquet_all_s\": read_parquet_all,\n",
        "                \"read_csv_all_s\": read_csv_all,\n",
        "                \"read_parquet_subset_s\": read_parquet_subset,\n",
        "                \"read_csv_subset_s\": read_csv_subset,\n",
        "                \"write_parquet_s\": write_parquet_time,\n",
        "                \"write_csv_s\": write_csv_time,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        del df\n",
        "        gc.collect()\n",
        "\n",
        "    summary = pd.DataFrame(report_rows)\n",
        "    print(\"Analysis Summary for Tables of Size \" + size + \" (sizes in MB, times in seconds):\")\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "Xkw4z1GMhXsH",
        "outputId": "39eebc07-7084-40c3-b28d-25ac2ecb8dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis Summary for Tables of Size 100MB (sizes in MB, times in seconds):\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(analyze(size=\\\"100MB\\\"))\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"table\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Songs\",\n          \"Users\",\n          \"Listens\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"parquet_size_mb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 39.893352000429275,\n        \"min\": 4.270175933837891,\n        \"max\": 79.98812866210938,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4.270175933837891,\n          20.345389366149902,\n          79.98812866210938\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"csv_size_mb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 88.57560903092028,\n        \"min\": 9.773173332214355,\n        \"max\": 178.86749649047852,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          9.773173332214355,\n          48.579237937927246,\n          178.86749649047852\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_saving_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4371941932432943,\n        \"min\": 55.28079151800098,\n        \"max\": 58.119167303228416,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          56.307170775713885,\n          58.119167303228416,\n          55.28079151800098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"compression_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02817070613467332,\n        \"min\": 2.416895509294703,\n        \"max\": 2.4716766961097725,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.416895509294703,\n          2.4716766961097725,\n          2.4328829351279313\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"memory_expansion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.6176504489559,\n        \"min\": 3.4748544087418183,\n        \"max\": 8.03590015035953,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.4748544087418183,\n          3.5296351727669015,\n          8.03590015035953\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"read_parquet_all_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6632126301441699,\n        \"min\": 0.0625764189999245,\n        \"max\": 3.091684301999976,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0625764189999245,\n          0.3866649750000306,\n          3.091684301999976\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"read_csv_all_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.7015225585371048,\n        \"min\": 0.16901915900007225,\n        \"max\": 6.910287744000016,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.16901915900007225,\n          0.8900721349998548,\n          6.910287744000016\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"read_parquet_subset_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.01094312067249,\n        \"min\": 0.053261436999946454,\n        \"max\": 3.648525196000037,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.053261436999946454,\n          0.28972850099989955,\n          3.648525196000037\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"read_csv_subset_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2804302708506863,\n        \"min\": 0.1506583650000266,\n        \"max\": 6.1457315039999685,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.1506583650000266,\n          0.8399506169998858,\n          6.1457315039999685\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"write_parquet_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.78539952739956,\n        \"min\": 0.06785523400003513,\n        \"max\": 3.2895821750000778,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.06785523400003513,\n          0.3451854069999172,\n          3.2895821750000778\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"write_csv_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.514307604388852,\n        \"min\": 0.35531581899999765,\n        \"max\": 12.617511702999991,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.35531581899999765,\n          2.6736031479999838,\n          12.617511702999991\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-44c1ea77-1f52-4466-ad71-e7f5f8f94046\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>table</th>\n",
              "      <th>parquet_size_mb</th>\n",
              "      <th>csv_size_mb</th>\n",
              "      <th>size_saving_pct</th>\n",
              "      <th>compression_ratio</th>\n",
              "      <th>memory_expansion</th>\n",
              "      <th>read_parquet_all_s</th>\n",
              "      <th>read_csv_all_s</th>\n",
              "      <th>read_parquet_subset_s</th>\n",
              "      <th>read_csv_subset_s</th>\n",
              "      <th>write_parquet_s</th>\n",
              "      <th>write_csv_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Songs</td>\n",
              "      <td>4.270176</td>\n",
              "      <td>9.773173</td>\n",
              "      <td>56.307171</td>\n",
              "      <td>2.416896</td>\n",
              "      <td>3.474854</td>\n",
              "      <td>0.062576</td>\n",
              "      <td>0.169019</td>\n",
              "      <td>0.053261</td>\n",
              "      <td>0.150658</td>\n",
              "      <td>0.067855</td>\n",
              "      <td>0.355316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Users</td>\n",
              "      <td>20.345389</td>\n",
              "      <td>48.579238</td>\n",
              "      <td>58.119167</td>\n",
              "      <td>2.471677</td>\n",
              "      <td>3.529635</td>\n",
              "      <td>0.386665</td>\n",
              "      <td>0.890072</td>\n",
              "      <td>0.289729</td>\n",
              "      <td>0.839951</td>\n",
              "      <td>0.345185</td>\n",
              "      <td>2.673603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Listens</td>\n",
              "      <td>79.988129</td>\n",
              "      <td>178.867496</td>\n",
              "      <td>55.280792</td>\n",
              "      <td>2.432883</td>\n",
              "      <td>8.035900</td>\n",
              "      <td>3.091684</td>\n",
              "      <td>6.910288</td>\n",
              "      <td>3.648525</td>\n",
              "      <td>6.145732</td>\n",
              "      <td>3.289582</td>\n",
              "      <td>12.617512</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44c1ea77-1f52-4466-ad71-e7f5f8f94046')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-44c1ea77-1f52-4466-ad71-e7f5f8f94046 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-44c1ea77-1f52-4466-ad71-e7f5f8f94046');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d6b402b5-a81f-40ee-b91b-e22780693fe5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d6b402b5-a81f-40ee-b91b-e22780693fe5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d6b402b5-a81f-40ee-b91b-e22780693fe5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     table  parquet_size_mb  csv_size_mb  size_saving_pct  compression_ratio  \\\n",
              "0    Songs         4.270176     9.773173        56.307171           2.416896   \n",
              "1    Users        20.345389    48.579238        58.119167           2.471677   \n",
              "2  Listens        79.988129   178.867496        55.280792           2.432883   \n",
              "\n",
              "   memory_expansion  read_parquet_all_s  read_csv_all_s  \\\n",
              "0          3.474854            0.062576        0.169019   \n",
              "1          3.529635            0.386665        0.890072   \n",
              "2          8.035900            3.091684        6.910288   \n",
              "\n",
              "   read_parquet_subset_s  read_csv_subset_s  write_parquet_s  write_csv_s  \n",
              "0               0.053261           0.150658         0.067855     0.355316  \n",
              "1               0.289729           0.839951         0.345185     2.673603  \n",
              "2               3.648525           6.145732         3.289582    12.617512  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(analyze(size=\"100MB\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rIDZx-dvhXsH"
      },
      "outputs": [],
      "source": [
        "# display(analyze(size=\"1GB\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8U3edewiDBa"
      },
      "source": [
        "# Section 2: Parse SQL Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_KoWHLohWkM"
      },
      "source": [
        "In this section, you should implement logic to parse the following SQL query:\n",
        "```sql\n",
        "    SELECT s.song_id, AVG(u.age) AS avg_age,\n",
        "       COUNT(DISTINCT l.user_id) AS count_distinct_users,\n",
        "    FROM Songs s\n",
        "    JOIN Listens l ON s.song_id = l.song_id\n",
        "    JOIN Users u ON l.user_id = u.user_id\n",
        "    GROUP BY s.song_id, s.title\n",
        "    ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
        "```\n",
        "\n",
        "You should manually extract the components from the provided query (i.e. you don't need to implement a general SQL parser, just handle this specific query)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TT3jWKFYhWkN"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT s.song_id, AVG(u.age) AS avg_age,\n",
        "COUNT(DISTINCT l.user_id)\n",
        "FROM Songs s\n",
        "JOIN Listens l ON s.song_id = l.song_id\n",
        "JOIN Users u ON l.user_id = u.user_id\n",
        "GROUP BY s.song_id, s.title\n",
        "ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "J1PmMhCRhv0r"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import re\n",
        "\n",
        "def parse_tables(query):\n",
        "\n",
        "    # pattern matches: \"from songs s\" or \"join listens l\"\n",
        "    pattern = r\"(from|join)\\s+([a-z_]+)\\s+([a-z])\"\n",
        "\n",
        "    matches = re.findall(pattern, query)\n",
        "\n",
        "    tables = {}\n",
        "    for _, table_name, alias in matches:\n",
        "        tables[alias] = table_name\n",
        "\n",
        "    return tables\n",
        "\n",
        "def parse_joins(query):\n",
        "\n",
        "    # 1) Get the base table from the FROM clause\n",
        "    base_match = re.search(r\"from\\s+([a-z_]+)\\s+([a-z])\", query)\n",
        "    if not base_match:\n",
        "        raise ValueError(\"Could not find FROM clause\")\n",
        "\n",
        "    base_table_name = base_match.group(1)\n",
        "    base_alias = base_match.group(2)\n",
        "    base_table = (base_alias, base_table_name)\n",
        "\n",
        "    # 2) Get each JOIN clause, in order\n",
        "    # pattern matches:\n",
        "    #   join listens l on s.song_id = l.song_id\n",
        "    join_pattern = (\n",
        "        r\"join\\s+([a-z_]+)\\s+([a-z])\\s+on\\s+\"\n",
        "        r\"([a-z])\\.([a-z_]+)\\s*=\\s*([a-z])\\.([a-z_]+)\"\n",
        "    )\n",
        "\n",
        "    joins = []\n",
        "    for m in re.finditer(join_pattern, query):\n",
        "        joined_table_name = m.group(1)\n",
        "        joined_alias = m.group(2)\n",
        "        left_alias = m.group(3)\n",
        "        left_col = m.group(4)\n",
        "        right_alias = m.group(5)\n",
        "        right_col = m.group(6)\n",
        "\n",
        "        joins.append(\n",
        "            {\n",
        "                \"joined_table_alias\": joined_alias,\n",
        "                \"joined_table_name\": joined_table_name,\n",
        "                \"left_alias\": left_alias,\n",
        "                \"left_column\": left_col,\n",
        "                \"right_alias\": right_alias,\n",
        "                \"right_column\": right_col,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\"base_table\" : base_table, \"Joins\" : joins}\n",
        "\n",
        "\n",
        "def parse_group_by(query):\n",
        "    \"\"\"\n",
        "    Return GROUP BY columns as a list of (alias, column) tuples.\n",
        "    Example: [('s', 'song_id'), ('s', 'title')]\n",
        "    \"\"\"\n",
        "    q = query.lower()\n",
        "\n",
        "    # Capture whatever is between GROUP BY and ORDER BY/semicolon/end\n",
        "    match = re.search(r\"group\\s+by\\s+(.+?)(order\\s+by|;|$)\", q, re.DOTALL)\n",
        "    if not match:\n",
        "        return []\n",
        "\n",
        "    groupby_text = match.group(1).strip()\n",
        "\n",
        "    columns = []\n",
        "    for col in groupby_text.split(\",\"):\n",
        "        col = col.strip()\n",
        "\n",
        "        # Expect pattern: alias.column\n",
        "        alias, column = col.split(\".\")\n",
        "        columns.append((alias, column))\n",
        "\n",
        "    return columns\n",
        "\n",
        "def parse_select_and_aggregations(query):\n",
        "    \"\"\"\n",
        "    Build:\n",
        "      aggregations: {agg_key: {...}}\n",
        "      select: list of items that may refer to agg_key\n",
        "    \"\"\"\n",
        "    q = query.lower()\n",
        "\n",
        "    m = re.search(r\"select\\s+(.+?)\\s+from\", q, re.DOTALL)\n",
        "    if not m:\n",
        "        return [], {}\n",
        "\n",
        "    select_text = m.group(1).strip()\n",
        "    raw_items = [item.strip() for item in select_text.split(\",\") if item.strip()]\n",
        "\n",
        "    select_list = []\n",
        "    aggregations = {}\n",
        "    agg_id = 1\n",
        "\n",
        "    for idx, item in enumerate(raw_items, start=1):\n",
        "        # AVG(...)\n",
        "        if item.startswith(\"avg(\"):\n",
        "            m_avg = re.match(\n",
        "                r\"avg\\(\\s*([a-z])\\.([a-z_]+)\\s*\\)(\\s+as\\s+([a-z_]+))?\",\n",
        "                item\n",
        "            )\n",
        "            if not m_avg:\n",
        "                raise ValueError(f\"Could not parse AVG aggregation: {item}\")\n",
        "            alias_letter = m_avg.group(1)\n",
        "            col_name = m_avg.group(2)\n",
        "            out_alias = m_avg.group(4) if m_avg.group(4) else None\n",
        "\n",
        "            aggregations[agg_id] = {\n",
        "                \"func\": \"avg\",\n",
        "                \"source\": (alias_letter, col_name),\n",
        "                \"distinct\": False,\n",
        "                \"output_name\": out_alias,\n",
        "            }\n",
        "\n",
        "            select_list.append(\n",
        "                {\n",
        "                    \"kind\": \"aggregation\",\n",
        "                    \"agg_key\": agg_id,\n",
        "                    \"alias\": out_alias,\n",
        "\n",
        "                }\n",
        "            )\n",
        "            agg_id += 1\n",
        "\n",
        "        # COUNT(DISTINCT ...)\n",
        "        elif item.startswith(\"count(\"):\n",
        "            m_cnt = re.match(\n",
        "                r\"count\\(\\s*distinct\\s+([a-z])\\.([a-z_]+)\\s*\\)(\\s+as\\s+([a-z_]+))?\",\n",
        "                item\n",
        "            )\n",
        "            if not m_cnt:\n",
        "                raise ValueError(f\"Could not parse COUNT aggregation: {item}\")\n",
        "            alias_letter = m_cnt.group(1)\n",
        "            col_name = m_cnt.group(2)\n",
        "            out_alias = m_cnt.group(4) if m_cnt.group(4) else None\n",
        "\n",
        "            aggregations[agg_id] = {\n",
        "                \"func\": \"count\",\n",
        "                \"source\": (alias_letter, col_name),\n",
        "                \"distinct\": True,\n",
        "                \"output_name\": out_alias,\n",
        "            }\n",
        "\n",
        "            select_list.append(\n",
        "                {\n",
        "                    \"kind\": \"aggregation\",\n",
        "                    \"agg_key\": agg_id,\n",
        "                    \"alias\": out_alias,\n",
        "                }\n",
        "            )\n",
        "            agg_id += 1\n",
        "\n",
        "        # Plain column: alias.column\n",
        "        else:\n",
        "            alias_letter, col_name = item.split(\".\")\n",
        "            select_list.append(\n",
        "                {\n",
        "                    \"kind\": \"column\",\n",
        "                    \"source\": (alias_letter, col_name),\n",
        "                    \"alias\": None,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return select_list, aggregations\n",
        "\n",
        "\n",
        "def parse_order_by(query, aggregations):\n",
        "    \"\"\"\n",
        "    Build order_by list where entries can refer to aggregations via agg_key.\n",
        "    \"\"\"\n",
        "    q = query.lower()\n",
        "\n",
        "    m = re.search(r\"order\\s+by\\s+(.+?)(;|$)\", q, re.DOTALL)\n",
        "    if not m:\n",
        "        return []\n",
        "\n",
        "    order_text = m.group(1).strip()\n",
        "    raw_items = [item.strip() for item in order_text.split(\",\") if item.strip()]\n",
        "\n",
        "    order_by = []\n",
        "\n",
        "    for item in raw_items:\n",
        "        direction = \"asc\"\n",
        "        expr = item\n",
        "\n",
        "        if expr.endswith(\" desc\"):\n",
        "            direction = \"desc\"\n",
        "            expr = expr[:-5].strip()\n",
        "        elif expr.endswith(\" asc\"):\n",
        "            direction = \"asc\"\n",
        "            expr = expr[:-4].strip()\n",
        "\n",
        "        # COUNT(DISTINCT ...) → match an aggregation\n",
        "        if expr.startswith(\"count(\"):\n",
        "            m_cnt = re.match(\n",
        "                r\"count\\(\\s*distinct\\s+([a-z])\\.([a-z_]+)\\s*\\)\",\n",
        "                expr\n",
        "            )\n",
        "            if not m_cnt:\n",
        "                raise ValueError(f\"Could not parse ORDER BY aggregation: {expr}\")\n",
        "            src = (m_cnt.group(1), m_cnt.group(2))\n",
        "\n",
        "            agg_key = None\n",
        "            for k, agg in aggregations.items():\n",
        "                if (\n",
        "                    agg[\"func\"] == \"count\"\n",
        "                    and agg[\"distinct\"]\n",
        "                    and agg[\"source\"] == src\n",
        "                ):\n",
        "                    agg_key = k\n",
        "                    break\n",
        "\n",
        "            if agg_key is None:\n",
        "                raise ValueError(f\"No matching aggregation found for ORDER BY expr: {expr}\")\n",
        "\n",
        "            order_by.append(\n",
        "                {\n",
        "                    \"kind\": \"aggregation\",\n",
        "                    \"agg_key\": agg_key,\n",
        "                    \"direction\": direction,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # assume plain column: alias.column\n",
        "            alias_letter, col_name = expr.split(\".\")\n",
        "            order_by.append(\n",
        "                {\n",
        "                    \"kind\": \"column\",\n",
        "                    \"source\": (alias_letter, col_name),\n",
        "                    \"direction\": direction,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return order_by\n",
        "\n",
        "def parse_sql(query):\n",
        "    \"\"\"\n",
        "    YOUR TASK: Extract tables, joins, and aggregations\n",
        "    \"\"\"\n",
        "    # Parse SQL string to identify:\n",
        "    # - Tables involved\n",
        "    # - Join conditions\n",
        "    # - GROUP BY columns\n",
        "    # - Aggregation functions\n",
        "    # Your implementation here\n",
        "    query = query.lower()\n",
        "    output = {}\n",
        "\n",
        "    output[\"tables\"] = parse_tables(query)\n",
        "    output[\"joins\"] = parse_joins(query)\n",
        "    output[\"GroupBy\"] = parse_group_by(query)\n",
        "    output[\"select\"], output[\"aggregations\"] = parse_select_and_aggregations(query)\n",
        "    output[\"orderBy\"] = parse_order_by(query, output[\"aggregations\"])\n",
        "\n",
        "    return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8mb80aeTAxM",
        "outputId": "02681db1-41c7-4339-907a-7c377b821792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tables: {'s': 'songs', 'l': 'listens', 'u': 'users'}\n",
            "joins: {'base_table': ('s', 'songs'), 'Joins': [{'joined_table_alias': 'l', 'joined_table_name': 'listens', 'left_alias': 's', 'left_column': 'song_id', 'right_alias': 'l', 'right_column': 'song_id'}, {'joined_table_alias': 'u', 'joined_table_name': 'users', 'left_alias': 'l', 'left_column': 'user_id', 'right_alias': 'u', 'right_column': 'user_id'}]}\n",
            "GroupBy: [('s', 'song_id'), ('s', 'title')]\n",
            "select: [{'kind': 'column', 'source': ('s', 'song_id'), 'alias': None}, {'kind': 'aggregation', 'agg_key': 1, 'alias': 'avg_age'}, {'kind': 'aggregation', 'agg_key': 2, 'alias': None}]\n",
            "aggregations: {1: {'func': 'avg', 'source': ('u', 'age'), 'distinct': False, 'output_name': 'avg_age'}, 2: {'func': 'count', 'source': ('l', 'user_id'), 'distinct': True, 'output_name': None}}\n",
            "orderBy: [{'kind': 'aggregation', 'agg_key': 2, 'direction': 'desc'}, {'kind': 'column', 'source': ('s', 'song_id'), 'direction': 'asc'}]\n"
          ]
        }
      ],
      "source": [
        "output = parse_sql(query)\n",
        "for key, value in output.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071jzSZqkGyC"
      },
      "source": [
        "# Section 3: Implement Join Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14LszEqZhWkN"
      },
      "source": [
        "In this section, you will implement the execution operators (*how* to join) and aggregation after joins.\n",
        "\n",
        "**Reminder:** If you use temporary files or folders, you should clean them up either as part of your join logic, or after each run. Otherwise you might run into correctness issues!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AVO_NRnkHq1"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "\n",
        "def HASHVALUE(value, B):\n",
        "    if isinstance(value, int):\n",
        "        return hash(value) % B\n",
        "    sha256 = hashlib.sha256()\n",
        "    sha256.update(str(value).encode(\"utf-8\"))\n",
        "    return int(sha256.hexdigest(), 16) % B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q46rgYihWkN"
      },
      "source": [
        "Implement `HashPartitionJoin`:\n",
        "1. Hash partition both tables\n",
        "2. Build hash table from smaller partition\n",
        "3. Probe with larger partition\n",
        "4. Return joined results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1pw1bPrhWkN"
      },
      "outputs": [],
      "source": [
        "# see ed: https://edstem.org/us/courses/87394/discussion/7151010 for discussion on this implementation\n",
        "\n",
        "\n",
        "class HashPartitionJoin:\n",
        "    def __init__(self, num_partitions=4, parquet_batch_size = 1000):\n",
        "        self.num_partitions = num_partitions\n",
        "        self.parquet_batch_size = parquet_batch_size\n",
        "\n",
        "    def join(self, table1: ColumnarDbFile, table2: ColumnarDbFile, join_key1, join_key2,\n",
        "             temp_dir='temp', columns_table1=None, columns_table2=None):\n",
        "        \"\"\"\n",
        "        Perform a hash partition join between two ColumnarDbFile instances.\n",
        "\n",
        "        Parameters:\n",
        "        - table1: Left table (ColumnarDbFile)\n",
        "        - table2: Right table (ColumnarDbFile)\n",
        "        - join_key1: Join key from table1\n",
        "        - join_key2: Join key from table2\n",
        "        - temp_dir: Directory to store temporary files\n",
        "        - columns_table1: List of columns to select from table1\n",
        "        - columns_table2: List of columns to select from table2\n",
        "\n",
        "        Returns:\n",
        "        - join_result_table: ColumnarDbFile instance containing the join results\n",
        "        \"\"\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Partition both tables\n",
        "        self._hash_partition(table1, join_key1, temp_dir, 'left', columns_table1)\n",
        "        self._hash_partition(table2, join_key2, temp_dir, 'right', columns_table2)\n",
        "\n",
        "        # Output file for the final joined table\n",
        "        output_base = os.path.join(temp_dir, \"hash_join_result\")\n",
        "        output_path = output_base + \".parquet\"\n",
        "        join_writer = None  # lazy-init when we see first joined batch\n",
        "\n",
        "        for part_id in range(self.num_partitions):\n",
        "            left_path = self._make_partition_path(temp_dir, \"left\", part_id)\n",
        "            right_path = self._make_partition_path(temp_dir, \"right\", part_id)\n",
        "\n",
        "            # If either side is missing, nothing to join\n",
        "            if not (os.path.exists(left_path) and os.path.exists(right_path)):\n",
        "                continue\n",
        "\n",
        "            # Load both sides fully for this partition\n",
        "            left_df = pq.read_table(left_path).to_pandas()\n",
        "            right_df = pq.read_table(right_path).to_pandas()\n",
        "\n",
        "            if left_df.empty or right_df.empty:\n",
        "                continue\n",
        "\n",
        "            # Decide which side is smaller for this partition\n",
        "            if len(left_df) <= len(right_df):\n",
        "                small_df, big_df = left_df, right_df\n",
        "                small_is_left = True\n",
        "            else:\n",
        "                small_df, big_df = right_df, left_df\n",
        "                small_is_left = False\n",
        "\n",
        "            # Build hash map from the smaller side\n",
        "            hash_map = {}\n",
        "            if small_is_left:\n",
        "                # small_df is left: hash on join_key1\n",
        "                for _, row in small_df.iterrows():\n",
        "                    key = row[join_key1]\n",
        "                    if key not in hash_map:\n",
        "                        hash_map[key] = []\n",
        "                    hash_map[key].append(row)\n",
        "            else:\n",
        "                # small_df is right: hash on join_key2\n",
        "                for _, row in small_df.iterrows():\n",
        "                    key = row[join_key2]\n",
        "                    if key not in hash_map:\n",
        "                        hash_map[key] = []\n",
        "                    hash_map[key].append(row)\n",
        "\n",
        "            # Nested-loop join probing with the larger side\n",
        "            joined_rows = []\n",
        "            if small_is_left:\n",
        "                # small = left, big = right\n",
        "                for _, r_row in big_df.iterrows():\n",
        "                    key = r_row[join_key2]\n",
        "                    if key not in hash_map:\n",
        "                        continue\n",
        "                    for l_row in hash_map[key]:\n",
        "                        combined = {}\n",
        "                        # copy all left columns\n",
        "                        for col in left_df.columns:\n",
        "                            combined[col] = l_row[col]\n",
        "                        # copy all right columns\n",
        "                        for col in right_df.columns:\n",
        "                            combined[col] = r_row[col]\n",
        "                        joined_rows.append(combined)\n",
        "            else:\n",
        "                # small = right, big = left\n",
        "                for _, l_row in big_df.iterrows():\n",
        "                    key = l_row[join_key1]\n",
        "                    if key not in hash_map:\n",
        "                        continue\n",
        "                    for r_row in hash_map[key]:\n",
        "                        combined = {}\n",
        "                        # copy all left columns\n",
        "                        for col in left_df.columns:\n",
        "                            combined[col] = l_row[col]\n",
        "                        # copy all right columns\n",
        "                        for col in right_df.columns:\n",
        "                            combined[col] = r_row[col]\n",
        "                        joined_rows.append(combined)\n",
        "\n",
        "            if not joined_rows:\n",
        "                continue\n",
        "\n",
        "            joined_df = pd.DataFrame(joined_rows)\n",
        "            joined_table = pa.Table.from_pandas(joined_df, preserve_index=False)\n",
        "\n",
        "            # Initialize writer on first non-empty batch\n",
        "            if join_writer is None:\n",
        "                join_writer = pq.ParquetWriter(output_path, joined_table.schema)\n",
        "\n",
        "            join_writer.write_table(joined_table)\n",
        "\n",
        "        if join_writer is not None:\n",
        "            join_writer.close()\n",
        "\n",
        "        # Wrap result in a ColumnarDbFile\n",
        "        join_result_table = ColumnarDbFile(\n",
        "            table_name=\"hash_join_result\",\n",
        "            file_dir=temp_dir,\n",
        "            file_pfx=\"\"\n",
        "        )\n",
        "        # Ensure base_file_name matches the file we just wrote (without .parquet)\n",
        "        join_result_table.base_file_name = output_base\n",
        "\n",
        "        return join_result_table\n",
        "\n",
        "    def _make_partition_path(self, output_dir, side, part_id):\n",
        "        return f\"{output_dir} / {side}_part{part_id}.parquet\"\n",
        "\n",
        "    def _hash_partition(self, table: ColumnarDbFile, join_key, output_dir, side, columns=None):\n",
        "        parquet_file = pq.ParquetFile(table.base_file_name)\n",
        "        writers: dict[int, pq.ParquetWriter] = {}\n",
        "        for batch in parquet_file.iter_batches(batch_size=self.parquet_batch_size, columns=columns):\n",
        "            batch_df = batch.to_pandas()\n",
        "            batch_df[\"_part\"] = batch_df[join_key].apply(lambda x: HASHVALUE, self.num_partitions)\n",
        "            if columns:\n",
        "                batch_df = batch_df[columns]\n",
        "\n",
        "            # Group rows by partition id and write them out\n",
        "            for part_id, part_df in batch_df.groupby(\"_part\"):\n",
        "                # Drop helper column before writing\n",
        "                part_df = part_df.drop(columns=[\"_part\"])\n",
        "\n",
        "                # Convert to Arrow Table\n",
        "                part_table = pa.Table.from_pandas(part_df, preserve_index=False)\n",
        "\n",
        "                # Lazily create writer for this partition\n",
        "                writer = writers.get(part_id)\n",
        "                if writer is None:\n",
        "                    part_path = self._make_partition_path(output_dir, side, part_id)\n",
        "                    writer = pq.ParquetWriter(part_path, part_table.schema)\n",
        "                    writers[part_id] = writer\n",
        "\n",
        "                # Append this batch's rows for this partition as a new row group\n",
        "                writer.write_table(part_table)\n",
        "\n",
        "        # Close all writers\n",
        "        for w in writers.values():\n",
        "            w.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "\n",
        "class HashPartitionJoin:\n",
        "    def __init__(self, num_partitions=4, parquet_batch_size=1000):\n",
        "        self.num_partitions = num_partitions\n",
        "        self.parquet_batch_size = parquet_batch_size\n",
        "\n",
        "    def join(self, table1: \"ColumnarDbFile\", table2: \"ColumnarDbFile\",\n",
        "             join_key1, join_key2,\n",
        "             temp_dir='temp',\n",
        "             columns_table1=None, columns_table2=None):\n",
        "        \"\"\"\n",
        "        Perform a simple hash-partition join between two ColumnarDbFile instances.\n",
        "\n",
        "        Assumptions / simplifications:\n",
        "        - Inner join only.\n",
        "        - We ignore NULL handling, duplicate column name issues, etc.\n",
        "        - We load each partition of left and right fully in memory.\n",
        "        \"\"\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Partition both tables\n",
        "        self._hash_partition(table1, join_key1, temp_dir, 'left', columns_table1)\n",
        "        self._hash_partition(table2, join_key2, temp_dir, 'right', columns_table2)\n",
        "\n",
        "        # Output file for the final joined table\n",
        "        output_base = os.path.join(temp_dir, \"hash_join_result\")\n",
        "        output_path = output_base + \".parquet\"\n",
        "        join_writer = None  # lazy-init when we see first joined batch\n",
        "\n",
        "        for part_id in range(self.num_partitions):\n",
        "            left_path = self._make_partition_path(temp_dir, \"left\", part_id)\n",
        "            right_path = self._make_partition_path(temp_dir, \"right\", part_id)\n",
        "\n",
        "            # If either side is missing, nothing to join\n",
        "            if not (os.path.exists(left_path) and os.path.exists(right_path)):\n",
        "                continue\n",
        "\n",
        "            # Load both sides fully for this partition\n",
        "            left_df = pq.read_table(left_path).to_pandas()\n",
        "            right_df = pq.read_table(right_path).to_pandas()\n",
        "\n",
        "            if left_df.empty or right_df.empty:\n",
        "                continue\n",
        "\n",
        "            # Build a hash map from join_key2 -> list of right rows\n",
        "            right_map = {}\n",
        "            for _, r_row in right_df.iterrows():\n",
        "                key = r_row[join_key2]\n",
        "                if key not in right_map:\n",
        "                    right_map[key] = []\n",
        "                right_map[key].append(r_row)\n",
        "\n",
        "            # Nested-loop join within this partition\n",
        "            joined_rows = []\n",
        "            for _, l_row in left_df.iterrows():\n",
        "                key = l_row[join_key1]\n",
        "                if key not in right_map:\n",
        "                    continue\n",
        "                for r_row in right_map[key]:\n",
        "                    combined = {}\n",
        "\n",
        "                    # Copy left row columns\n",
        "                    for col in left_df.columns:\n",
        "                        combined[col] = l_row[col]\n",
        "\n",
        "                    # Copy right row columns\n",
        "                    for col in right_df.columns:\n",
        "                        # If you want to avoid duplicate join key columns,\n",
        "                        # you could skip join_key2 here, but we keep it simple.\n",
        "                        combined[col] = r_row[col]\n",
        "\n",
        "                    joined_rows.append(combined)\n",
        "\n",
        "            if not joined_rows:\n",
        "                continue\n",
        "\n",
        "            joined_df = pd.DataFrame(joined_rows)\n",
        "            joined_table = pa.Table.from_pandas(joined_df, preserve_index=False)\n",
        "\n",
        "            # Initialize writer on first non-empty batch\n",
        "            if join_writer is None:\n",
        "                join_writer = pq.ParquetWriter(output_path, joined_table.schema)\n",
        "\n",
        "            join_writer.write_table(joined_table)\n",
        "\n",
        "        if join_writer is not None:\n",
        "            join_writer.close()\n",
        "\n",
        "        # Wrap result in a ColumnarDbFile\n",
        "        join_result_table = ColumnarDbFile(\n",
        "            table_name=\"hash_join_result\",\n",
        "            file_dir=temp_dir,\n",
        "            file_pfx=\"\"\n",
        "        )\n",
        "        # Ensure base_file_name matches the file we just wrote (without .parquet)\n",
        "        join_result_table.base_file_name = output_base\n",
        "\n",
        "        return join_result_table\n",
        "\n",
        "    def _make_partition_path(self, output_dir, side, part_id):\n",
        "        # no weird spaces, just clean paths\n",
        "        return os.path.join(output_dir, f\"{side}_part{part_id}.parquet\")\n",
        "\n",
        "    def _hash_partition(self, table: \"ColumnarDbFile\", join_key, output_dir, side, columns=None):\n",
        "        \"\"\"\n",
        "        Very simple hash partitioning:\n",
        "        - Reads the table in batches.\n",
        "        - Computes part_id = hash(join_key_value) % num_partitions.\n",
        "        - Writes per-partition Parquet files.\n",
        "        \"\"\"\n",
        "        parquet_file = pq.ParquetFile(table.base_file_name)\n",
        "\n",
        "        writers: dict[int, pq.ParquetWriter] = {}\n",
        "\n",
        "        for batch in parquet_file.iter_batches(batch_size=self.parquet_batch_size, columns=columns):\n",
        "            batch_df = batch.to_pandas()\n",
        "\n",
        "            # Compute partition id\n",
        "            batch_df[\"_part\"] = batch_df[join_key].apply(\n",
        "                lambda x: hash(x) % self.num_partitions\n",
        "            )\n",
        "\n",
        "            # Group rows by partition id and write them out\n",
        "            for part_id, part_df in batch_df.groupby(\"_part\"):\n",
        "                part_df = part_df.drop(columns=[\"_part\"])\n",
        "                part_table = pa.Table.from_pandas(part_df, preserve_index=False)\n",
        "\n",
        "                writer = writers.get(part_id)\n",
        "                if writer is None:\n",
        "                    part_path = self._make_partition_path(output_dir, side, part_id)\n",
        "                    writer = pq.ParquetWriter(part_path, part_table.schema)\n",
        "                    writers[part_id] = writer\n",
        "\n",
        "                writer.write_table(part_table)\n",
        "\n",
        "        for w in writers.values():\n",
        "            w.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzYK2oWS5O47"
      },
      "outputs": [],
      "source": [
        "# Optional: Verify your implementation against pd.merge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzzs-5K8hWkN"
      },
      "source": [
        "Implement `SortMergeJoin`:\n",
        "1. Sort both tables by join key\n",
        "2. Merge sorted sequences\n",
        "3. Handle duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odvuVpv2hWkN"
      },
      "outputs": [],
      "source": [
        "BWAY_MERGE_FACTOR = 10\n",
        "\n",
        "class SortMergeJoin:\n",
        "    def __init__(\n",
        "        self, bway_merge_factor: int = BWAY_MERGE_FACTOR, num_pages_per_split=1000\n",
        "    ):\n",
        "        self.bway_merge_factor = bway_merge_factor\n",
        "        self.num_pages_per_split = num_pages_per_split\n",
        "\n",
        "    def _external_sort(\n",
        "        self,\n",
        "        table: ColumnarDbFile,\n",
        "        join_key: str,\n",
        "        output_dir: str,\n",
        "        side: str,\n",
        "        columns: Optional[List[str]] = None,\n",
        "    ) -> ColumnarDbFile:\n",
        "        \"\"\"\n",
        "        Perform an external sort on a table based on the join key and return a sorted ColumnarDbFile.\n",
        "        Use _bway_merge to merge sorted files\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "\n",
        "    def _bway_merge(self, sorted_files: List[str], output_file: str, join_key: str):\n",
        "        \"\"\"\n",
        "        Merge multiple sorted Parquet files into a single sorted Parquet file using B-way merge.\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "\n",
        "    def join(\n",
        "        self,\n",
        "        table1: ColumnarDbFile,\n",
        "        table2: ColumnarDbFile,\n",
        "        join_key1: str,\n",
        "        join_key2: str,\n",
        "        temp_dir: str = \"temp\",\n",
        "        columns_table1: Optional[List[str]] = None,\n",
        "        columns_table2: Optional[List[str]] = None,\n",
        "    ) -> Optional[ColumnarDbFile]:\n",
        "        \"\"\"\n",
        "        Perform a sort-merge join between two ColumnarDbFile instances and return a sorted ColumnarDbFile.\n",
        "        \"\"\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Sort both tables externally\n",
        "        sorted_table1 = self._external_sort(\n",
        "            table1, join_key1, temp_dir, \"left\", columns_table1\n",
        "        )\n",
        "        sorted_table2 = self._external_sort(\n",
        "            table2, join_key2, temp_dir, \"right\", columns_table2\n",
        "        )\n",
        "\n",
        "        # Your implementation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yhxUqHS5ShN"
      },
      "outputs": [],
      "source": [
        "# Optional: Verify your implementation against pd.merge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af8XhdLWhWkN"
      },
      "source": [
        "Implement GROUP BY after joins:\n",
        "- Here you could use `pd.groupby` or do manual aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iwQ65LBhWkN"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feYU7Tdlke9Z"
      },
      "source": [
        "# Section 4: Query Planning & Optimization\n",
        "\n",
        "In this section, you'll implement smart query planning using metadata analysis. The key idea is to **avoid loading data unnecessarily** by:\n",
        "1. Analyzing Parquet metadata first (row counts, column names, file sizes)\n",
        "2. Making intelligent decisions about join order and algorithm selection\n",
        "3. Loading only the columns you actually need for the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQt-sR-zhWkN"
      },
      "outputs": [],
      "source": [
        "def analyze_metadata_before_loading(file_paths):\n",
        "    \"\"\"YOUR TASK: Get table statistics WITHOUT loading data\n",
        "\n",
        "    Hints:\n",
        "    - Use pq.ParquetFile() to access metadata\n",
        "    - Extract: num_rows, column names, file sizes\n",
        "    - DON'T use pd.read_parquet() here - that loads data!\n",
        "    \"\"\"\n",
        "    metadata = {}\n",
        "\n",
        "    # TODO: For each table ('songs', 'users', 'listens'):\n",
        "    #   - Open the Parquet file (but don't load data)\n",
        "    #   - Extract metadata like row count, columns, sizes\n",
        "    #   - Store in a dictionary\n",
        "    pass  # Your implementation here\n",
        "\n",
        "\n",
        "def plan_query_execution(metadata, parsed_query):\n",
        "    \"\"\"YOUR TASK: Use metadata to make smart decisions\n",
        "\n",
        "    Questions to answer:\n",
        "    - Which table is smallest? Largest?\n",
        "    - Will a hash table fit in memory?\n",
        "    - Which columns does the query actually need?\n",
        "    - What's the optimal join order?\n",
        "    \"\"\"\n",
        "    # TODO: Based on metadata, decide:\n",
        "    #   1. Join order (smallest first? or different strategy?)\n",
        "    #   2. Algorithm choice (HPJ if fits in memory, else SMJ)\n",
        "    #   3. Which columns to load for each table\n",
        "    pass  # Your implementation here\n",
        "\n",
        "\n",
        "# After planning, load ONLY what you need:\n",
        "# Example (you implement the actual logic):\n",
        "# columns_needed = ['song_id', 'artist']  # From your planning\n",
        "# df = pd.read_parquet('songs.parquet', columns=columns_needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhwOd7sfhWkN"
      },
      "outputs": [],
      "source": [
        "class QueryPlanner:\n",
        "    pass # Your implementation here\n",
        "\n",
        "\n",
        "class QueryExecutor:\n",
        "    def __init__(self, tables, num_partitions=8, output_dir=\"temp\", planner=None):\n",
        "        self.tables = tables\n",
        "        self.num_partitions = num_partitions\n",
        "        self.output_dir = output_dir\n",
        "        self.planner = planner or QueryPlanner()\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def execute_hardcoded_query(self):\n",
        "        \"\"\"\n",
        "        Executes the following SQL query:\n",
        "\n",
        "        SELECT s.song_id, AVG(u.age) AS avg_age,\n",
        "        COUNT(DISTINCT l.user_id)\n",
        "        FROM Songs s\n",
        "        JOIN Listens l ON s.song_id = l.song_id\n",
        "        JOIN Users u ON l.user_id = u.user_id\n",
        "        GROUP BY s.song_id, s.title\n",
        "        ORDER BY COUNT(DISTINCT l.user_id) DESC, s.song_id;\n",
        "        \"\"\"\n",
        "\n",
        "        # Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BunT6g1HklaH"
      },
      "source": [
        "# Section 5: Performance Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpRb3IsakmzT"
      },
      "outputs": [],
      "source": [
        "def benchmark_query(executor, dataset_size):\n",
        "    \"\"\"Benchmark the query execution time and memory usage.\"\"\"\n",
        "    print(f\"\\nBenchmarking with {dataset_size} dataset...\")\n",
        "    start_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "    start_time = time.time()\n",
        "\n",
        "    result = executor.execute_hardcoded_query()\n",
        "\n",
        "    end_time = time.time()\n",
        "    end_mem = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
        "    print(f\"Memory Usage: {end_mem - start_mem:.2f} MB\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUny8jDziWxk"
      },
      "source": [
        "## 100MB Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OBi6vhriYRK"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HalUj5s-ifAu"
      },
      "source": [
        "## 1GB Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwmefvmzigRO"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8h2f0TAijZT"
      },
      "source": [
        "## Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AQL4sxdinZn"
      },
      "outputs": [],
      "source": [
        "# Your implementation here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs145-project2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
